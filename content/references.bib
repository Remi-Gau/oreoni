%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Agah Karakuzu at 2022-08-01 15:33:59 +0300 


%% Saved with string encoding Unicode (UTF-8) 



@article{Karakuzu:2022wq,
	author = {Karakuzu, Agah and DuPre, Elizabeth and Tetrel, Loic and Bermudez, Patrick and Boudreau, Mathieu and Chin, Mary and Poline, Jean-Baptiste and Das, Samir and Bellec, Pierre and Stikov, Nikola},
	date-added = {2022-08-01 15:33:55 +0300},
	date-modified = {2022-08-01 15:33:55 +0300},
	publisher = {OSF Preprints},
	title = {NeuroLibre: A preprint server for full-fledged reproducible neuroscience},
	year = {2022}}

@article{Bourget:2022tf,
	author = {Bourget, Marie-H{\'e}l{\`e}ne and Kamentsky, Lee and Ghosh, Satrajit S and Mazzamuto, Giacomo and Lazari, Alberto and Markiewicz, Christopher J and Oostenveld, Robert and Niso, Guiomar and Halchenko, Yaroslav O and Lipp, Ilona},
	date-added = {2022-08-01 14:24:50 +0300},
	date-modified = {2022-08-01 14:24:51 +0300},
	journal = {Frontiers in Neuroscience},
	publisher = {Frontiers Media SA},
	title = {Microscopy-BIDS: An Extension to the Brain Imaging Data Structure for Microscopy Data},
	volume = {16},
	year = {2022}}

@article{Karakuzu:2022wv,
	author = {Karakuzu, Agah and Biswas, Labonny and Cohen‐Adad, Julien and Stikov, Nikola},
	date-added = {2022-08-01 14:02:46 +0300},
	date-modified = {2022-08-01 14:02:46 +0300},
	isbn = {0740-3194},
	journal = {Magnetic Resonance in Medicine},
	number = {3},
	pages = {1212--1228},
	publisher = {Wiley Online Library},
	title = {Vendor‐neutral sequences and fully transparent workflows improve inter‐vendor reproducibility of quantitative MRI},
	volume = {88},
	year = {2022}}

@article{Strand:2021tm,
	author = {Strand, Julia},
	date-added = {2022-07-28 01:38:44 +0300},
	date-modified = {2022-07-28 01:39:02 +0300},
	journal = {PsyArXiv},
	publisher = {PsyArXiv},
	title = {Error Tight: Exercises for Lab Groups to Prevent Research Mistakes},
	year = {2021}}

@article{Goodman2016-nt,
	abstract = {The language and conceptual framework of ``research
              reproducibility'' are nonstandard and unsettled across the
              sciences. In this Perspective, we review an array of explicit and
              implicit definitions of reproducibility and related terminology,
              and discuss how to avoid potential misunderstandings when these
              terms are used as a surrogate for ``truth.''},
	author = {Goodman, Steven N and Fanelli, Daniele and Ioannidis, John P A},
	date-added = {2022-07-28 01:14:25 +0300},
	date-modified = {2022-07-28 01:14:25 +0300},
	journal = {Science Translational Medicine},
	language = {en},
	month = jun,
	number = 341,
	pages = {341ps12},
	title = {What does research reproducibility mean?},
	volume = 8,
	year = 2016}

@article{Plesser2018-ma,
	abstract = {A cornerstone of science is the possibility to critically assess
               the correctness of scientific claims made and conclusions drawn
               by other scientists. This requires a systematic approach to and
               precise description of experimental procedure and subsequent
               data analysis, as well as careful attention to potential sources
               of error, both systematic and statistic. Ideally, an experiment
               or analysis should be described in sufficient detail that other
               scientists with sufficient skills and means can follow the steps
               described in published work and obtain the same results within
               the margins of experimental error. Furthermore, where
               fundamental insights into nature are obtained, such as a
               measurement of the speed of light or the propagation of action
               potentials along axons, independent confirmation of the
               measurement or phenomenon is expected using different
               experimental means. In some cases, doubts about the
               interpretation of certain results have given rise to new
               branches of science, such as Schr{\"o}dinger's development of
               the theory of first-passage times to address contradictory
               experimental data concerning the existence of fractional
               elementary charge (Schr{\"o}dinger, 1915). Experimental
               scientists have long been aware of these issues and have
               developed a systematic approach over decades, well established
               in the literature and as international standards.When scientists
               began to use digital computers to perform simulation experiments
               and data analysis, such attention to experimental err...},
	author = {Plesser, Hans E},
	date-added = {2022-07-28 01:14:10 +0300},
	date-modified = {2022-07-28 01:14:10 +0300},
	journal = {Frontiers in Neuroinformatics},
	keywords = {computational science; repeatability; replicability; reproducibility; artifacts},
	language = {en},
	publisher = {Frontiers},
	title = {Reproducibility vs. replicability: A brief history of a confused terminology},
	volume = 0,
	year = 2018}

@misc{The_Turing_Way_Community2019-be,
	abstract = {Reproducible research is necessary to ensure that scientific
               work can be trusted. Funders and publishers are beginning to
               require that publications include access to the underlying data
               and the analysis code. The goal is to ensure that all results
               can be independently verified and built upon in future work.
               This is sometimes easier said than done. Sharing these research
               outputs means understanding data management, library sciences,
               software development, and continuous integration techniques:
               skills that are not widely taught or expected of academic
               researchers and data scientists. The Turing Way is a handbook to
               support students, their supervisors, funders and journal editors
               in ensuring that reproducible data science is ``too easy not to
               do''. It will include training material on version control,
               analysis testing, and open and transparent communication with
               future users, and build on Turing Institute case studies and
               workshops. This project is openly developed and any and all
               questions, comments and recommendations are welcome at our
               github repository:
               https://github.com/alan-turing-institute/the-turing-way. Release
               log v0.0.4: Continuous integration chapter merged to master.
               v0.0.3: Reproducible environments chapter merged to master.
               v0.0.2: Version control chapter merged to master. v0.0.1:
               Reproducibility chapter merged to master.},
	author = {{The Turing Way Community} and Arnold, Becky and Bowler, Louise and Gibson, Sarah and Herterich, Patricia and Higman, Rosie and Krystalli, Anna and Morley, Alexander and O'Reilly, Martin and Whitaker, Kirstie},
	date-added = {2022-07-28 01:13:58 +0300},
	date-modified = {2022-07-28 01:13:58 +0300},
	publisher = {Zenodo},
	title = {The Turing Way: A handbook for reproducible data science},
	year = 2019}

@article{Norgaard2019-pp,
	author = {N{\o}rgaard, Martin and Ganz, Melanie and Svarer, Claus and Feng, Ling and Ichise, Masanori and Lanzenberger, Rupert and Lubberink, Mark and Parsey, Ramin V and Politis, Marios and Rabiner, Eugenii A and Slifstein, Mark and Sossi, Vesna and Suhara, Tetsuya and Talbot, Peter S and Turkheimer, Federico and Strother, Stephen C and Knudsen, Gitte M},
	journal = {Journal of Cerebral Blood Flow \& Metabolism},
	number = 2,
	pages = {210--222},
	title = {Cerebral serotonin transporter measurements with [{11C]DASB}: A review on acquisition and preprocessing across 21 {PET} centres},
	volume = 39,
	year = 2019}

@article{Tadel2011-ju,
	abstract = {Brainstorm is a collaborative open-source application dedicated
              to magnetoencephalography (MEG) and electroencephalography (EEG)
              data visualization and processing, with an emphasis on cortical
              source estimation techniques and their integration with
              anatomical magnetic resonance imaging (MRI) data. The primary
              objective of the software is to connect MEG/EEG neuroscience
              investigators with both the best-established and cutting-edge
              methods through a simple and intuitive graphical user interface
              (GUI).},
	author = {Tadel, Fran{\c c}ois and Baillet, Sylvain and Mosher, John C and Pantazis, Dimitrios and Leahy, Richard M},
	journal = {Computational Intelligence and Neuroscience},
	language = {en},
	month = apr,
	pages = {879716},
	title = {Brainstorm: A user-friendly application for {MEG/EEG} analysis},
	volume = 2011,
	year = 2011}

@article{Dale1993-cp,
	author = {Dale, A M and Sereno, Martin I},
	journal = {Journal of Cognitive Neuroscience},
	number = 2,
	pages = {162--176},
	title = {Improved localizadon of cortical activity by combining {EEG} and {MEG} with {MRI} cortical surface reconstruction: A linear approach},
	volume = 5,
	year = 1993}

@article{Amunts2016-hu,
	abstract = {Decoding the human brain is perhaps the most fascinating
              scientific challenge in the 21st century. The Human Brain Project
              (HBP), a 10-year European Flagship, targets the reconstruction of
              the brain's multi-scale organization. It uses productive loops of
              experiments, medical, data, data analytics, and simulation on all
              levels that will eventually bridge the scales. The HBP IT
              architecture is unique, utilizing cloud-based collaboration and
              development platforms with databases, workflow systems, petabyte
              storage, and supercomputers. The HBP is developing toward a
              European research infrastructure advancing brain research,
              medicine, and brain-inspired information technology.},
	author = {Amunts, Katrin and Ebell, Christoph and Muller, Jeff and Telefont, Martin and Knoll, Alois and Lippert, Thomas},
	journal = {Neuron},
	language = {en},
	month = nov,
	number = 3,
	pages = {574--581},
	title = {The Human Brain Project: Creating a European research infrastructure to decode the human brain},
	volume = 92,
	year = 2016}

@article{Dworkin2020-yx,
	abstract = {Similarly to many scientific disciplines, neuroscience has
              increasingly attempted to confront pervasive gender imbalances.
              Although publishing and conference participation are often
              highlighted, recent research has called attention to the
              prevalence of gender imbalance in citations. Because of the
              downstream effects of citations on visibility and career
              advancement, understanding the role of gender in citation
              practices is vital for addressing scientific inequity. Here, we
              investigate whether gendered patterns are present in neuroscience
              citations. Using data from five top neuroscience journals, we
              find that reference lists tend to include more papers with men as
              first and last author than would be expected if gender were
              unrelated to referencing. Importantly, we show that this
              imbalance is driven largely by the citation practices of men and
              is increasing over time as the field diversifies. We assess and
              discuss possible mechanisms and consider how researchers might
              approach these issues in their own work.},
	author = {Dworkin, Jordan D and Linn, Kristin A and Teich, Erin G and Zurn, Perry and Shinohara, Russell T and Bassett, Danielle S},
	journal = {Nat. Neurosci.},
	language = {en},
	month = aug,
	number = 8,
	pages = {918--926},
	title = {The extent and drivers of gender imbalance in neuroscience reference lists},
	volume = 23,
	year = 2020}

@article{Nosek2014-pg,
	author = {Nosek, Brian A and Lakens, Daniel},
	journal = {Social Psychology},
	number = 3,
	pages = {137--141},
	title = {Registered Reports: A method to increase the credibility of published reports},
	volume = 45,
	year = 2014}

@article{De_Jonge2021-jy,
	author = {de Jonge, Hans and Cruz, Maria and Holst, Stephanie},
	journal = {Nature},
	keywords = {Funding; Publishing; Research data},
	language = {en},
	month = nov,
	number = 7885,
	pages = {372},
	title = {Funders need to credit open science},
	volume = 599,
	year = 2021}

@article{Lustig2008-ke,
	author = {Lustig, M and Donoho, D L and Santos, J M and Pauly, J M},
	journal = {IEEE Signal Processing Magazine},
	month = mar,
	number = 2,
	pages = {72--82},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	title = {Compressed sensing {MRI}},
	volume = 25,
	year = 2008}

@article{Madan2021-bo,
	abstract = {We are now in a time of readily available brain imaging data. Not
              only are researchers now sharing data more than ever before, but
              additionally large-scale data collecting initiatives are underway
              with the vision that many future researchers will use the data
              for secondary analyses. Here I provide an overview of available
              datasets and some example use cases. Example use cases include
              examining individual differences, more robust findings,
              reproducibility-both in public input data and availability as a
              replication sample, and methods development. I further discuss a
              variety of considerations associated with using existing data and
              the opportunities associated with large datasets. Suggestions for
              further readings on general neuroimaging and topic-specific
              discussions are also provided.},
	author = {Madan, Christopher R},
	journal = {Neuroinformatics},
	keywords = {Brain imaging; Connectome; Functional connectivity; Naturalistic neuroimaging; Sample size; Secondary data; fMRI},
	language = {en},
	month = may,
	title = {Scan Once, Analyse Many: Using Large {Open-Access} Neuroimaging Datasets to Understand the Brain},
	year = 2021}

@article{Uecker2015-wg,
	abstract = {The Berkeley Advanced Reconstruction Toolbox (BART) is a
                framework for image reconstruction in Magnet Resonance Imaging
                (MRI). It consists of a programming library and a toolbox of
                command-line programs. The library provides common operations
                on multi-dimensional arrays, Fourier and wavelet transforms, as
                well as generic implementations of iterative optimization
                algorithms. The command-line tools provide direct access to
                basic operations on multi-dimensional arrays as well as
                efficient implementations of many calibration and
                reconstruction algorithms for MRI.},
	author = {Uecker, Martin and Ong, Frank and Tamir, Jonathan I and Bahri, Dara and Virtue, Patrick and Cheng, Joseph Y and Zhang, Tao and Lustig, Michael},
	conference = {Annual Meeting ISMRM},
	date-modified = {2022-08-01 14:12:53 +0300},
	journal = {Proc. of. the International Society for Magnetic Resonance in Medicine (ISMRM)},
	keywords = {Magnetic Resonance Imaging; image reconstruction; compressed sensing; parallel imaging; graphical processing unit},
	month = oct,
	title = {Berkeley Advanced Reconstruction Toolbox ({BART})},
	year = 2015}

@article{Wagenmakers2016-fo,
	abstract = {Psychological scientists Eric-Jan Wagenmakers and Gilles Dutilh
              present an illustrated guide to the career benefits of submitting
              your research plans before beginning your data collection.},
	author = {Wagenmakers, Eric-Jan and Dutilh, Gilles},
	journal = {APS Observer},
	month = oct,
	number = 9,
	title = {Seven selfish reasons for preregistration},
	volume = 29,
	year = 2016}

@article{Yarkoni2019-zm,
	author = {Yarkoni, Tal and Markiewicz, Christopher J and de la Vega, Alejandro and Gorgolewski, Krzysztof J and Salo, Taylor and Halchenko, Yaroslav O and McNamara, Quinten and DeStasio, Krista and Poline, Jean-Baptiste and Petrov, Dmitry and Hayot-Sasson, Val{\'e}rie and Nielson, Dylan M and Carlin, Johan and Kiar, Gregory and Whitaker, Kirstie and DuPre, Elizabeth and Wagner, Adina and Tirrell, Lee S and Jas, Mainak and Hanke, Michael and Poldrack, Russell A and Esteban, Oscar and Appelhoff, Stefan and Holdgraf, Chris and Staden, Isla and Thirion, Bertrand and Kleinschmidt, Dave F and Lee, John A and Visconti di Oleggio Castello, Matteo and Notter, Michael P and Blair, Ross},
	journal = {Journal of Open Source Software},
	language = {en},
	month = aug,
	number = 40,
	title = {{PyBIDS}: Python tools for {BIDS} datasets},
	volume = 4,
	year = 2019}

@article{Gorgolewski2017-xd,
	abstract = {The rate of progress in human neurosciences is limited by the
              inability to easily apply a wide range of analysis methods to the
              plethora of different datasets acquired in labs around the world.
              In this work, we introduce a framework for creating, testing,
              versioning and archiving portable applications for analyzing
              neuroimaging data organized and described in compliance with the
              Brain Imaging Data Structure (BIDS). The portability of these
              applications (BIDS Apps) is achieved by using container
              technologies that encapsulate all binary and other dependencies
              in one convenient package. BIDS Apps run on all three major
              operating systems with no need for complex setup and
              configuration and thanks to the comprehensiveness of the BIDS
              standard they require little manual user input. Previous
              containerized data processing solutions were limited to single
              user environments and not compatible with most multi-tenant High
              Performance Computing systems. BIDS Apps overcome this limitation
              by taking advantage of the Singularity container technology. As a
              proof of concept, this work is accompanied by 22 ready to use
              BIDS Apps, packaging a diverse set of commonly used neuroimaging
              algorithms.},
	author = {Gorgolewski, Krzysztof J and Alfaro-Almagro, Fidel and Auer, Tibor and Bellec, Pierre and Capot{\u a}, Mihai and Chakravarty, M Mallar and Churchill, Nathan W and Cohen, Alexander Li and Craddock, R Cameron and Devenyi, Gabriel A and Eklund, Anders and Esteban, Oscar and Flandin, Guillaume and Ghosh, Satrajit S and Guntupalli, J Swaroop and Jenkinson, Mark and Keshavan, Anisha and Kiar, Gregory and Liem, Franziskus and Raamana, Pradeep Reddy and Raffelt, David and Steele, Christopher J and Quirion, Pierre-Olivier and Smith, Robert E and Strother, Stephen C and Varoquaux, Ga{\"e}l and Wang, Yida and Yarkoni, Tal and Poldrack, Russell A},
	journal = {PLoS Computational Biology},
	language = {en},
	month = mar,
	number = 3,
	pages = {e1005209},
	title = {{BIDS} apps: Improving ease of use, accessibility, and reproducibility of neuroimaging data analysis methods},
	volume = 13,
	year = 2017}

@unpublished{Malkinson2021-tg,
	abstract = {ABSTRACTEditorial decision-making is a fundamental element of the
              scientific enterprise. We examined whether contributions to
              editorial decisions at various stages of the publication process
              is subject to gender disparity, based on analytics collected by
              the biomedical researcher-led journal eLife. Despite efforts to
              increase women representation, the board of reviewing editors
              (BRE) was men-dominant (69\%). Moreover, authors suggested more
              men from the BRE pool, even after correcting for men's numerical
              over-representation. Although women editors were proportionally
              involved in the initial editorial process, they were
              under-engaged in editorial activities involving reviewers and
              authors. Additionally, converging evidence showed gender
              homophily in manuscripts assignment, such that men Senior Editors
              over-engaged men Reviewing Editors. This tendency was stronger in
              more gender-balanced scientific disciplines. Together, our
              findings confirm that gender disparities exist along the
              editorial process and suggest that merely increasing the
              proportion of women might not be sufficient to eliminate this
              bias.},
	author = {Malkinson, Tal Seidel and Terhune, Devin B and Kollamkulam, Mathew and Guerreiro, Maria J and Bassett, Dani S and Makin, Tamar R},
	journal = {bioRxiv},
	month = nov,
	title = {Gender imbalance in the editorial activities of a researcher-led journal},
	year = 2021}

@inproceedings{Di_Cosmo2018-rb,
	abstract = {Software is at the heart of our digital society. It powers our
                industries, fuels innovation, mediates access to all digital
                information, is a pillar of modern scientific research, and has
                enabled the emergence of new forms of social and political
                organizations-``code is law'', as Lessig said [2]. The source
                code of this software is a unique form of knowledge: it is
                designed to be read by humans (the developers), and yet it is
                ready to be translated into an executable form for a machine.
                As Len Shustek puts it, ``Source code provides a view into the
                mind of the designer'' [3]. Software source code is precious,
                and embodies a growing part of our scientific, technical and
                organisational knowledge. Software Heritage is an open,
                non-profit initiative whose mission is to ensure that this
                precious body of knowledge will be preserved over time and made
                available to all.We do this for multiple reasons. To preserve
                the scientific and technological knowledge embedded in software
                source code, that is a precious part of our heritage. To allow
                better software development and reuse for society and industry,
                by building the largest and open software knowledge database,
                enabling the development of a broad range of value added
                applications.},
	author = {Di Cosmo, Roberto},
	conference = {International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS)},
	keywords = {Software;Software engineering;History;Law;Heart;Fuels;Technological innovation},
	month = may,
	pages = {2--2},
	title = {Software Heritage: Why and How We Collect, Preserve and Share All the Software Source Code},
	year = 2018}

@article{Moreau2020-fe,
	abstract = {Metadata are what makes databases searchable. Without them,
              researchers would have difficulty finding data with features they
              are interested in. Brain imaging genetics is at the intersection
              of two disciplines, each with dedicated dictionaries and
              ontologies facilitating data search and analysis. Here, we
              present the genetics Brain Imaging Data Structure extension,
              consisting of metadata files for human brain imaging data to
              which they are linked, and describe succinctly the genomic and
              transcriptomic data associated with them, which may be in
              different databases. This extension will facilitate identifying
              micro-scale molecular features that are linked to macro-scale
              imaging repositories, facilitating data aggregation across
              studies.},
	author = {Moreau, Clara A and Jean-Louis, Martineau and Blair, Ross and Markiewicz, Christopher J and Turner, Jessica A and Calhoun, Vince D and Nichols, Thomas E and Pernet, Cyril R},
	journal = {Gigascience},
	keywords = {Brain Imaging Data Structure; genomics; human brain imaging; transcriptomics},
	language = {en},
	month = oct,
	number = 10,
	title = {The {genetics-BIDS} extension: Easing the search for genetic data associated with human brain imaging},
	volume = 9,
	year = 2020}

@article{Eke2022-nc,
	author = {Eke, Damian and Bernard, Amy and Bjaalie, Jan G and Chavarriaga, Ricardo and Hanakawa, Takashi and Hannan, Anthony and Hill, Sean and Martone, Maryann Elizabeth and McMahon, Agnes and Ruebel, Oliver and Thiels, Edda and Pestilli, Franco},
	journal = {Neuron},
	number = 4,
	pages = {600--612},
	title = {International Data Governance for Neuroscience},
	volume = 110,
	year = 2022}

@article{Churchill2012-sr,
	abstract = {Subject-specific artifacts caused by head motion and
              physiological noise are major confounds in BOLD fMRI analyses.
              However, there is little consensus on the optimal choice of data
              preprocessing steps to minimize these effects. To evaluate the
              effects of various preprocessing strategies, we present a
              framework which comprises a combination of (1) nonparametric
              testing including reproducibility and prediction metrics of the
              data-driven NPAIRS framework (Strother et al. [2002]: NeuroImage
              15:747-771), and (2) intersubject comparison of SPM effects,
              using DISTATIS (a three-way version of metric multidimensional
              scaling (Abdi et al. [2009]: NeuroImage 45:89-95). It is shown
              that the quality of brain activation maps may be significantly
              limited by sub-optimal choices of data preprocessing steps (or
              ``pipeline'') in a clinical task-design, an fMRI adaptation of
              the widely used Trail-Making Test. The relative importance of
              motion correction, physiological noise correction, motion
              parameter regression, and temporal detrending were examined for
              fMRI data acquired in young, healthy adults. Analysis performance
              and the quality of activation maps were evaluated based on
              Penalized Discriminant Analysis (PDA). The relative importance of
              different preprocessing steps was assessed by (1) a nonparametric
              Friedman rank test for fixed sets of preprocessing steps, applied
              to all subjects; and (2) evaluating pipelines chosen specifically
              for each subject. Results demonstrate that preprocessing choices
              have significant, but subject-dependant effects, and that
              individually-optimized pipelines may significantly improve the
              reproducibility of fMRI results over fixed pipelines. This was
              demonstrated by the detection of a significant interaction with
              motion parameter regression and physiological noise correction,
              even though the range of subject head motion was small across the
              group (≪ 1 voxel). Optimizing pipelines on an individual-subject
              basis also revealed brain activation patterns either weak or
              absent under fixed pipelines, which has implications for the
              overall interpretation of fMRI data, and the relative importance
              of preprocessing methods.},
	author = {Churchill, Nathan W and Oder, Anita and Abdi, Herv{\'e} and Tam, Fred and Lee, Wayne and Thomas, Christopher and Ween, Jon E and Graham, Simon J and Strother, Stephen C},
	journal = {Human Brain Mapping},
	language = {en},
	month = mar,
	number = 3,
	pages = {609--627},
	title = {Optimizing preprocessing and analysis pipelines for single-subject {fMRI}. I. Standard temporal motion and physiological noise correction methods},
	volume = 33,
	year = 2012}

@article{Niso2019-nr,
	abstract = {We present a simple, reproducible analysis pipeline applied to
              resting-state magnetoencephalography (MEG) data from the Open MEG
              Archive (OMEGA). The data workflow was implemented with
              Brainstorm, which like OMEGA is free and openly accessible. The
              proposed pipeline produces group maps of ongoing brain activity
              decomposed in the typical frequency bands of electrophysiology.
              The procedure is presented as a technical proof of concept for
              streamlining a broader range and more sophisticated studies of
              resting-state electrophysiological data. It also features the
              recently introduced extension of the brain imaging data structure
              (BIDS) to MEG data, highlighting the scalability and
              generalizability of Brainstorm analytical pipelines to other, and
              potentially larger data volumes.},
	author = {Niso, Guiomar and Tadel, Francois and Bock, Elizabeth and Cousineau, Martin and Santos, Andr{\'e}s and Baillet, Sylvain},
	journal = {Front. Neurosci.},
	keywords = {MEG-BIDS; analytical pipelines; magnetoencephalography; open data; open science; power spectral density; reproducibility; resting-state},
	language = {en},
	month = apr,
	pages = {284},
	title = {Brainstorm Pipeline Analysis of {Resting-State} Data From the Open {MEG} Archive},
	volume = 13,
	year = 2019}

@article{Smith2013-ce,
	abstract = {Resting-state functional magnetic resonance imaging (rfMRI)
              allows one to study functional connectivity in the brain by
              acquiring fMRI data while subjects lie inactive in the MRI
              scanner, and taking advantage of the fact that functionally
              related brain regions spontaneously co-activate. rfMRI is one of
              the two primary data modalities being acquired for the Human
              Connectome Project (the other being diffusion MRI). A key
              objective is to generate a detailed in vivo mapping of functional
              connectivity in a large cohort of healthy adults (over 1000
              subjects), and to make these datasets freely available for use by
              the neuroimaging community. In each subject we acquire a total of
              1h of whole-brain rfMRI data at 3 T, with a spatial resolution of
              2$\times$2$\times$2 mm and a temporal resolution of 0.7s,
              capitalizing on recent developments in slice-accelerated
              echo-planar imaging. We will also scan a subset of the cohort at
              higher field strength and resolution. In this paper we outline
              the work behind, and rationale for, decisions taken regarding the
              rfMRI data acquisition protocol and pre-processing pipelines, and
              present some initial results showing data quality and example
              functional connectivity analyses.},
	author = {Smith, Stephen M and Beckmann, Christian F and Andersson, Jesper and Auerbach, Edward J and Bijsterbosch, Janine and Douaud, Gwena{\"e}lle and Duff, Eugene and Feinberg, David A and Griffanti, Ludovica and Harms, Michael P and Kelly, Michael and Laumann, Timothy and Miller, Karla L and Moeller, Steen and Petersen, Steve and Power, Jonathan and Salimi-Khorshidi, Gholamreza and Snyder, Abraham Z and Vu, An T and Woolrich, Mark W and Xu, Junqian and Yacoub, Essa and U{\u g}urbil, Kamil and Van Essen, David C and Glasser, Matthew F and {WU-Minn HCP Consortium}},
	journal = {Neuroimage},
	language = {en},
	month = oct,
	pages = {144--168},
	title = {Resting-state {fMRI} in the Human Connectome Project},
	volume = 80,
	year = 2013}

@article{Schreiweis2019-yc,
	author = {Schreiweis, Christiane and Volle, Emmanuelle and Durr, Alexandra and Auffret, Alexandra and Delarasse, C{\'e}cile and George, Nathalie and Dumont, Magali and Hassan, Bassem A and Renier, Nicolas and Rosso, Charlotte and Thiebaut de Schotten, Michel and Burgui{\`e}re, Eric and Zujovic, Violetta},
	journal = {Nature Human Behaviour},
	language = {en},
	month = sep,
	number = 12,
	pages = {1238--1239},
	publisher = {Nature Publishing Group},
	title = {A neuroscientific approach to increase gender equality},
	volume = 3,
	year = 2019}

@article{Rubin2020-ox,
	author = {Rubin, Mark},
	journal = {The Quantitative Methods in Psychology},
	number = 4,
	pages = {376--390},
	title = {Does preregistration improve the credibility of research findings?},
	volume = 16,
	year = 2020}

@article{Casadevall2014-wf,
	abstract = {ABSTRACT Numerous essays have addressed the misuse of the journal
              impact factor for judging the value of science, but the practice
              continues, primarily as a result of the actions of scientists
              themselves. This seemingly irrational behavior is referred to as
              ``impact factor mania.'' Although the literature on the impact
              factor is extensive, little has been written on the underlying
              causes of impact factor mania. In this perspective, we consider
              the reasons for the persistence of impact factor mania and its
              pernicious effects on science. We conclude that impact factor
              mania persists because it confers significant benefits to
              individual scientists and journals. Impact factor mania is a
              variation of the economic theory known as the ``tragedy of the
              commons,'' in which scientists act rationally in their own
              self-interests despite the detrimental consequences of their
              actions on the overall scientific enterprise. Various measures to
              reduce the influence of the impact factor are considered.
              IMPORTANCE Science and scientists are currently afflicted by an
              epidemic of mania manifested by associating the value of research
              with the journal where the work is published rather than the
              content of the work itself. The mania is causing profound
              distortions in the way science is done that are deleterious to
              the overall scientific enterprise. In this essay, we consider the
              forces responsible for the persistence of the mania and conclude
              that it is maintained because it disproportionately benefits
              elements of the scientific enterprise, including certain
              well-established scientists, journals, and administrative
              interests. Our essay suggests steps that can be taken to deal
              with this debilitating and destructive epidemic.},
	author = {Casadevall, Arturo and Fang, Ferric C},
	journal = {MBio},
	language = {en},
	month = mar,
	number = 2,
	pages = {e00064--14},
	title = {Causes for the persistence of impact factor mania},
	volume = 5,
	year = 2014}

@article{Churchill2012-pi,
	abstract = {A variety of preprocessing techniques are available to correct
              subject-dependant artifacts in fMRI, caused by head motion and
              physiological noise. Although it has been established that the
              chosen preprocessing steps (or ``pipeline'') may significantly
              affect fMRI results, it is not well understood how preprocessing
              choices interact with other parts of the fMRI experimental
              design. In this study, we examine how two experimental factors
              interact with preprocessing: between-subject heterogeneity, and
              strength of task contrast. Two levels of cognitive contrast were
              examined in an fMRI adaptation of the Trail-Making Test, with
              data from young, healthy adults. The importance of standard
              preprocessing with motion correction, physiological noise
              correction, motion parameter regression and temporal detrending
              were examined for the two task contrasts. We also tested subspace
              estimation using Principal Component Analysis (PCA), and
              Independent Component Analysis (ICA). Results were obtained for
              Penalized Discriminant Analysis, and model performance quantified
              with reproducibility (R) and prediction metrics (P). Simulation
              methods were also used to test for potential biases from
              individual-subject optimization. Our results demonstrate that (1)
              individual pipeline optimization is not significantly more biased
              than fixed preprocessing. In addition, (2) when applying a fixed
              pipeline across all subjects, the task contrast significantly
              affects pipeline performance; in particular, the effects of PCA
              and ICA models vary with contrast, and are not by themselves
              optimal preprocessing steps. Also, (3) selecting the optimal
              pipeline for each subject improves within-subject (P,R) and
              between-subject overlap, with the weaker cognitive contrast being
              more sensitive to pipeline optimization. These results
              demonstrate that sensitivity of fMRI results is influenced not
              only by preprocessing choices, but also by interactions with
              other experimental design factors. This paper outlines a
              quantitative procedure to denoise data that would otherwise be
              discarded due to artifact; this is particularly relevant for weak
              signal contrasts in single-subject, small-sample and clinical
              datasets.},
	author = {Churchill, Nathan W and Yourganov, Grigori and Oder, Anita and Tam, Fred and Graham, Simon J and Strother, Stephen C},
	journal = {PLoS One},
	language = {en},
	month = feb,
	number = 2,
	pages = {e31147},
	title = {Optimizing preprocessing and analysis pipelines for single-subject {fMRI}: 2. Interactions with {ICA}, {PCA}, task contrast and inter-subject heterogeneity},
	volume = 7,
	year = 2012}

@misc{Strand2021-xt,
	author = {Strand, Julia Feld},
	publisher = {OSF},
	title = {Error tight: Exercises for lab groups to prevent research mistakes},
	year = 2021}

@article{Soskic2021-wv,
	author = {{\v S}o{\v s}ki{\'c}, An{\dj}ela and Jovanovi{\'c}, Vojislav and Styles, Suzy J and Kappenman, Emily S and Kovi{\'c}, Vanja},
	journal = {Neuropsychology Review},
	title = {How to do better {N400} studies: Reproducibility, consistency and adherence to research standards in the existing literature},
	year = 2021}

@article{Hall2022-hn,
	abstract = {Abstract Analysing data from experiments is a complex,
               multi-step process, often with multiple defensible choices
               available at each step. While analysts often report a single
               analysis without documenting how it was chosen, this can cause
               serious transparency and methodological issues. To make the
               sensitivity of analysis results to analytical choices
               transparent, some statisticians and methodologists advocate the
               use of ?multiverse analysis?: reporting the full range of
               outcomes that result from all combinations of defensible
               analytic choices. Summarizing this combinatorial explosion of
               statistical results presents unique challenges; several
               approaches to visualizing the output of multiverse analyses have
               been proposed across a variety of fields (e.g. psychology,
               statistics, economics, neuroscience). In this article, we (1)
               introduce a consistent conceptual framework and terminology for
               multiverse analyses that can be applied across fields; (2)
               identify the tasks researchers try to accomplish when
               visualizing multiverse analyses and (3) classify multiverse
               visualizations into ?archetypes?, assessing how well each
               archetype supports each task. Our work sets a foundation for
               subsequent research on developing visualization tools and
               techniques to support multiverse analysis and its reporting.},
	author = {Hall, Brian D and Liu, Yang and Jansen, Yvonne and Dragicevic, Pierre and Chevalier, Fanny and Kay, Matthew},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	journal = {Computer Graphics Forum},
	language = {en},
	month = feb,
	publisher = {Wiley},
	title = {A survey of tasks and visualizations in multiverse analysis reports},
	year = 2022}

@article{Cordes2020-kt,
	abstract = {PURPOSE: To introduce a new sequence description format for
              vendor-independent MR sequences that include all calculation
              logic portably. To introduce a new MRI sequence development
              approach which utilizes flexibly reusable modules. METHODS: The
              proposed sequence description contains a sequence module
              hierarchy for loop and group logic, which is enhanced by a novel
              strategy for performing efficient parameter and pulse shape
              calculation. These calculations are powered by a flow graph
              structure. By using the flow graph, all calculations are
              performed with no redundancy and without requiring preprocessing.
              The generation of this interpretable structure is a separate step
              that combines MRI techniques while actively considering their
              context. The driver interface is slim and highly flexible through
              scripting support. The sequences do not require any
              vendor-specific compiling or processing step. A
              vendor-independent frontend for sequence configuration can be
              used. Tests that ensure physical feasibility of the sequence are
              integrated into the calculation logic. RESULTS: The framework was
              used to define a set of standard sequences. Resulting images were
              compared to respective images acquired with sequences provided by
              the device manufacturer. Images were acquired using a standard
              commercial MRI system. CONCLUSIONS: The approach produces
              configurable, vendor-independent sequences, whose configurability
              enables rapid prototyping. The transparent data structure
              simplifies the process of sharing reproducible sequences,
              modules, and techniques.},
	author = {Cordes, Cristoffer and Konstandin, Simon and Porter, David and G{\"u}nther, Matthias},
	journal = {Magnetic Resonance in Medicine},
	keywords = {high performance computing; modular MR sequence development; platform-independent pulse sequence programming; portable; reproducible; vendor-independent MRI},
	language = {en},
	month = apr,
	number = 4,
	pages = {1277--1290},
	title = {Portable and platform-independent {MR} pulse sequence programs},
	volume = 83,
	year = 2020}

@article{Strother2006-np,
	author = {Strother, Stephen C},
	journal = {IEEE Engineering in Medicine and Biology},
	language = {en},
	month = mar,
	number = 2,
	pages = {27--41},
	title = {Evaluating {fMRI} preprocessing pipelines},
	volume = 25,
	year = 2006}

@article{Cox1997-nh,
	abstract = {The tools needed for analysis and visualization of
              three-dimensional human brain functional magnetic resonance image
              results are outlined, covering the processing categories of data
              storage, interactive vs batch mode operations, visualization,
              spatial normalization (Talairach coordinates, etc.), analysis of
              functional activation, integration of multiple datasets, and
              interface standards. One freely available software package is
              described in some detail. The features and scope that a generally
              useful and extensible fMRI toolset should have are contrasted
              with what is available today. The article ends with a discussion
              of how the fMRI research community can cooperate to create
              standards and develop software that meets the community's needs.},
	author = {Cox, R W and Hyde, J S},
	journal = {NMR in Biomedicine},
	language = {en},
	month = jun,
	number = {4-5},
	pages = {171--178},
	title = {Software tools for analysis and visualization of {fMRI} data},
	volume = 10,
	year = 1997}

@article{Layton2017-kx,
	abstract = {PURPOSE: Implementing new magnetic resonance experiments, or
              sequences, often involves extensive programming on
              vendor-specific platforms, which can be time consuming and
              costly. This situation is exacerbated when research sequences
              need to be implemented on several platforms simultaneously, for
              example, at different field strengths. This work presents an
              alternative programming environment that is hardware-independent,
              open-source, and promotes rapid sequence prototyping. METHODS: A
              novel file format is described to efficiently store the hardware
              events and timing information required for an MR pulse sequence.
              Platform-dependent interpreter modules convert the file to
              appropriate instructions to run the sequence on MR hardware.
              Sequences can be designed in high-level languages, such as
              MATLAB, or with a graphical interface. Spin physics simulation
              tools are incorporated into the framework, allowing for
              comparison between real and virtual experiments. RESULTS: Minimal
              effort is required to implement relatively advanced sequences
              using the tools provided. Sequences are executed on three
              different MR platforms, demonstrating the flexibility of the
              approach. CONCLUSION: A high-level, flexible and
              hardware-independent approach to sequence programming is ideal
              for the rapid development of new sequences. The framework is
              currently not suitable for large patient studies or routine
              scanning although this would be possible with deeper integration
              into existing workflows. Magn Reson Med 77:1544-1552, 2017.
              \copyright{} 2016 International Society for Magnetic Resonance in
              Medicine.},
	author = {Layton, Kelvin J and Kroboth, Stefan and Jia, Feng and Littin, Sebastian and Yu, Huijun and Leupold, Jochen and Nielsen, Jon-Fredrik and St{\"o}cker, Tony and Zaitsev, Maxim},
	journal = {Magnetic Resonance in Medicine},
	keywords = {Pulseq; open-source; platform independent; pulse sequence programming; rapid development},
	language = {en},
	month = apr,
	number = 4,
	pages = {1544--1552},
	title = {Pulseq: A rapid and hardware-independent pulse sequence prototyping framework},
	volume = 77,
	year = 2017}

@article{Gorgolewski2016-dl,
	abstract = {The development of magnetic resonance imaging (MRI) techniques
              has defined modern neuroimaging. Since its inception, tens of
              thousands of studies using techniques such as functional MRI and
              diffusion weighted imaging have allowed for the non-invasive
              study of the brain. Despite the fact that MRI is routinely used
              to obtain data for neuroscience research, there has been no
              widely adopted standard for organizing and describing the data
              collected in an imaging experiment. This renders sharing and
              reusing data (within or between labs) difficult if not impossible
              and unnecessarily complicates the application of automatic
              pipelines and quality assurance protocols. To solve this problem,
              we have developed the Brain Imaging Data Structure (BIDS), a
              standard for organizing and describing MRI datasets. The BIDS
              standard uses file formats compatible with existing software,
              unifies the majority of practices already common in the field,
              and captures the metadata necessary for most common data
              processing operations.},
	author = {Gorgolewski, Krzysztof J and Auer, Tibor and Calhoun, Vince D and Craddock, R Cameron and Das, Samir and Duff, Eugene P and Flandin, Guillaume and Ghosh, Satrajit S and Glatard, Tristan and Halchenko, Yaroslav O and Handwerker, Daniel A and Hanke, Michael and Keator, David and Li, Xiangrui and Michael, Zachary and Maumet, Camille and Nichols, B Nolan and Nichols, Thomas E and Pellman, John and Poline, Jean Baptiste and Rokem, Ariel and Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and Turner, Jessica A and Varoquaux, Ga{\"e}l and Poldrack, Russell A},
	journal = {Scientific Data},
	pages = {1--9},
	title = {The brain imaging data structure: A format for organizing and describing outputs of neuroimaging experiments},
	volume = 3,
	year = 2016}

@inproceedings{Craddock2013-xj,
	author = {Craddock, Cameron and Sikka, Sharad and Cheung, Brian and Khanuja, Ranjeet and Ghosh, Satrajit S and Yan, Chaogan and Li, Qingyang and Lurie, Daniel and Vogelstein, Joshua and Burns, Randal and {Others}},
	booktitle = {Frontiers Neuroinformatics},
	conference = {Neuroinformatics},
	pages = {10--3389},
	title = {Towards automated analysis of connectomes: The configurable pipeline for the analysis of connectomes (c-pac)},
	volume = 42,
	year = 2013}

@article{Yucel2021-pk,
	abstract = {The application of functional near-infrared spectroscopy (fNIRS)
              in the neurosciences has been expanding over the last 40 years.
              Today, it is addressing a wide range of applications within
              different populations and utilizes a great variety of
              experimental paradigms. With the rapid growth and the
              diversification of research methods, some inconsistencies are
              appearing in the way in which methods are presented, which can
              make the interpretation and replication of studies unnecessarily
              challenging. The Society for Functional Near-Infrared
              Spectroscopy has thus been motivated to organize a representative
              (but not exhaustive) group of leaders in the field to build a
              consensus on the best practices for describing the methods
              utilized in fNIRS studies. Our paper has been designed to provide
              guidelines to help enhance the reliability, repeatability, and
              traceability of reported fNIRS studies and encourage best
              practices throughout the community. A checklist is provided to
              guide authors in the preparation of their manuscripts and to
              assist reviewers when evaluating fNIRS papers.},
	author = {Y{\"u}cel, Meryem A and L{\"u}hmann, Alexander V and Scholkmann, Felix and Gervain, Judit and Dan, Ippeita and Ayaz, Hasan and Boas, David and Cooper, Robert J and Culver, Joseph and Elwell, Clare E and Eggebrecht, Adam and Franceschini, Maria A and Grova, Christophe and Homae, Fumitaka and Lesage, Fr{\'e}d{\'e}ric and Obrig, Hellmuth and Tachtsidis, Ilias and Tak, Sungho and Tong, Yunjie and Torricelli, Alessandro and Wabnitz, Heidrun and Wolf, Martin},
	journal = {Neurophotonics},
	keywords = {functional near-infrared spectroscopy; guidelines; publication best practices},
	language = {en},
	month = jan,
	number = 1,
	pages = {012101},
	title = {Best practices for {fNIRS} publications},
	volume = 8,
	year = 2021}

@article{Niso2016-gh,
	abstract = {In contrast with other imaging modalities, there is presently a
              scarcity of fully open resources in magnetoencephalography (MEG)
              available to the neuroimaging community. Here we present a
              collaborative effort led by the McConnell Brain Imaging Centre of
              the Montreal Neurological Institute, and the Universit{\'e} de
              Montr{\'e}al to build and share a centralised repository to
              curate MEG data in raw and processed form for open dissemination.
              The Open MEG Archive (OMEGA, omega.bic.mni.mcgill.ca) is bound to
              become a continuously expanding repository of multimodal data
              with a primary focus on MEG, in addition to storing anatomical
              MRI volumes, demographic participant data and questionnaires, and
              other forms of electrophysiological data such as EEG. The OMEGA
              initiative offers both the technological framework for multi-site
              MEG data aggregation, and serves as one of the largest freely
              available resting-state and eventually task-related MEG datasets
              presently available.},
	author = {Niso, Guiomar and Rogers, Christine and Moreau, Jeremy T and Chen, Li-Yuan and Madjar, Cecile and Das, Samir and Bock, Elizabeth and Tadel, Fran{\c c}ois and Evans, Alan C and Jolicoeur, Pierre and Baillet, Sylvain},
	journal = {Neuroimage},
	keywords = {Database; EEG; Electrophysiology; MEG; MRI; Open-access},
	language = {en},
	month = jan,
	number = {Pt B},
	pages = {1182--1187},
	title = {{OMEGA}: The Open {MEG} Archive},
	volume = 124,
	year = 2016}

@misc{Meyer2021-hb,
	author = {Meyer, Kyle and Hanke, Michael and Halchenko, Yaroslav and Poldrack, Benjamin and Wagner, Adina},
	month = apr,
	title = {datalad/datalad-container: 1.1.4},
	year = 2021}

@article{Hanke2009-gp,
	abstract = {Decoding patterns of neural activity onto cognitive states is one
              of the central goals of functional brain imaging. Standard
              univariate fMRI analysis methods, which correlate cognitive and
              perceptual function with the blood oxygenation-level dependent
              (BOLD) signal, have proven successful in identifying anatomical
              regions based on signal increases during cognitive and perceptual
              tasks. Recently, researchers have begun to explore new
              multivariate techniques that have proven to be more flexible,
              more reliable, and more sensitive than standard univariate
              analysis. Drawing on the field of statistical learning theory,
              these new classifier-based analysis techniques possess
              explanatory power that could provide new insights into the
              functional properties of the brain. However, unlike the wealth of
              software packages for univariate analyses, there are few packages
              that facilitate multivariate pattern classification analyses of
              fMRI data. Here we introduce a Python-based, cross-platform, and
              open-source software toolbox, called PyMVPA, for the application
              of classifier-based analysis techniques to fMRI datasets. PyMVPA
              makes use of Python's ability to access libraries written in a
              large variety of programming languages and computing environments
              to interface with the wealth of existing machine learning
              packages. We present the framework in this paper and provide
              illustrative examples on its usage, features, and
              programmability.},
	author = {Hanke, Michael and Halchenko, Yaroslav O and Sederberg, Per B and Hanson, Stephen Jos{\'e} and Haxby, James V and Pollmann, Stefan},
	journal = {Neuroinformatics},
	language = {en},
	month = jan,
	number = 1,
	pages = {37--53},
	title = {{PyMVPA}: A python toolbox for multivariate pattern analysis of {fMRI} data},
	volume = 7,
	year = 2009}

@article{Wettenhovi2021-dr,
	author = {Wettenhovi, V-V and Vauhkonen, M and Kolehmainen, V},
	journal = {Physics in Medicine and Biology},
	language = {en},
	number = 6,
	title = {{OMEGA}: open-source emission tomography software},
	volume = 66,
	year = 2021}

@article{Esteban2020-bc,
	abstract = {Functional magnetic resonance imaging (fMRI) is a standard tool
              to investigate the neural correlates of cognition. fMRI
              noninvasively measures brain activity, allowing identification of
              patterns evoked by tasks performed during scanning. Despite the
              long history of this technique, the idiosyncrasies of each
              dataset have led to the use of ad-hoc preprocessing protocols
              customized for nearly every different study. This approach is
              time consuming, error prone and unsuitable for combining datasets
              from many sources. Here we showcase fMRIPrep
              (http://fmriprep.org), a robust tool to prepare human fMRI data
              for statistical analysis. This software instrument addresses the
              reproducibility concerns of the established protocols for fMRI
              preprocessing. By leveraging the Brain Imaging Data Structure to
              standardize both the input datasets (MRI data as stored by the
              scanner) and the outputs (data ready for modeling and analysis),
              fMRIPrep is capable of preprocessing a diversity of datasets
              without manual intervention. In support of the growing popularity
              of fMRIPrep, this protocol describes how to integrate the tool in
              a task-based fMRI investigation workflow.},
	author = {Esteban, Oscar and Ciric, Rastko and Finc, Karolina and Blair, Ross W and Markiewicz, Christopher J and Moodie, Craig A and Kent, James D and Goncalves, Mathias and DuPre, Elizabeth and Gomez, Daniel E P and Ye, Zhifang and Salo, Taylor and Valabregue, Romain and Amlien, Inge K and Liem, Franziskus and Jacoby, Nir and Stoji{\'c}, Hrvoje and Cieslak, Matthew and Urchs, Sebastian and Halchenko, Yaroslav O and Ghosh, Satrajit S and De La Vega, Alejandro and Yarkoni, Tal and Wright, Jessey and Thompson, William H and Poldrack, Russell A and Gorgolewski, Krzysztof J},
	journal = {Nature Protocols},
	language = {en},
	month = jul,
	number = 7,
	pages = {2186--2202},
	title = {Analysis of task-based functional {MRI} data preprocessed with {fMRIPrep}},
	volume = 15,
	year = 2020}

@article{Nabyonga-Orem2020-vt,
	author = {Nabyonga-Orem, Juliet and Asamani, James Avoka and Nyirenda, Thomas and Abimbola, Seye},
	journal = {BMJ Glob Health},
	keywords = {health policy; health services research; health systems},
	language = {en},
	month = sep,
	number = 9,
	title = {Article processing charges are stalling the progress of African researchers: a call for urgent reforms},
	volume = 5,
	year = 2020}

@article{Liu2021-up,
	abstract = {Multiverse analysis is an approach to data analysis in which all
              ``reasonable'' analytic decisions are evaluated in parallel and
              interpreted collectively, in order to foster robustness and
              transparency. However, specifying a multiverse is demanding
              because analysts must manage myriad variants from a cross-product
              of analytic decisions, and the results require nuanced
              interpretation. We contribute Baba: an integrated domain-specific
              language (DSL) and visual analysis system for authoring and
              reviewing multiverse analyses. With the Boba DSL, analysts write
              the shared portion of analysis code only once, alongside local
              variations defining alternative decisions, from which the
              compiler generates a multiplex of scripts representing all
              possible analysis paths. The Boba Visualizer provides linked
              views of model results and the multiverse decision space to
              enable rapid, systematic assessment of consequential decisions
              and robustness, including sampling uncertainty and model fit. We
              demonstrate Boba's utility through two data analysis case
              studies, and reflect on challenges and design opportunities for
              multiverse analysis software.},
	author = {Liu, Yang and Kale, Alex and Althoff, Tim and Heer, Jeffrey},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	language = {en},
	month = feb,
	number = 2,
	pages = {1753--1763},
	title = {Boba: Authoring and visualizing multiverse analyses},
	volume = 27,
	year = 2021}

@article{Henson2019-hc,
	abstract = {We describe the steps involved in analysis of multi-modal,
              multi-subject human neuroimaging data using the SPM12 free and
              open source software (https://www.fil.ion.ucl.ac.uk/spm/) and a
              publically-available dataset organized according to the Brain
              Imaging Data Structure (BIDS) format
              (https://openneuro.org/datasets/ds000117/). The dataset contains
              electroencephalographic (EEG), magnetoencephalographic (MEG), and
              functional and structural magnetic resonance imaging (MRI) data
              from 16 subjects who undertook multiple runs of a simple task
              performed on a large number of famous, unfamiliar and scrambled
              faces. We demonstrate: (1) batching and scripting of
              preprocessing of multiple runs/subjects of combined MEG and EEG
              data, (2) creation of trial-averaged evoked responses, (3)
              source-reconstruction of the power (induced and evoked) across
              trials within a time-frequency window around the ``N/M170''
              evoked component, using structural MRI for forward modeling and
              simultaneous inversion (fusion) of MEG and EEG data, (4)
              group-based optimisation of spatial priors during M/EEG source
              reconstruction using fMRI data on the same paradigm, and (5)
              statistical mapping across subjects of cortical source power
              increases for faces vs. scrambled faces.},
	author = {Henson, Richard N and Abdulrahman, Hunar and Flandin, Guillaume and Litvak, Vladimir},
	journal = {Front. Neurosci.},
	keywords = {EEG; MEG; SPM; fMRI; faces; fusion; inversion; multimodal},
	language = {en},
	month = apr,
	pages = {300},
	title = {Multimodal Integration of {M/EEG} and {f/MRI} Data in {SPM12}},
	volume = 13,
	year = 2019}

@article{Ravi2019-mi,
	author = {Ravi, Keerthi and Geethanath, Sairam and Vaughan, John},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	journal = {Journal of Open Source Software},
	month = oct,
	number = 42,
	pages = {1725},
	publisher = {The Open Journal},
	title = {{PyPulseq}: A python package for {MRI} pulse sequence design},
	volume = 4,
	year = 2019}

@misc{Kiar2021-cw,
	author = {Kiar, Greg and Chatelain, Yohan and Glatard, Tristan and Salari, Ali and de Oliveira Castro, Pablo and {michaelnicht} and H{\'e}bert, Antoine and Vadariya, Mayank},
	month = jun,
	title = {verificarlo/fuzzy: Fuzzy v0.5.0},
	year = 2021}

@misc{Visconti_di_Oleggio_Castello2020-pm,
	author = {Visconti di Oleggio Castello, Matteo and Dobson, James E and Sackett, Terry and Kodiweera, Chandana and Haxby, James V and Goncalves, Mathias and Ghosh, Satrajit and Halchenko, Yaroslav O},
	month = jan,
	title = {{ReproNim/reproin} 0.6.0},
	year = 2020}

@article{Simonsohn2015-lm,
	abstract = {Empirical results often hinge on data analytic decisions that are
              simultaneously defensible, arbitrary, and motivated. To mitigate
              this problem we introduce Specification-Curve Analysis, which
              consists of three steps: (i) identifying the set of theoretically
              justified, statistically valid, and non-redundant analytic
              specifications, (ii) displaying alternative results graphically,
              allowing the identification of decisions producing different
              results, and (iii) conducting statistical tests to determine
              whether as a whole results are inconsistent with the null
              hypothesis. We illustrate its use by applying it to three
              published findings. One proves robust, one weak, one not robust
              at all.},
	author = {Simonsohn, Uri and Simmons, Joseph P and Nelson, Leif D},
	journal = {SSRN Electronic Journal},
	number = {November},
	title = {Specification curve: Descriptive and inferential statistics on all reasonable specifications},
	year = 2015}

@article{Inati2017-xo,
	abstract = {PURPOSE: This work proposes the ISMRM Raw Data format as a common
              MR raw data format, which promotes algorithm and data sharing.
              METHODS: A file format consisting of a flexible header and tagged
              frames of k-space data was designed. Application Programming
              Interfaces were implemented in C/C++, MATLAB, and Python.
              Converters for Bruker, General Electric, Philips, and Siemens
              proprietary file formats were implemented in C++. Raw data were
              collected using magnetic resonance imaging scanners from four
              vendors, converted to ISMRM Raw Data format, and reconstructed
              using software implemented in three programming languages (C++,
              MATLAB, Python). RESULTS: Images were obtained by reconstructing
              the raw data from all vendors. The source code, raw data, and
              images comprising this work are shared online, serving as an
              example of an image reconstruction project following a paradigm
              of reproducible research. CONCLUSION: The proposed raw data
              format solves a practical problem for the magnetic resonance
              imaging community. It may serve as a foundation for reproducible
              research and collaborations. The ISMRM Raw Data format is a
              completely open and community-driven format, and the scientific
              community is invited (including commercial vendors) to
              participate either as users or developers. Magn Reson Med
              77:411-421, 2017. \copyright{} 2016 Wiley Periodicals, Inc.},
	author = {Inati, Souheil J and Naegele, Joseph D and Zwart, Nicholas R and Roopchansingh, Vinai and Lizak, Martin J and Hansen, David C and Liu, Chia-Ying and Atkinson, David and Kellman, Peter and Kozerke, Sebastian and Xue, Hui and Campbell-Washburn, Adrienne E and S{\o}rensen, Thomas S and Hansen, Michael S},
	journal = {Magnetic Resonance in Medicine},
	keywords = {image reconstruction; magnetic resonance imaging; open source; raw data format},
	language = {en},
	month = jan,
	number = 1,
	pages = {411--421},
	title = {{ISMRM} Raw data format: A proposed standard for {MRI} raw datasets},
	volume = 77,
	year = 2017}

@article{Ugurbil2013-pk,
	abstract = {The Human Connectome Project (HCP) relies primarily on three
              complementary magnetic resonance (MR) methods. These are: 1)
              resting state functional MR imaging (rfMRI) which uses
              correlations in the temporal fluctuations in an fMRI time series
              to deduce 'functional connectivity'; 2) diffusion imaging (dMRI),
              which provides the input for tractography algorithms used for the
              reconstruction of the complex axonal fiber architecture; and 3)
              task based fMRI (tfMRI), which is employed to identify functional
              parcellation in the human brain in order to assist analyses of
              data obtained with the first two methods. We describe technical
              improvements and optimization of these methods as well as
              instrumental choices that impact speed of acquisition of fMRI and
              dMRI images at 3T, leading to whole brain coverage with 2 mm
              isotropic resolution in 0.7 s for fMRI, and 1.25 mm isotropic
              resolution dMRI data for tractography analysis with three-fold
              reduction in total dMRI data acquisition time. Ongoing technical
              developments and optimization for acquisition of similar data at
              7 T magnetic field are also presented, targeting higher spatial
              resolution, enhanced specificity of functional imaging signals,
              mitigation of the inhomogeneous radio frequency (RF) fields, and
              reduced power deposition. Results demonstrate that overall, these
              approaches represent a significant advance in MR imaging of the
              human brain to investigate brain function and structure.},
	author = {U{\u g}urbil, Kamil and Xu, Junqian and Auerbach, Edward J and Moeller, Steen and Vu, An T and Duarte-Carvajalino, Julio M and Lenglet, Christophe and Wu, Xiaoping and Schmitter, Sebastian and Van de Moortele, Pierre Francois and Strupp, John and Sapiro, Guillermo and De Martino, Federico and Wang, Dingxin and Harel, Noam and Garwood, Michael and Chen, Liyong and Feinberg, David A and Smith, Stephen M and Miller, Karla L and Sotiropoulos, Stamatios N and Jbabdi, Saad and Andersson, Jesper L R and Behrens, Timothy E J and Glasser, Matthew F and Van Essen, David C and Yacoub, Essa and {WU-Minn HCP Consortium}},
	journal = {Neuroimage},
	language = {en},
	month = oct,
	pages = {80--104},
	title = {Pushing spatial and temporal resolution for functional and diffusion {MRI} in the Human Connectome Project},
	volume = 80,
	year = 2013}

@article{Pernet2015-jx,
	author = {Pernet, Cyril R and Poline, Jean-Baptiste},
	journal = {GigaScience},
	number = 1,
	title = {Improving functional magnetic resonance imaging reproducibility},
	volume = 4,
	year = 2015}

@article{Kleiner2007-jj,
	author = {Kleiner, M and Brainard, D and Pelli, D},
	journal = {Perception},
	number = 14,
	title = {What's new in Psychtoolbox-3?},
	volume = 36,
	year = 2007}

@article{Karakuzu2020-xw,
	author = {Karakuzu, Agah and Boudreau, Mathieu and Duval, Tanguy and Boshkovski, Tommy and Leppert, Ilana and Cabana, Jean-Fran{\c c}ois and Gagnon, Ian and Beliveau, Pascale and Pike, G and Cohen-Adad, Julien and Stikov, Nikola},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	journal = {Journal of Open Source Software},
	month = sep,
	number = 53,
	pages = {2343},
	publisher = {The Open Journal},
	title = {{qMRLab}: Quantitative {MRI} analysis, under one umbrella},
	volume = 5,
	year = 2020}

@article{Camerer2016-ge,
	abstract = {The replicability of some scientific findings has recently been
              called into question. To contribute data about replicability in
              economics, we replicated 18 studies published in the American
              Economic Review and the Quarterly Journal of Economics between
              2011 and 2014. All of these replications followed predefined
              analysis plans that were made publicly available beforehand, and
              they all have a statistical power of at least 90\% to detect the
              original effect size at the 5\% significance level. We found a
              significant effect in the same direction as in the original study
              for 11 replications (61\%); on average, the replicated effect
              size is 66\% of the original. The replicability rate varies
              between 67\% and 78\% for four additional replicability
              indicators, including a prediction market measure of peer
              beliefs.},
	author = {Camerer, Colin F and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	journal = {Science},
	language = {en},
	month = mar,
	number = 6280,
	pages = {1433--1436},
	title = {Evaluating replicability of laboratory experiments in economics},
	volume = 351,
	year = 2016}

@article{Gorgolewski2011-nf,
	abstract = {Current neuroimaging software offer users an incredible
              opportunity to analyze their data in different ways, with
              different underlying assumptions. Several sophisticated software
              packages (e.g., AFNI, BrainVoyager, FSL, FreeSurfer, Nipy, R,
              SPM) are used to process and analyze large and often diverse
              (highly multi-dimensional) data. However, this heterogeneous
              collection of specialized applications creates several issues
              that hinder replicable, efficient, and optimal use of
              neuroimaging analysis approaches: (1) No uniform access to
              neuroimaging analysis software and usage information; (2) No
              framework for comparative algorithm development and
              dissemination; (3) Personnel turnover in laboratories often
              limits methodological continuity and training new personnel takes
              time; (4) Neuroimaging software packages do not address
              computational efficiency; and (5) Methods sections in journal
              articles are inadequate for reproducing results. To address these
              issues, we present Nipype (Neuroimaging in Python: Pipelines and
              Interfaces; http://nipy.org/nipype), an open-source,
              community-developed, software package, and scriptable library.
              Nipype solves the issues by providing Interfaces to existing
              neuroimaging software with uniform usage semantics and by
              facilitating interaction between these packages using Workflows.
              Nipype provides an environment that encourages interactive
              exploration of algorithms, eases the design of Workflows within
              and between packages, allows rapid comparative development of
              algorithms and reduces the learning curve necessary to use
              different packages. Nipype supports both local and remote
              execution on multi-core machines and clusters, without additional
              scripting. Nipype is Berkeley Software Distribution licensed,
              allowing anyone unrestricted usage. An open, community-driven
              development philosophy allows the software to quickly adapt and
              address the varied needs of the evolving neuroimaging community,
              especially in the context of increasing demand for reproducible
              research.},
	author = {Gorgolewski, Krzysztof J and Burns, Christopher D and Madison, Cindee and Clark, Dav and Halchenko, Yaroslav O and Waskom, Michael L and Ghosh, Satrajit S},
	journal = {Frontiers in Neuroinformatics},
	keywords = {data processing; neuroimaging; neuroimaging, data processing, workflow, pipeline,; pipeline; python; reproducible research; workflow},
	number = {August},
	title = {Nipype: A flexible, lightweight and extensible neuroimaging data processing framework in python},
	volume = 5,
	year = 2011}

@inproceedings{Alfaro-Almagro2016-pj,
	author = {Alfaro-Almagro, F and Jenkinson, M and Bangerter, N and Andersson, J and Griffanti, L and Douaud, G},
	conference = {Human Brain Mapping},
	location = {Geneve, Switzerland},
	pages = {1877},
	title = {{UK} Biobank brain imaging: Automated processing pipeline and quality control for 100,000 subjects},
	year = 2016}

@article{Tustison2013-rb,
	author = {Tustison, Nicholas J and Johnson, Hans J and Rohlfing, Torsten and Klein, Arno and Ghosh, Satrajit S and Ibanez, Luis and Avants, Brian B},
	journal = {Frontiers in Neuroscience},
	keywords = {best practices; comparative evaluations; confirmation bias; open science; reproducibility},
	language = {en},
	month = sep,
	pages = {162},
	title = {Instrumentation bias in the use and evaluation of scientific software: Recommendations for reproducible practices in the computational sciences},
	volume = 7,
	year = 2013}

@misc{Dfg2021-ct,
	author = {{DFG}},
	booktitle = {{DFG}},
	title = {Data tracking in research: Aggregation and use or sale of usage data by academic publishers},
	year = 2021}

@article{Gau2021-rp,
	abstract = {Brainhack is an innovative meeting format that promotes
              scientific collaboration and education in an open, inclusive
              environment. This NeuroView describes the myriad benefits for
              participants and the research community and how Brainhacks
              complement conventional formats to augment scientific progress.},
	author = {Gau, R{\'e}mi and Noble, Stephanie and Heuer, Katja and Bottenhorn, Katherine L and Bilgin, Isil P and Yang, Yu-Fang and Huntenburg, Julia M and Bayer, Johanna M M and Bethlehem, Richard A I and Rhoads, Shawn A and Vogelbacher, Christoph and Borghesani, Valentina and Levitis, Elizabeth and Wang, Hao-Ting and Van Den Bossche, Sofie and Kobeleva, Xenia and Legarreta, Jon Haitz and Guay, Samuel and Atay, Selim Melvin and Varoquaux, Gael P and Huijser, Dorien C and Sandstr{\"o}m, Malin S and Herholz, Peer and Nastase, Samuel A and Badhwar, Amanpreet and Dumas, Guillaume and Schwab, Simon and Moia, Stefano and Dayan, Michael and Bassil, Yasmine and Brooks, Paula P and Mancini, Matteo and Shine, James M and O'Connor, David and Xie, Xihe and Poggiali, Davide and Friedrich, Patrick and Heinsfeld, Anibal S and Riedl, Lydia and Toro, Roberto and Caballero-Gaudes, C{\'e}sar and Eklund, Anders and Garner, Kelly G and Nolan, Christopher R and Demeter, Damion V and Barrios, Fernando A and Merchant, Junaid S and McDevitt, Elizabeth A and Oostenveld, Robert and Craddock, R Cameron and Rokem, Ariel and Doyle, Andrew and Ghosh, Satrajit S and Nikolaidis, Aki and Stanley, Olivia W and Uru{\~n}uela, Eneko and {Brainhack Community}},
	journal = {Neuron},
	keywords = {Brainhack; best practices; collaboration; community building; hackathon; inclusivity; neuroscience; open science; reproducibility; training},
	language = {en},
	month = jun,
	number = 11,
	pages = {1769--1775},
	title = {Brainhack: Developing a culture of open, inclusive, community-driven neuroscience},
	volume = 109,
	year = 2021}

@article{Cox1996-ui,
	abstract = {A package of computer programs for analysis and visualization of
              three-dimensional human brain functional magnetic resonance
              imaging (FMRI) results is described. The software can color
              overlay neural activation maps onto higher resolution anatomical
              scans. Slices in each cardinal plane can be viewed
              simultaneously. Manual placement of markers on anatomical
              landmarks allows transformation of anatomical and functional
              scans into stereotaxic (Talairach-Tournoux) coordinates. The
              techniques for automatically generating transformed functional
              data sets from manually labeled anatomical data sets are
              described. Facilities are provided for several types of
              statistical analyses of multiple 3D functional data sets. The
              programs are written in ANSI C and Motif 1.2 to run on Unix
              workstations.},
	author = {Cox, R W},
	journal = {Computers and Biomedical Research},
	keywords = {cots},
	number = 29,
	pages = {162--173},
	title = {{AFNI}: software for analysis and visualization of functional magnetic resonance neuroimages},
	volume = 29,
	year = 1996}

@article{Taulu2014-pu,
	author = {Taulu, Samu and Simola, Juha and Nenonen, Jukka and Parkkonen, Lauri},
	journal = {Magnetoencephalography},
	pages = {35--71},
	title = {Novel noise reduction methods},
	year = 2014}

@article{Laird2021-nb,
	abstract = {Large, open datasets have emerged as important resources in the
              field of human connectomics. In this review, the evolution of
              data sharing involving magnetic resonance imaging is described. A
              summary of the challenges and progress in conducting reproducible
              data analyses is provided, including description of recent
              progress made in the development of community guidelines and
              recommendations, software and data management tools, and
              initiatives to enhance training and education. Finally, this
              review concludes with a discussion of ethical conduct relevant to
              analyses of large, open datasets and a researcher's
              responsibility to prevent further stigmatization of historically
              marginalized racial and ethnic groups. Moving forward, future
              work should include an enhanced emphasis on the social
              determinants of health, which may further contextualize findings
              among diverse population-based samples. Leveraging the progress
              to date and guided by interdisciplinary collaborations, the
              future of connectomics promises to be an impressive era of
              innovative research, yielding a more inclusive understanding of
              brain structure and function.},
	author = {Laird, Angela R},
	journal = {Neuroimage},
	keywords = {Connectomics; Large open datasets; Neuroimaging data sharing; Reproducible analytics},
	language = {en},
	month = dec,
	pages = {118579},
	title = {Large, open datasets for human connectomics research: Considerations for reproducible and responsible data use},
	volume = 244,
	year = 2021}

@article{Poline2022-li,
	abstract = {In this perspective article, we consider the critical issue of
              data and other research object standardisation and, specifically,
              how international collaboration, and organizations such as the
              International Neuroinformatics Coordinating Facility (INCF) can
              encourage that emerging neuroscience data be Findable,
              Accessible, Interoperable, and Reusable (FAIR). As
              neuroscientists engaged in the sharing and integration of
              multi-modal and multiscale data, we see the current insufficiency
              of standards as a major impediment in the Interoperability and
              Reusability of research results. We call for increased
              international collaborative standardisation of neuroscience data
              to foster integration and efficient reuse of research objects.},
	author = {Poline, Jean-Baptiste and Kennedy, David N and Sommer, Friedrich T and Ascoli, Giorgio A and Van Essen, David C and Ferguson, Adam R and Grethe, Jeffrey S and Hawrylycz, Michael J and Thompson, Paul M and Poldrack, Russell A and Ghosh, Satrajit S and Keator, David B and Athey, Thomas L and Vogelstein, Joshua T and Mayberg, Helen S and Martone, Maryann E},
	journal = {Neuroinformatics},
	keywords = {International Neuroinformatics Coordinating Facility; Interoperability; Neuroscience; Standards,},
	language = {en},
	month = jan,
	title = {Is Neuroscience {FAIR}? A Call for Collaborative Standardisation of Neuroscience Data},
	year = 2022}

@article{Clayson2021-nt,
	abstract = {In studies of event-related brain potentials (ERPs), numerous
              decisions about data processing are required to extract ERP
              scores from continuous data. Unfortunately, the systematic impact
              of these choices on the data quality and psychometric reliability
              of ERP scores or even ERP scores themselves is virtually unknown,
              which is a barrier to the standardization of ERPs. The aim of the
              present study was to optimize processing pipelines for the
              error-related negativity (ERN) and error positivity (Pe) by
              considering a multiverse of data processing choices. A multiverse
              analysis of a data processing pipeline examines the impact of a
              large set of different reasonable choices to determine the
              robustness of effects, such as the impact of different decisions
              on between-trial standard deviations (i.e., data quality) and
              between-condition differences (i.e., experimental effects). ERN
              and Pe data from 298 healthy young adults were used to determine
              the impact of different methodological choices on data quality
              and experimental effects (correct vs. error trials) at several
              key stages: highpass filtering, lowpass filtering, ocular
              artifact correction, reference, baseline adjustment, scoring
              sensors, and measurement procedure. This multiverse analysis
              yielded 3,456 ERN scores and 576 Pe scores per person. An
              optimized pipeline for ERN included a 15 Hz lowpass filter,
              ICA-based ocular artifact correction, and a region of interest
              (ROI) approach to scoring. For Pe, the optimized pipeline
              included a 0.10 Hz highpass filter, 30 Hz lowpass filter,
              regression-based ocular artifact correction, a -200 to 0 ms
              baseline adjustment window, and an ROI approach to scoring. The
              multiverse approach can be used to optimize pipelines for
              eventual standardization, which would support efforts toward
              establishing normative ERP databases. The proposed process of
              analyzing the data-processing multiverse of ERP scores paves the
              way for better refinement, identification, and selection of data
              processing parameters, ultimately improving the precision and
              utility of ERPs.},
	author = {Clayson, Peter E and Baldwin, Scott A and Rocha, Harold A and Larson, Michael J},
	journal = {Neuroimage},
	keywords = {Data quality; ERP psychometric reliability; Event-related potentials (ERPs); Multilevel models; Multiverse analysis; Open science},
	language = {en},
	month = nov,
	pages = {118712},
	title = {The data-processing multiverse of event-related potentials ({ERPs)}: A roadmap for the optimization and standardization of {ERP} processing and reduction pipelines},
	year = 2021}

@article{Moshontz2021-qj,
	abstract = {Posting preprints online allows psychological scientists to get
               feedback, speed dissemination, and ensure public access to their
               work. This guide is designed to help psychological scientists
               post preprints and manage them across the publication pipeline.
               We review terminology, provide a historical and legal overview
               of preprints, and give guidance on posting and managing
               preprints before, during, or after the peer-review process to
               achieve different aims (e.g., get feedback, speed dissemination,
               achieve open access). We offer concrete recommendations to
               authors, such as post preprints that are complete and carefully
               proofread; post preprints in a dedicated preprint server that
               assigns DOIs, provides editable metadata, is indexed by
               GoogleScholar, supports review and endorsements, and supports
               version control; include a draft date and information about the
               paper?s status on the cover page; license preprints with CC BY
               licenses that permit public use with attribution; and keep
               preprints up to date after major revisions. Although our focus
               is on preprints (unpublished versions of a work), we also offer
               information relevant to postprints (author-formatted,
               post-peer-review versions of a work) and work that will not
               otherwise be published (e.g., theses and dissertations).},
	author = {Moshontz, Hannah and Binion, Grace and Walton, Haley and Brown, Benjamin T and Syed, Moin},
	journal = {Advances in Methods and Practices in Psychological Science},
	month = apr,
	number = 2,
	publisher = {SAGE Publications Inc},
	title = {A guide to posting and managing preprints},
	volume = 4,
	year = 2021}

@article{Bigdely-Shamlo2020-pt,
	abstract = {Significant achievements have been made in the fMRI field by
              pooling statistical results from multiple studies
              (meta-analysis). More recently, fMRI standardization efforts have
              focused on enabling the joint analysis of raw fMRI data across
              studies (mega-analysis), with the hope of achieving more detailed
              insights. However, it has not been clear if such analyses in the
              EEG field are possible or equally fruitful. Here we present the
              results of a large-scale EEG mega-analysis using 18 studies from
              six sites representing several different experimental paradigms.
              We demonstrate that when meta-data are consistent across studies,
              both channel-level and source-level EEG mega-analysis are
              possible and can provide insights unavailable in single studies.
              The analysis uses a fully-automated processing pipeline to reduce
              line noise, interpolate noisy channels, perform robust
              referencing, remove eye-activity, and further identify outlier
              signals. We define several robust measures based on channel
              amplitude and dispersion to assess the comparability of data
              across studies and observe the effect of various processing steps
              on these measures. Using ICA-based dipolar sources, we also
              observe consistent differences in overall frequency baseline
              amplitudes across brain areas. For example, we observe higher
              alpha in posterior vs anterior regions and higher beta in
              temporal regions. We also detect consistent differences in the
              slope of the aperiodic portion of the EEG spectrum across brain
              areas. In a companion paper, we apply mega-analysis to assess
              commonalities in event-related EEG features across studies. The
              continuous raw and preprocessed data used in this analysis are
              available through the DataCatalog at https://cancta.net.},
	author = {Bigdely-Shamlo, Nima and Touryan, Jonathan and Ojeda, Alejandro and Kothe, Christian and Mullen, Tim and Robbins, Kay},
	journal = {Neuroimage},
	keywords = {EEG/MEG; Large-scale; Mega-analysis; Meta-analysis; Neuroinformatics; Signal statistics},
	language = {en},
	month = feb,
	pages = {116361},
	title = {Automated {EEG} mega-analysis I: Spectral and amplitude characteristics across studies},
	volume = 207,
	year = 2020}

@misc{Khoo2019-xy,
	author = {Khoo, Shaun Yon-Seng},
	journal = {LIBER Quarterly: The Journal of the Association of European Research Libraries},
	number = 1,
	pages = {1--18},
	title = {Article Processing Charge Hyperinflation and Price Insensitivity: An Open Access Sequel to the Serials Crisis},
	volume = 29,
	year = 2019}

@article{Appelhoff2019-kg,
	author = {Appelhoff, Stefan and Sanderson, Matthew and Brooks, Teon and van Vliet, Marijn and Quentin, Romain and Holdgraf, Chris and Chaumon, Maximilien and Mikulan, Ezequiel and Tavabi, Kambiz and H{\"o}chenberger, Richard and Welke, Dominik and Brunner, Clemens and Rockhill, Alexander and Larson, Eric and Gramfort, Alexandre and Jas, Mainak},
	journal = {Journal of Open Source Software},
	number = 44,
	pages = {1896},
	title = {{MNE-BIDS}: Organizing electrophysiological data into the {BIDS} format and facilitating their analysis},
	volume = 4,
	year = 2019}

@inproceedings{Norgaard2019-su,
	abstract = {Brain imaging studies have set the stage for measuring brain
                function in psychiatric disorders, such as depression, with the
                goal of developing effective treatment strategies. However,
                data arising from such studies are often hampered by noise
                confounds such as motion-related artifacts, affecting both the
                spatial and temporal correlation structure of the data. Failure
                to adequately control for these types of noise can have
                significant impact on subsequent statistical analyses. In this
                paper, we demonstrate a framework for extending the
                non-parametric testing of statistical significance in
                predictive modeling by including a plausible set of
                preprocessing strategies to measure the predictive power. Our
                approach adopts permutation tests to estimate how likely we are
                to obtain a given predictive performance in an independent
                sample, depending on the preprocessing strategy used to
                generate the data. We demonstrate and apply the framework on
                examples of longitudinal Positron Emission Tomography (PET)
                data following a pharmacological intervention.},
	author = {N{\o}rgaard, Martin and Ozenne, Brice and Svarer, Claus and Frokjaer, Vibe G and Schain, Martin and Strother, Stephen C and Ganz, Melanie},
	conference = {Medical Image Computing and Computer Assisted Intervention},
	pages = {196--204},
	publisher = {Springer International Publishing},
	title = {Preprocessing, prediction and significance: Framework and application to brain imaging},
	year = 2019}

@article{Gorgolewski2016-iy,
	abstract = {Recent years have seen an increase in alarming signals regarding
               the lack of replicability in neuroscience, psychology, and other
               related fields. To avoid a widespread crisis in neuroimaging
               research and consequent loss of credibility in the public eye,
               we need to improve how we do science. This article aims to be a
               practical guide for researchers at any stage of their careers
               that will help them make their research more reproducible and
               transparent while minimizing the additional effort that this
               might require. The guide covers three major topics in open
               science (data, code, and publications) and offers practical
               advice as well as highlighting advantages of adopting more open
               research practices that go beyond improved transparency and
               reproducibility.},
	author = {Gorgolewski, Krzysztof J and Poldrack, Russell A},
	journal = {PLoS Biology},
	month = jul,
	number = 7,
	pages = {e1002506},
	publisher = {Public Library of Science},
	title = {A practical guide for improving transparency and reproducibility in neuroimaging research},
	volume = 14,
	year = 2016}

@article{Mortamet2009-lk,
	abstract = {MRI has evolved into an important diagnostic technique in medical
              imaging. However, reliability of the derived diagnosis can be
              degraded by artifacts, which challenge both radiologists and
              automatic computer-aided diagnosis. This work proposes a
              fully-automatic method for measuring image quality of
              three-dimensional (3D) structural MRI. Quality measures are
              derived by analyzing the air background of magnitude images and
              are capable of detecting image degradation from several sources,
              including bulk motion, residual magnetization from incomplete
              spoiling, blurring, and ghosting. The method has been validated
              on 749 3D T(1)-weighted 1.5T and 3T head scans acquired at 36
              Alzheimer's Disease Neuroimaging Initiative (ADNI) study sites
              operating with various software and hardware combinations.
              Results are compared against qualitative grades assigned by the
              ADNI quality control center (taken as the reference standard).
              The derived quality indices are independent of the MRI system
              used and agree with the reference standard quality ratings with
              high sensitivity and specificity (>85\%). The proposed procedures
              for quality assessment could be of great value for both research
              and routine clinical imaging. It could greatly improve workflow
              through its ability to rule out the need for a repeat scan while
              the patient is still in the magnet bore.},
	author = {Mortamet, B{\'e}n{\'e}dicte and Bernstein, Matt A and Jack, Jr, Clifford R and Gunter, Jeffrey L and Ward, Chadwick and Britson, Paula J and Meuli, Reto and Thiran, Jean-Philippe and Krueger, Gunnar and {Alzheimer's Disease Neuroimaging Initiative}},
	journal = {Magn. Reson. Med.},
	language = {en},
	month = aug,
	number = 2,
	pages = {365--372},
	title = {Automatic quality assessment in structural brain magnetic resonance imaging},
	volume = 62,
	year = 2009}

@article{Popov2018-uu,
	abstract = {The auditory steady state evoked response (ASSR) is a robust and
              frequently utilized phenomenon in psychophysiological research.
              It reflects the auditory cortical response to an
              amplitude-modulated constant carrier frequency signal. The
              present report provides a concrete example of a group analysis of
              the EEG data from 29 healthy human participants, recorded during
              an ASSR paradigm, using the FieldTrip toolbox. First, we
              demonstrate sensor-level analysis in the time domain, allowing
              for a description of the event-related potentials (ERPs), as well
              as their statistical evaluation. Second, frequency analysis is
              applied to describe the spectral characteristics of the ASSR,
              followed by group level statistical analysis in the frequency
              domain. Third, we show how time- and frequency-domain analysis
              approaches can be combined in order to describe the temporal and
              spectral development of the ASSR. Finally, we demonstrate source
              reconstruction techniques to characterize the primary neural
              generators of the ASSR. Throughout, we pay special attention to
              explaining the design of the analysis pipeline for single
              subjects and for the group level analysis. The pipeline presented
              here can be adjusted to accommodate other experimental paradigms
              and may serve as a template for similar analyses.},
	author = {Popov, Tzvetan and Oostenveld, Robert and Schoffelen, Jan M},
	journal = {Front. Neurosci.},
	keywords = {ASSR; EEG; ERP; FieldTrip; beamforming; group analysis},
	language = {en},
	month = oct,
	pages = {711},
	title = {{FieldTrip} Made Easy: An Analysis Protocol for Group Analysis of the Auditory Steady State Brain Response in Time, Frequency, and Space},
	volume = 12,
	year = 2018}

@article{Baggio2013-ay,
	abstract = {Gender-specific medicine is the study of how diseases differ
               between men and women in terms of prevention, clinical signs,
               therapeutic approach, prognosis, psychological and social
               impact. It is a neglected dimension of medicine. In this review
               we like to point out some major issues in five enormous fields
               of medicine: cardiovascular diseases (CVDs), pharmacology,
               oncology, liver diseases and osteoporosis. CVDs have been
               studied in the last decades mainly in men, but they are the
               first cause of mortality and disability in women. Risk factors
               for CVD have different impacts in men and women; clinical
               manifestations of CVD and the influence of drugs on CVD have lot
               of gender differences. Sex-related differences in
               pharmacokinetics and pharmacodynamics are also emerging. These
               differences have obvious relevance to the efficacy and side
               effect profiles of various medications in the two sexes. This
               evidence should be considered for drug development as well as
               before starting any therapy. Gender disparity in cancer
               incidence, aggressiveness and prognosis has been observed for a
               variety of cancers and, even if partially known, is
               underestimated in clinical practice for the treatment of the
               major types of cancer. It is necessary to systematize and encode
               all the known data for each type of tumor on gender differences,
               to identify where this variable has to be considered for the
               purposes of the prognosis, the choice of treatment and possible
               toxicity. Clinical data suggest that men and women exhibit
               differences regarding the epidemiology and the progression of
               certain liver diseases , i.e., autoimmune conditions, genetic
               hemochromatosis, non-alcoholic steatohepatitis and chronic
               hepatitis C. Numerous hypotheses have been formulated to justify
               this sex imbalance including sex hormones, reproductive and
               genetic factors. Nevertheless, none of these hypothesis has thus
               far gathered enough convincing evidence and in most cases the
               evidence is conflicting. Osteoporosis is an important public
               health problem both in women and men. On the whole, far more
               epidemiologic, diagnostic and therapeutic studies have been
               carried out in women than in men. In clinical practice, if this
               disease remains underestimated in women, patients' and
               physicians' awareness is even lower for male osteoporosis, for
               which diagnostic and therapeutic strategies are at present less
               defined. In conclusion this review emphasizes the urgency of
               basic science and clinical research to increase our
               understanding of the gender differences of diseases.},
	author = {Baggio, Giovannella and Corsini, Alberto and Floreani, Annarosa and Giannini, Sandro and Zagonel, Vittorina},
	journal = {Clinical Chemistry and Laboratory Medicine},
	keywords = {cancer; cardiovascular diseases; liver diseases; osteoporosis; pharmacology},
	language = {en},
	month = apr,
	number = 4,
	pages = {713--727},
	publisher = {De Gruyter},
	title = {Gender medicine: A task for the third millennium},
	volume = 51,
	year = 2013}

@misc{Connolly2022-hq,
	author = {Connolly, Andy and Halchenko, Yaroslav},
	month = mar,
	title = {{ReproNim/reprostim}:},
	year = 2022}

@article{Hansen2013-ol,
	abstract = {This work presents a new open source framework for medical image
              reconstruction called the ``Gadgetron.'' The framework implements
              a flexible system for creating streaming data processing
              pipelines where data pass through a series of modules or
              ``Gadgets'' from raw data to reconstructed images. The data
              processing pipeline is configured dynamically at run-time based
              on an extensible markup language configuration description. The
              framework promotes reuse and sharing of reconstruction modules
              and new Gadgets can be added to the Gadgetron framework through a
              plugin-like architecture without recompiling the basic framework
              infrastructure. Gadgets are typically implemented in C/C++, but
              the framework includes wrapper Gadgets that allow the user to
              implement new modules in the Python scripting language for rapid
              prototyping. In addition to the streaming framework
              infrastructure, the Gadgetron comes with a set of dedicated
              toolboxes in shared libraries for medical image reconstruction.
              This includes generic toolboxes for data-parallel (e.g.,
              GPU-based) execution of compute-intensive components. The basic
              framework architecture is independent of medical imaging
              modality, but this article focuses on its application to
              Cartesian and non-Cartesian parallel magnetic resonance imaging.},
	author = {Hansen, Michael Schacht and S{\o}rensen, Thomas Sangild},
	journal = {Magnetic Resonance in Medicine},
	language = {en},
	month = jun,
	number = 6,
	pages = {1768--1776},
	title = {Gadgetron: An open source framework for medical image reconstruction},
	volume = 69,
	year = 2013}

@article{Kidwell2016-sw,
	abstract = {Beginning January 2014, Psychological Science gave authors the
              opportunity to signal open data and materials if they qualified
              for badges that accompanied published articles. Before badges,
              less than 3\% of Psychological Science articles reported open
              data. After badges, 23\% reported open data, with an accelerating
              trend; 39\% reported open data in the first half of 2015, an
              increase of more than an order of magnitude from baseline. There
              was no change over time in the low rates of data sharing among
              comparison journals. Moreover, reporting openness does not
              guarantee openness. When badges were earned, reportedly available
              data were more likely to be actually available, correct, usable,
              and complete than when badges were not earned. Open materials
              also increased to a weaker degree, and there was more variability
              among comparison journals. Badges are simple, effective signals
              to promote open practices and improve preservation of data and
              materials by using independent repositories.},
	author = {Kidwell, Mallory C and Lazarevi{\'c}, Ljiljana B and Baranski, Erica and Hardwicke, Tom E and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and Hess-Holden, Chelsey and Errington, Timothy M and Fiedler, Susann and Nosek, Brian A},
	journal = {PLoS Biology},
	language = {en},
	month = may,
	number = 5,
	pages = {e1002456},
	title = {Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency},
	volume = 14,
	year = 2016}

@article{Knudsen2020-nz,
	abstract = {It is a growing concern that outcomes of neuroimaging studies
              often cannot be replicated. To counteract this, the magnetic
              resonance (MR) neuroimaging community has promoted acquisition
              standards and created data sharing platforms, based on a
              consensus on how to organize and share MR neuroimaging data.
              Here, we take a similar approach to positron emission tomography
              (PET) data. To facilitate comparison of findings across studies,
              we first recommend publication standards for tracer
              characteristics, image acquisition, image preprocessing, and
              outcome estimation for PET neuroimaging data. The co-authors of
              this paper, representing more than 25 PET centers worldwide,
              voted to classify information as mandatory, recommended, or
              optional. Second, we describe a framework to facilitate data
              archiving and data sharing within and across centers. Because of
              the high cost of PET neuroimaging studies, sample sizes tend to
              be small and relatively few sites worldwide have the required
              multidisciplinary expertise to properly conduct and analyze PET
              studies. Data sharing will make it easier to combine datasets
              from different centers to achieve larger sample sizes and
              stronger statistical power to test hypotheses. The combining of
              datasets from different centers may be enhanced by adoption of a
              common set of best practices in data acquisition and analysis.},
	author = {Knudsen, Gitte M and Ganz, Melanie and Appelhoff, Stefan and Boellaard, Ronald and Bormans, Guy and Carson, Richard E and Catana, Ciprian and Doudet, Doris and Gee, Antony D and Greve, Douglas N and Gunn, Roger N and Halldin, Christer and Herscovitch, Peter and Huang, Henry and Keller, Sune H and Lammertsma, Adriaan A and Lanzenberger, Rupert and Liow, Jeih-San and Lohith, Talakad G and Lubberink, Mark and Lyoo, Chul H and Mann, J John and Matheson, Granville J and Nichols, Thomas E and N{\o}rgaard, Martin and Ogden, Todd and Parsey, Ramin and Pike, Victor W and Price, Julie and Rizzo, Gaia and Rosa-Neto, Pedro and Schain, Martin and Scott, Peter Jh and Searle, Graham and Slifstein, Mark and Suhara, Tetsuya and Talbot, Peter S and Thomas, Adam and Veronese, Mattia and Wong, Dean F and Yaqub, Maqsood and Zanderigo, Francesca and Zoghbi, Sami and Innis, Robert B},
	journal = {Journal of Cerebral Blood Flow and Metabolism},
	keywords = {Consensus guidelines; data sharing; data structure; open source; positron emission tomography},
	language = {en},
	month = aug,
	number = 8,
	pages = {1576--1585},
	title = {Guidelines for the content and format of {PET} brain data in publications and archives: A consensus paper},
	volume = 40,
	year = 2020}

@article{Oostenveld2011-qo,
	abstract = {This paper describes FieldTrip, an open source software package
              that we developed for the analysis of MEG, EEG, and other
              electrophysiological data. The software is implemented as a
              MATLAB toolbox and includes a complete set of consistent and
              user-friendly high-level functions that allow experimental
              neuroscientists to analyze experimental data. It includes
              algorithms for simple and advanced analysis, such as
              time-frequency analysis using multitapers, source reconstruction
              using dipoles, distributed sources and beamformers, connectivity
              analysis, and nonparametric statistical permutation tests at the
              channel and source level. The implementation as toolbox allows
              the user to perform elaborate and structured analyses of large
              data sets using the MATLAB command line and batch scripting.
              Furthermore, users and developers can easily extend the
              functionality and implement new algorithms. The modular design
              facilitates the reuse in other software packages.},
	author = {Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs},
	journal = {Computational Intelligence and Neuroscience},
	language = {en},
	pages = {156869},
	title = {{FieldTrip}: Open source software for advanced analysis of {MEG}, {EEG}, and invasive electrophysiological data},
	volume = 2011,
	year = 2011}

@article{Appelhoff2019-ga,
	abstract = {The following statement is designed to clarify the complementary
               nature of the Brain Imaging Data Structure (BIDS) and the
               NeuroImaging Data Model (NIDM). It is not designed to be a
               comprehensive review of either of these initiatives, but rather
               to highlight the synergy of the co-application of each of these
               emerging technologies. The context for this statement is the
               preservation and communication of neuroimaging data and the
               additional experimental data and analytics that are associated
               with the neuroimaging data.},
	author = {Appelhoff, Stefan and Bates, Julianna F and Ghosh, Satrajit and Keator, David B and Kennedy, David N and Poldrack, Russell and Poline, Jean-Baptiste and Steffener, Jason and Nolan Nichols, B and Feingold, Franklin and Pernet, Cyril and Nilsonne, Gustav and Maumet, Camille and Flandin, Guillaume and Gau, R{\'e}mi and Oostenveld, Robert and Dupr{\'e}, Elizabeth and Delorme, Arnaud and Markiewicz, Christopher J and Perez, Natacha and Helmer, Karl G and Jarecka, Dorota and Grethe, Jeffrey S and Patterson, Dianne and Auer, Tibor and Bartsch, Hauke and Nichols, Thomas E and Calhoun, Vince and Ganz, Melanie and Smith, Robert E and Yarkoni, Tal},
	journal = {F1000Research},
	keywords = {BIDS, NIDM, neuroimaging},
	language = {en},
	number = 1924,
	pages = {1924},
	publisher = {F1000 Research Limited},
	title = {{BIDS} and the {NeuroImaging} Data Model ({NIDM})},
	volume = 8,
	year = 2019}

@article{Poldrack2019-fn,
	abstract = {The Target Article by Lee et al. (2019) highlights the ways in
              which ongoing concerns about research reproducibility extend to
              model-based approaches in cognitive science. Whereas Lee et al.
              focus primarily on the importance of research practices to
              improve model robustness, we propose that the transparent sharing
              of model specifications, including their inputs and outputs, is
              also essential to improving the reproducibility of model-based
              analyses. We outline an ongoing effort (within the context of the
              Brain Imaging Data Structure community) to develop standards for
              the sharing of the structure of computational models and their
              outputs.},
	author = {Poldrack, Russell A and Feingold, Franklin and Frank, Michael J and Gleeson, Padraig and de Hollander, Gilles and Huys, Quentin Jm and Love, Bradley C and Markiewicz, Christopher J and Moran, Rosalyn and Ritter, Petra and Rogers, Timothy T and Turner, Brandon M and Yarkoni, Tal and Zhan, Ming and Cohen, Jonathan D},
	journal = {Computational Brain \& Behavior},
	language = {en},
	month = dec,
	number = {3-4},
	pages = {229--232},
	title = {The importance of standards for sharing of computational models and data},
	volume = 2,
	year = 2019}

@article{Magland2016-nb,
	abstract = {PURPOSE: To describe SequenceTree, an open source, integrated
              software environment for implementing MRI pulse sequences and,
              ideally, exporting them to actual MRI scanners. The software is a
              user-friendly alternative to vendor-supplied pulse sequence
              design and editing tools and is suited for programmers and
              nonprogrammers alike. METHODS: The integrated user interface was
              programmed using the Qt4/C++ toolkit. As parameters and code are
              modified, the pulse sequence diagram is automatically updated
              within the user interface. Several aspects of pulse programming
              are handled automatically, allowing users to focus on
              higher-level aspects of sequence design. Sequences can be
              simulated using a built-in Bloch equation solver and then
              exported for use on a Siemens MRI scanner. Ideally, other types
              of scanners will be supported in the future. RESULTS:
              SequenceTree has been used for 8 years in our laboratory and
              elsewhere and has contributed to more than 50 peer-reviewed
              publications in areas such as cardiovascular imaging, solid state
              and nonproton NMR, MR elastography, and high-resolution
              structural imaging. CONCLUSION: SequenceTree is an innovative,
              open source, visual pulse sequence environment for MRI combining
              simplicity with flexibility and is ideal both for advanced users
              and users with limited programming experience.},
	author = {Magland, Jeremy F and Li, Cheng and Langham, Michael C and Wehrli, Felix W},
	journal = {Magnetic Resonance in Medicine},
	keywords = {MRI simulation; graphical user interface; pulse sequence; software},
	language = {en},
	month = jan,
	number = 1,
	pages = {257--265},
	title = {Pulse sequence programming in a dynamic visual environment: {SequenceTree}},
	volume = 75,
	year = 2016}

@article{Halchenko2021-op,
	author = {Halchenko, Yaroslav and Meyer, Kyle and Poldrack, Benjamin and Solanky, Debanjum and Wagner, Adina and Gors, Jason and MacFarlane, Dave and Pustina, Dorian and Sochat, Vanessa and Ghosh, Satrajit and M{\"o}nch, Christian and Markiewicz, Christopher and Waite, Laura and Shlyakhter, Ilya and de la Vega, Alejandro and Hayashi, Soichi and H{\"a}usler, Christian and Poline, Jean-Baptiste and Kadelka, Tobias and Skyt{\'e}n, Kusti and Jarecka, Dorota and Kennedy, David and Strauss, Ted and Cieslak, Matt and Vavra, Peter and Ioanas, Horea-Ioan and Schneider, Robin and Pfl{\"u}ger, Mika and Haxby, James and Eickhoff, Simon and Hanke, Michael},
	journal = {Journal of Open Source Software},
	number = 63,
	pages = {3262},
	title = {{DataLad}: Distributed system for joint management of code, data, and their relationship},
	volume = 6,
	year = 2021}

@article{Patel2015-nm,
	abstract = {OBJECTIVES: Model specification-what adjusting variables are
              analytically modeled-may influence results of observational
              associations. We present a standardized approach to quantify the
              variability of results obtained with choices of adjustments
              called the ``vibration of effects'' (VoE). STUDY DESIGN AND
              SETTING: We estimated the VoE for 417 clinical, environmental,
              and physiological variables in association with all-cause
              mortality using National Health and Nutrition Examination Survey
              data. We selected 13 variables as adjustment covariates and
              computed 8,192 Cox models for each of 417 variables' associations
              with all-cause mortality. RESULTS: We present the VoE by
              assessing the variance of the effect size and in the
              -log10(P-value) obtained by different combinations of
              adjustments. We present whether there are multimodality patterns
              in effect sizes and P-values and the trajectory of results with
              increasing adjustments. For 31\% of the 417 variables, we
              observed a Janus effect, with the effect being in opposite
              direction in the 99th versus the 1st percentile of analyses. For
              example, the vitamin E variant $\alpha$-tocopherol had a VoE that
              indicated higher and lower risk for mortality. CONCLUSION:
              Estimating VoE offers empirical estimates of associations are
              under different model specifications. When VoE is large, claims
              for observational associations should be very cautious.},
	author = {Patel, Chirag J and Burford, Belinda and Ioannidis, John P A},
	journal = {Journal of Clinical Epidemiology},
	keywords = {Biostatistics; Confounding; Environment-wide association study; Model specification; Observational association; Vibration of effects},
	language = {en},
	month = sep,
	number = 9,
	pages = {1046--1058},
	title = {Assessment of vibration of effects due to model specification can demonstrate the instability of observational associations},
	volume = 68,
	year = 2015}

@article{Llorens2021-yp,
	abstract = {Despite increased awareness of the lack of gender equity in
              academia and a growing number of initiatives to address issues of
              diversity, change is slow, and inequalities remain. A major
              source of inequity is gender bias, which has a substantial
              negative impact on the careers, work-life balance, and mental
              health of underrepresented groups in science. Here, we argue that
              gender bias is not a single problem but manifests as a collection
              of distinct issues that impact researchers' lives. We disentangle
              these facets and propose concrete solutions that can be adopted
              by individuals, academic institutions, and society.},
	author = {Llorens, Ana{\"\i}s and Tzovara, Athina and Bellier, Ludovic and Bhaya-Grossman, Ilina and Bidet-Caulet, Aur{\'e}lie and Chang, William K and Cross, Zachariah R and Dominguez-Faus, Rosa and Flinker, Adeen and Fonken, Yvonne and Gorenstein, Mark A and Holdgraf, Chris and Hoy, Colin W and Ivanova, Maria V and Jimenez, Richard T and Jun, Soyeon and Kam, Julia W Y and Kidd, Celeste and Marcelle, Enitan and Marciano, Deborah and Martin, Stephanie and Myers, Nicholas E and Ojala, Karita and Perry, Anat and Pinheiro-Chagas, Pedro and Ri{\`e}s, Stephanie K and Saez, Ignacio and Skelin, Ivan and Slama, Katarina and Staveland, Brooke and Bassett, Danielle S and Buffalo, Elizabeth A and Fairhall, Adrienne L and Kopell, Nancy J and Kray, Laura J and Lin, Jack J and Nobre, Anna C and Riley, Dylan and Solbakk, Anne-Kristin and Wallis, Joni D and Wang, Xiao-Jing and Yuval-Greenberg, Shlomit and Kastner, Sabine and Knight, Robert T and Dronkers, Nina F},
	journal = {Neuron},
	language = {en},
	month = jul,
	number = 13,
	pages = {2047--2074},
	title = {Gender bias in academia: A lifetime problem that needs solutions},
	volume = 109,
	year = 2021}

@article{Griswold2002-fv,
	author = {Griswold, Mark A and Jakob, Peter M and Heidemann, Robin M and Nittka, Mathias and Jellus, Vladimir and Wang, Jianmin and Kiefer, Berthold and Haase, Axel},
	journal = {Magnetic Resonance in Medicine},
	number = 6,
	pages = {1202--1210},
	title = {Generalized autocalibrating partially parallel acquisitions ({GRAPPA})},
	volume = 47,
	year = 2002}

@article{Hutson2018-wy,
	author = {Hutson, Matthew},
	journal = {Science},
	language = {en},
	month = feb,
	number = 6377,
	pages = {725--726},
	title = {Artificial intelligence faces reproducibility crisis},
	volume = 359,
	year = 2018}

@article{Nosek2018-ft,
	abstract = {Psychological researchers are preregistering studies at
              unprecedented and accelerating rates, setting a model for
              improving scientific practices.},
	author = {Nosek, Brian A and Stephen Lindsay, D},
	journal = {APS Observer},
	month = feb,
	number = 3,
	title = {Preregistration becoming the norm in psychological science},
	volume = 31,
	year = 2018}

@article{Jochimsen2004-li,
	author = {Jochimsen, Thies H and von Mengershausen, Michael},
	journal = {Journal of Magnetic Resonance},
	number = 1,
	pages = {67--78},
	title = {{ODIN}: Object-oriented development interface for {NMR}},
	volume = 170,
	year = 2004}

@article{Borghi2021-fw,
	abstract = {Research data is increasingly viewed as an important scholarly
               output. While a growing body of studies have investigated
               researcher practices and perceptions related to data sharing,
               information about data-related practices throughout the research
               process (including data collection and analysis) remains largely
               anecdotal. Building on our previous study of data practices in
               neuroimaging research, we conducted a survey of data management
               practices in the field of psychology. Our survey included
               questions about the type(s) of data collected, the tools used
               for data analysis, practices related to data organization,
               maintaining documentation, backup procedures, and long-term
               archiving of research materials. Our results demonstrate the
               complexity of managing and sharing data in psychology. Data is
               collected in multifarious forms from human participants,
               analyzed using a range of software tools, and archived in
               formats that may become obsolete. As individuals, our
               participants demonstrated relatively good data management
               practices, however they also indicated that there was little
               standardization within their research group. Participants
               generally indicated that they were willing to change their
               current practices in light of new technologies, opportunities,
               or requirements.},
	author = {Borghi, John A and Van Gulick, Ana E},
	journal = {PLoS One},
	month = may,
	number = 5,
	pages = {e0252047},
	publisher = {Public Library of Science},
	title = {Data management and sharing: Practices and perceptions of psychology researchers},
	volume = 16,
	year = 2021}

@article{Keator2013-az,
	abstract = {Data sharing efforts increasingly contribute to the acceleration
               of scientific discovery. Neuroimaging data is accumulating in
               distributed domain-spec{\ldots}},
	author = {Keator, D B and Helmer, K and Steffener, J and Turner, J A and Van Erp, T G M and Gadde, S and Ashish, N and Burns, G A and Nichols, B N},
	journal = {Neuroimage},
	month = nov,
	pages = {647--661},
	publisher = {Academic Press},
	title = {Towards structured sharing of raw and derived neuroimaging data across existing resources},
	volume = 82,
	year = 2013}

@article{Breuer2005-bw,
	abstract = {In all current parallel imaging techniques, aliasing artifacts
              resulting from an undersampled acquisition are removed by means
              of a specialized image reconstruction algorithm. In this study a
              new approach termed ``controlled aliasing in parallel imaging
              results in higher acceleration'' (CAIPIRINHA) is presented. This
              technique modifies the appearance of aliasing artifacts during
              the acquisition to improve the subsequent parallel image
              reconstruction procedure. This new parallel multi-slice technique
              is more efficient compared to other multi-slice parallel imaging
              concepts that use only a pure postprocessing approach. In this
              new approach, multiple slices of arbitrary thickness and distance
              are excited simultaneously with the use of multi-band
              radiofrequency (RF) pulses similar to Hadamard pulses. These data
              are then undersampled, yielding superimposed slices that appear
              shifted with respect to each other. The shift of the aliased
              slices is controlled by modulating the phase of the individual
              slices in the multi-band excitation pulse from echo to echo. We
              show that the reconstruction quality of the aliased slices is
              better using this shift. This may potentially allow one to use
              higher acceleration factors than are used in techniques without
              this excitation scheme. Additionally, slices that have
              essentially the same coil sensitivity profiles can be separated
              with this technique.},
	author = {Breuer, Felix A and Blaimer, Martin and Heidemann, Robin M and Mueller, Matthias F and Griswold, Mark A and Jakob, Peter M},
	journal = {Magnetic Resonance in Medicine},
	language = {en},
	month = mar,
	number = 3,
	pages = {684--691},
	title = {Controlled aliasing in parallel imaging results in higher acceleration ({CAIPIRINHA}) for multi-slice imaging},
	volume = 53,
	year = 2005}

@article{Nooner2012-hm,
	abstract = {The National Institute of Mental Health strategic plan for
               advancing psychiatric neuroscience calls for an acceleration of
               discovery and the delineation of developmental trajectories for
               risk and resilience across the lifespan. To attain these
               objectives, sufficiently powered datasets with broad and deep
               phenotypic characterization, state-of-the-art neuroimaging, and
               genetic samples must be generated and made openly available to
               the scientific community. The enhanced Nathan Kline Institute
               Rockland Sample (NKI-RS) is a response to this need. NKI-RS is
               an ongoing, institutionally-centered endeavor aimed at creating
               a large-scale (N>1000), deeply phenotyped,
               community-ascertained, lifespan sample (ages 6-85 years old)
               with advanced neuroimaging and genetics. These data will be
               publically shared, openly and prospectively (i.e., on a weekly
               basis). Herein, we describe the conceptual basis of the NKI-RS,
               including study design, sampling considerations, and steps to
               synchronize phenotypic and neuroimaging assessment.
               Additionally, we describe our process for sharing the data with
               the scientific community while protecting participant
               confidentiality, maintaining an adequate database, and
               certifying data integrity. The pilot phase of the NKI-RS,
               including challenges in recruiting, characterizing, imaging, and
               sharing data, is discussed while also explaining how this
               experience informed the final design of the enhanced NKI-RS. It
               is our hope that familiarity with the conceptual underpinnings
               of the enhanced NKI-RS will facilitate harmonization with future
               data collection efforts aimed at advancing psychiatric
               neuroscience and nosology.},
	author = {Nooner, Kate Brody and Colcombe, Stanley and Tobe, Russell and Mennes, Maarten and Benedict, Melissa and Moreno, Alexis and Panek, Laura and Brown, Shaquanna and Zavitz, Stephen and Li, Qingyang and Sikka, Sharad and Gutman, David and Bangaru, Saroja and Schlachter, Rochelle Tziona and Kamiel, Stephanie and Anwar, Ayesha and Hinz, Caitlin and Kaplan, Michelle and Rachlin, Anna and Adelsberg, Samantha and Cheung, Brian and Khanuja, Ranjit and Yan, Chaogan and Craddock, Cameron and Calhoun, Vincent and Courtney, William and King, Margaret and Wood, Dylan and Cox, Christine and Kelly, Clare and DiMartino, Adriana and Petkova, Eva and Reiss, Philip and Duan, Nancy and Thompsen, Dawn and Biswal, Bharat and Coffey, Barbara and Hoptman, Matthew and Javitt, Daniel C and Pomara, Nunzio and Sidtis, John and Koplewicz, Harold and Castellanos, Francisco Xavier and Leventhal, Bennett and Milham, Michael},
	journal = {Frontiers in Neuroscience},
	keywords = {fMRI; DTI; Developmental; Lifespan; Brain; phenotype; Psychiatry; discovery; Open Science},
	language = {en},
	publisher = {Frontiers},
	title = {The {NKI-Rockland} sample: A model for accelerating the pace of discovery science in psychiatry},
	volume = 0,
	year = 2012}

@inproceedings{Troupin2018-px,
	author = {Troupin, Charles and Mu{\~n}oz, Cristian and Fern{\'a}ndez, Juan Gabriel and R{\'u}jula, Miquel {\`A}ngel},
	conference = {International Conference on Marine Data and Information Systems},
	institution = {Barcelona},
	title = {Scientific results traceability: Software citation using {GitHub} and Zenodo},
	year = 2018}

@article{Scheel2020-lw,
	author = {Scheel, Anne M},
	journal = {Quality of Life Research},
	language = {en},
	month = dec,
	number = 12,
	pages = {3181--3182},
	title = {Registered Reports: A process to safeguard high-quality evidence},
	volume = 29,
	year = 2020}

@article{Cheifet2021-cr,
	author = {Cheifet, Barbara},
	journal = {Genome Biology},
	language = {en},
	month = feb,
	number = 1,
	pages = {65},
	title = {Promoting reproducibility with Code Ocean},
	volume = 22,
	year = 2021}

@unpublished{De_la_Vega2022-xo,
	abstract = {Functional magnetic resonance imaging (fMRI) has revolutionized
              cognitive neuroscience, but methodological barriers limit the
              generalizability of findings from the lab to the real world.
              Here, we present Neuroscout, an end-to-end platform for analysis
              of naturalistic fMRI data designed to facilitate the adoption of
              robust and generalizable research practices. Neuroscout leverages
              state-of-the-art machine learning models to automatically
              annotate stimuli from dozens of naturalistic fMRI studies,
              allowing researchers to easily test neuroscientific hypotheses
              across multiple ecologically-valid datasets. In addition,
              Neuroscout builds on a robust ecosystem of open tools and
              standards to provide an easy-to-use analysis builder and a fully
              automated execution engine that reduce the burden of reproducible
              research. Through a series of meta-analytic case studies, we
              validate the automatic feature extraction approach and
              demonstrate its potential to support more robust fMRI research.
              Owing to its ease of use and a high degree of automation,
              Neuroscout makes it possible to overcome modeling challenges
              commonly arising in naturalistic analysis and to easily scale
              analyses within and across datasets, democratizing generalizable
              fMRI research. \#\#\# Competing Interest Statement The authors
              have declared no competing interest.},
	author = {de la Vega, Alejandro and Rocca, Roberta and Blair, Ross W and Markiewicz, Christopher J and Mentch, Jeff and Kent, James D and Herholz, Peer and Ghosh, Satrajit S and Poldrack, Russell A and Yarkoni, Tal},
	journal = {bioRxiv},
	language = {en},
	month = apr,
	pages = {2022.04.05.487222},
	title = {Neuroscout, a unified platform for generalizable and reproducible {fMRI} research},
	year = 2022}

@article{Taylor2017-nq,
	abstract = {This paper describes the data repository for the Cambridge Centre
              for Ageing and Neuroscience (Cam-CAN) initial study cohort. The
              Cam-CAN Stage 2 repository contains multi-modal (MRI, MEG, and
              cognitive-behavioural) data from a large (approximately N=700),
              cross-sectional adult lifespan (18-87years old) population-based
              sample. The study is designed to characterise age-related changes
              in cognition and brain structure and function, and to uncover the
              neurocognitive mechanisms that support healthy cognitive ageing.
              The database contains raw and preprocessed structural MRI,
              functional MRI (active tasks and resting state), and MEG data
              (active tasks and resting state), as well as derived scores from
              cognitive behavioural experiments spanning five broad domains
              (attention, emotion, action, language, and memory), and
              demographic and neuropsychological data. The dataset thus
              provides a depth of neurocognitive phenotyping that is currently
              unparalleled, enabling integrative analyses of age-related
              changes in brain structure, brain function, and cognition, and
              providing a testbed for novel analyses of multi-modal
              neuroimaging data.},
	author = {Taylor, Jason R and Williams, Nitin and Cusack, Rhodri and Auer, Tibor and Shafto, Meredith A and Dixon, Marie and Tyler, Lorraine K and {Cam-Can} and Henson, Richard N},
	journal = {Neuroimage},
	keywords = {Ageing; Brain imaging; Cognition; Data repository; Magnetic resonance imaging; Magnetoencephalography},
	language = {en},
	month = jan,
	number = {Pt B},
	pages = {262--269},
	title = {The Cambridge Centre for Ageing and Neuroscience ({Cam-CAN}) data repository: Structural and functional {MRI}, {MEG}, and cognitive data from a cross-sectional adult lifespan sample},
	volume = 144,
	year = 2017}

@misc{Andersen2018-un,
	author = {Andersen, Lau M},
	journal = {Frontiers in Neuroscience},
	title = {Group Analysis in {MNE-Python} of Evoked Responses from a Tactile Stimulation Paradigm: A Pipeline for Reproducibility at Every Step of Processing, Going from Individual Sensor Space Representations to an across-Group Source Space Representation},
	volume = 12,
	year = 2018}

@article{Aczel2021-hu,
	abstract = {Any large dataset can be analyzed in a number of ways, and it is
              possible that the use of different analysis strategies will lead
              to different results and conclusions. One way to assess whether
              the results obtained depend on the analysis strategy chosen is to
              employ multiple analysts and leave each of them free to follow
              their own approach. Here, we present consensus-based guidance for
              conducting and reporting such multi-analyst studies, and we
              discuss how broader adoption of the multi-analyst approach has
              the potential to strengthen the robustness of results and
              conclusions obtained from analyses of datasets in basic and
              applied research.},
	author = {Aczel, Balazs and Szaszi, Barnabas and Nilsonne, Gustav and van den Akker, Olmo R and Albers, Casper J and van Assen, Marcel Alm and Bastiaansen, Jojanneke A and Benjamin, Daniel and Boehm, Udo and Botvinik-Nezer, Rotem and Bringmann, Laura F and Busch, Niko A and Caruyer, Emmanuel and Cataldo, Andrea M and Cowan, Nelson and Delios, Andrew and van Dongen, Noah Nn and Donkin, Chris and van Doorn, Johnny B and Dreber, Anna and Dutilh, Gilles and Egan, Gary F and Gernsbacher, Morton Ann and Hoekstra, Rink and Hoffmann, Sabine and Holzmeister, Felix and Huber, Juergen and Johannesson, Magnus and Jonas, Kai J and Kindel, Alexander T and Kirchler, Michael and Kunkels, Yoram K and Lindsay, D Stephen and Mangin, Jean-Francois and Matzke, Dora and Munaf{\`o}, Marcus R and Newell, Ben R and Nosek, Brian A and Poldrack, Russell A and van Ravenzwaaij, Don and Rieskamp, J{\"o}rg and Salganik, Matthew J and Sarafoglou, Alexandra and Schonberg, Tom and Schweinsberg, Martin and Shanks, David and Silberzahn, Raphael and Simons, Daniel J and Spellman, Barbara A and St-Jean, Samuel and Starns, Jeffrey J and Uhlmann, Eric Luis and Wicherts, Jelte and Wagenmakers, Eric-Jan},
	journal = {Elife},
	keywords = {analytical variability; expert consensus; medicine; metascience; multi-analyst; neuroscience; none; science forum; statistical practice},
	language = {en},
	month = nov,
	title = {Consensus-based guidance for conducting and reporting multi-analyst studies},
	volume = 10,
	year = 2021}

@article{Robbins2021-bm,
	abstract = {Event-related data analysis plays a central role in EEG and MEG
               (MEEG) and other neuroimaging modalities such as fMRI. Choices
               about which events to r{\ldots}},
	author = {Robbins, K and Truong, D and Appelhoff, S and Delorme, A and Makeig, S},
	journal = {Neuroimage},
	month = nov,
	pages = {118766},
	publisher = {Academic Press},
	title = {Capturing the nature of events and event context using Hierarchical Event Descriptors ({HED})},
	year = 2021}

@article{Errington2021-xl,
	abstract = {Replicability is an important feature of scientific research, but
              aspects of contemporary research culture, such as an emphasis on
              novelty, can make replicability seem less important than it
              should be. The Reproducibility Project: Cancer Biology was set up
              to provide evidence about the replicability of preclinical
              research in cancer biology by repeating selected experiments from
              high-impact papers. A total of 50 experiments from 23 papers were
              repeated, generating data about the replicability of a total of
              158 effects. Most of the original effects were positive effects
              (136), with the rest being null effects (22). A majority of the
              original effect sizes were reported as numerical values (117),
              with the rest being reported as representative images (41). We
              employed seven methods to assess replicability, and some of these
              methods were not suitable for all the effects in our sample. One
              method compared effect sizes: for positive effects, the median
              effect size in the replications was 85\% smaller than the median
              effect size in the original experiments, and 92\% of replication
              effect sizes were smaller than the original. The other methods
              were binary - the replication was either a success or a failure -
              and five of these methods could be used to assess both positive
              and null effects when effect sizes were reported as numerical
              values. For positive effects, 40\% of replications (39/97)
              succeeded according to three or more of these five methods, and
              for null effects 80\% of replications (12/15) were successful on
              this basis; combining positive and null effects, the success rate
              was 46\% (51/112). A successful replication does not definitively
              confirm an original finding or its theoretical interpretation.
              Equally, a failure to replicate does not disconfirm a finding,
              but it does suggest that additional investigation is needed to
              establish its reliability.},
	author = {Errington, Timothy M and Mathur, Maya and Soderberg, Courtney K and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	journal = {Elife},
	keywords = {Reproducibility Project: Cancer Biology; cancer biology; computational biology; credibility; human; meta-analysis; mouse; replication; reproducibility; reproducibility in cancer biology; systems biology; transparency},
	language = {en},
	month = dec,
	title = {Investigating the replicability of preclinical cancer biology},
	volume = 10,
	year = 2021}

@article{Wilkinson2016-fm,
	abstract = {There is an urgent need to improve the infrastructure supporting
              the reuse of scholarly data. A diverse set of
              stakeholders-representing academia, industry, funding agencies,
              and scholarly publishers-have come together to design and jointly
              endorse a concise and measureable set of principles that we refer
              to as the FAIR Data Principles. The intent is that these may act
              as a guideline for those wishing to enhance the reusability of
              their data holdings. Distinct from peer initiatives that focus on
              the human scholar, the FAIR Principles put specific emphasis on
              enhancing the ability of machines to automatically find and use
              the data, in addition to supporting its reuse by individuals.
              This Comment is the first formal publication of the FAIR
              Principles, and includes the rationale behind them, and some
              exemplar implementations in the community.},
	author = {Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I Jsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E and Bouwman, Jildau and Brookes, Anthony J and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J G and Groth, Paul and Goble, Carole and Grethe, Jeffrey S and Heringa, Jaap and 't Hoen, Peter A C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J and Martone, Maryann E and Mons, Albert and Packer, Abel L and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	journal = {Scientific Data},
	language = {en},
	month = mar,
	pages = {160018},
	title = {The {FAIR} guiding principles for scientific data management and stewardship},
	volume = 3,
	year = 2016}

@article{Pernet2014-xg,
	abstract = {This tutorial presents several misconceptions related to the use
              the General Linear Model (GLM) in functional Magnetic Resonance
              Imaging (fMRI). The goal is not to present mathematical proofs
              but to educate using examples and computer code (in Matlab). In
              particular, I address issues related to (1) model
              parameterization (modeling baseline or null events) and scaling
              of the design matrix; (2) hemodynamic modeling using basis
              functions, and (3) computing percentage signal change. Using a
              simple controlled block design and an alternating block design, I
              first show why ``baseline'' should not be modeled (model
              over-parameterization), and how this affects effect sizes. I also
              show that, depending on what is tested; over-parameterization
              does not necessarily impact upon statistical results. Next, using
              a simple periodic vs. random event related design, I show how the
              hemodynamic model (hemodynamic function only or using
              derivatives) can affects parameter estimates, as well as detail
              the role of orthogonalization. I then relate the above results to
              the computation of percentage signal change. Finally, I discuss
              how these issues affect group analyses and give some
              recommendations.},
	author = {Pernet, Cyril R},
	journal = {Front. Neurosci.},
	keywords = {GLM; baseline; derivatives; fMRI; modeling; percentage signal change},
	language = {en},
	month = jan,
	pages = {1},
	title = {Misconceptions in the use of the General Linear Model applied to functional {MRI}: a tutorial for junior neuro-imagers},
	volume = 8,
	year = 2014}

@misc{Markiewicz2021-yr,
	author = {Markiewicz, Christopher J and De La Vega, Alejandro and Wagner, Adina and Halchenko, Yaroslav O and Finc, Karolina and Ciric, Rastko and Goncalves, Mathias and Nielson, Dylan M and Kent, James D and Lee, John A and Poldrack, Russell A and Gorgolewski, Krzysztof J},
	month = jul,
	publisher = {Zenodo},
	title = {poldracklab/fitlins: v0.9.2},
	year = 2021}

@article{Litvak2011-ik,
	abstract = {SPM is a free and open source software written in MATLAB (The
              MathWorks, Inc.). In addition to standard M/EEG preprocessing, we
              presently offer three main analysis tools: (i) statistical
              analysis of scalp-maps, time-frequency images, and volumetric 3D
              source reconstruction images based on the general linear model,
              with correction for multiple comparisons using random field
              theory; (ii) Bayesian M/EEG source reconstruction, including
              support for group studies, simultaneous EEG and MEG, and fMRI
              priors; (iii) dynamic causal modelling (DCM), an approach
              combining neural modelling with data analysis for which there are
              several variants dealing with evoked responses, steady state
              responses (power spectra and cross-spectra), induced responses,
              and phase coupling. SPM8 is integrated with the FieldTrip toolbox
              , making it possible for users to combine a variety of standard
              analysis methods with new schemes implemented in SPM and build
              custom analysis tools using powerful graphical user interface
              (GUI) and batching tools.},
	author = {Litvak, Vladimir and Mattout, J{\'e}r{\'e}mie and Kiebel, Stefan and Phillips, Christophe and Henson, Richard and Kilner, James and Barnes, Gareth and Oostenveld, Robert and Daunizeau, Jean and Flandin, Guillaume and Penny, Will and Friston, Karl},
	journal = {Computational Intelligence and Neuroscience},
	language = {en},
	month = mar,
	pages = {852961},
	title = {{EEG} and {MEG} data analysis in {SPM8}},
	volume = 2011,
	year = 2011}

@article{McKiernan2016-jy,
	abstract = {Open access, open data, open source and other open scholarship
              practices are growing in popularity and necessity. However,
              widespread adoption of these practices has not yet been achieved.
              One reason is that researchers are uncertain about how sharing
              their work will affect their careers. We review literature
              demonstrating that open research is associated with increases in
              citations, media attention, potential collaborators, job
              opportunities and funding opportunities. These findings are
              evidence that open research practices bring significant benefits
              to researchers relative to more traditional closed practices.},
	author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
	journal = {eLife},
	keywords = {none; open access; open data; open science; open source; research},
	language = {en},
	month = jul,
	title = {How open science helps researchers succeed},
	volume = 5,
	year = 2016}

@article{Maumet2016-zq,
	abstract = {Only a tiny fraction of the data and metadata produced by an
               fMRI study is finally conveyed to the community. This lack of
               transparency not only hinders the reproducibility of
               neuroimaging results but also impairs future meta-analyses. In
               this work we introduce NIDM-Results, a format specification
               providing a machine-readable description of neuroimaging
               statistical results along with key image data summarising the
               experiment. NIDM-Results provides a unified representation of
               mass univariate analyses including a level of detail consistent
               with available best practices. This standardized representation
               allows authors to relay methods and results in a
               platform-independent regularized format that is not tied to a
               particular neuroimaging software package. Tools are available to
               export NIDM-Result graphs and associated files from the widely
               used SPM and FSL software packages, and the NeuroVault
               repository can import NIDM-Results archives. The specification
               is publically available at:
               http://nidm.nidash.org/specs/nidm-results.html .},
	author = {Maumet, Camille and Auer, Tibor and Bowring, Alexander and Chen, Gang and Das, Samir and Flandin, Guillaume and Ghosh, Satrajit and Glatard, Tristan and Gorgolewski, Krzysztof J and Helmer, Karl G and Jenkinson, Mark and Keator, David B and Nichols, B Nolan and Poline, Jean-Baptiste and Reynolds, Richard and Sochat, Vanessa and Turner, Jessica and Nichols, Thomas E},
	journal = {Scientific Data},
	language = {en},
	month = dec,
	number = 1,
	pages = {1--15},
	publisher = {Nature Publishing Group},
	title = {Sharing brain mapping statistical results with the neuroimaging data model},
	volume = 3,
	year = 2016}

@article{Esteban2017-ow,
	abstract = {Quality control of MRI is essential for excluding problematic
              acquisitions and avoiding bias in subsequent image processing and
              analysis. Visual inspection is subjective and impractical for
              large scale datasets. Although automated quality assessments have
              been demonstrated on single-site datasets, it is unclear that
              solutions can generalize to unseen data acquired at new sites.
              Here, we introduce the MRI Quality Control tool (MRIQC), a tool
              for extracting quality measures and fitting a binary
              (accept/exclude) classifier. Our tool can be run both locally and
              as a free online service via the OpenNeuro.org portal. The
              classifier is trained on a publicly available, multi-site dataset
              (17 sites, N = 1102). We perform model selection evaluating
              different normalization and feature exclusion approaches aimed at
              maximizing across-site generalization and estimate an accuracy of
              76\%$\pm$13\% on new sites, using leave-one-site-out
              cross-validation. We confirm that result on a held-out dataset (2
              sites, N = 265) also obtaining a 76\% accuracy. Even though the
              performance of the trained classifier is statistically above
              chance, we show that it is susceptible to site effects and unable
              to account for artifacts specific to new sites. MRIQC performs
              with high accuracy in intra-site prediction, but performance on
              unseen sites leaves space for improvement which might require
              more labeled data and new approaches to the between-site
              variability. Overcoming these limitations is crucial for a more
              objective quality assessment of neuroimaging data, and to enable
              the analysis of extremely large and multi-site samples.},
	author = {Esteban, Oscar and Birman, Daniel and Schaer, Marie and Koyejo, Oluwasanmi O and Poldrack, Russell A and Gorgolewski, Krzysztof J},
	journal = {PLoS One},
	language = {en},
	month = sep,
	number = 9,
	pages = {e0184661},
	title = {{MRIQC}: Advancing the automatic prediction of image quality in {MRI} from unseen sites},
	volume = 12,
	year = 2017}

@article{Sandve2013-kn,
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	journal = {PLoS Computational Biology},
	language = {en},
	month = oct,
	number = 10,
	pages = {e1003285},
	title = {Ten simple rules for reproducible computational research},
	volume = 9,
	year = 2013}

@article{Poldrack2020-vp,
	abstract = {Importance: Great interest exists in identifying methods to
              predict neuropsychiatric disease states and treatment outcomes
              from high-dimensional data, including neuroimaging and genomics
              data. The goal of this review is to highlight several potential
              problems that can arise in studies that aim to establish
              prediction. Observations: A number of neuroimaging studies have
              claimed to establish prediction while establishing only
              correlation, which is an inappropriate use of the statistical
              meaning of prediction. Statistical associations do not
              necessarily imply the ability to make predictions in a
              generalized manner; establishing evidence for prediction thus
              requires testing of the model on data separate from those used to
              estimate the model's parameters. This article discusses various
              measures of predictive performance and the limitations of some
              commonly used measures, with a focus on the importance of using
              multiple measures when assessing performance. For classification,
              the area under the receiver operating characteristic curve is an
              appropriate measure; for regression analysis, correlation should
              be avoided, and median absolute error is preferred. Conclusions
              and Relevance: To ensure accurate estimates of predictive
              validity, the recommended best practices for predictive modeling
              include the following: (1) in-sample model fit indices should not
              be reported as evidence for predictive accuracy, (2) the
              cross-validation procedure should encompass all operations
              applied to the data, (3) prediction analyses should not be
              performed with samples smaller than several hundred observations,
              (4) multiple measures of prediction accuracy should be examined
              and reported, (5) the coefficient of determination should be
              computed using the sums of squares formulation and not the
              correlation coefficient, and (6) k-fold cross-validation rather
              than leave-one-out cross-validation should be used.},
	author = {Poldrack, Russell A and Huckins, Grace and Varoquaux, Gael},
	date-modified = {2022-07-28 01:08:09 +0300},
	journal = {JAMA Psychiatry},
	language = {en},
	month = may,
	number = 5,
	pages = {534--540},
	title = {Establishment of best practices for evidence for prediction: A review},
	volume = 77,
	year = 2020}

@article{Levitis2021-rb,
	abstract = {As the global health crisis unfolded, many academic conferences
              moved online in 2020. This move has been hailed as a positive
              step towards inclusivity in its attenuation of economic,
              physical, and legal barriers and effectively enabled many
              individuals from groups that have traditionally been
              underrepresented to join and participate. A number of studies
              have outlined how moving online made it possible to gather a more
              global community and has increased opportunities for individuals
              with various constraints, e.g., caregiving responsibilities. Yet,
              the mere existence of online conferences is no guarantee that
              everyone can attend and participate meaningfully. In fact, many
              elements of an online conference are still significant barriers
              to truly diverse participation: the tools used can be
              inaccessible for some individuals; the scheduling choices can
              favour some geographical locations; the set-up of the conference
              can provide more visibility to well-established researchers and
              reduce opportunities for early-career researchers. While
              acknowledging the benefits of an online setting, especially for
              individuals who have traditionally been underrepresented or
              excluded, we recognize that fostering social justice requires
              inclusivity to actively be centered in every aspect of online
              conference design. Here, we draw from the literature and from our
              own experiences to identify practices that purposefully encourage
              a diverse community to attend, participate in, and lead online
              conferences. Reflecting on how to design more inclusive online
              events is especially important as multiple scientific
              organizations have announced that they will continue offering an
              online version of their event when in-person conferences can
              resume.},
	author = {Levitis, Elizabeth and van Praag, Cassandra D Gould and Gau, R{\'e}mi and Heunis, Stephan and DuPre, Elizabeth and Kiar, Gregory and Bottenhorn, Katherine L and Glatard, Tristan and Nikolaidis, Aki and Whitaker, Kirstie Jane and Mancini, Matteo and Niso, Guiomar and Afyouni, Soroosh and Alonso-Ortiz, Eva and Appelhoff, Stefan and Arnatkeviciute, Aurina and Atay, Selim Melvin and Auer, Tibor and Baracchini, Giulia and Bayer, Johanna M M and Beauvais, Michael J S and Bijsterbosch, Janine D and Bilgin, Isil P and Bollmann, Saskia and Bollmann, Steffen and Botvinik-Nezer, Rotem and Bright, Molly G and Calhoun, Vince D and Chen, Xiao and Chopra, Sidhant and Chuan-Peng, Hu and Close, Thomas G and Cookson, Savannah L and Craddock, R Cameron and De La Vega, Alejandro and De Leener, Benjamin and Demeter, Damion V and Di Maio, Paola and Dickie, Erin W and Eickhoff, Simon B and Esteban, Oscar and Finc, Karolina and Frigo, Matteo and Ganesan, Saampras and Ganz, Melanie and Garner, Kelly G and Garza-Villarreal, Eduardo A and Gonzalez-Escamilla, Gabriel and Goswami, Rohit and Griffiths, John D and Grootswagers, Tijl and Guay, Samuel and Guest, Olivia and Handwerker, Daniel A and Herholz, Peer and Heuer, Katja and Huijser, Dorien C and Iacovella, Vittorio and Joseph, Michael J E and Karakuzu, Agah and Keator, David B and Kobeleva, Xenia and Kumar, Manoj and Laird, Angela R and Larson-Prior, Linda J and Lautarescu, Alexandra and Lazari, Alberto and Legarreta, Jon Haitz and Li, Xue-Ying and Lv, Jinglei and Mansour L, Sina and Meunier, David and Moraczewski, Dustin and Nandi, Tulika and Nastase, Samuel A and Nau, Matthias and Noble, Stephanie and Norgaard, Martin and Obungoloch, Johnes and Oostenveld, Robert and Orchard, Edwina R and Pinho, Ana Lu{\'\i}sa and Poldrack, Russell A and Qiu, Anqi and Raamana, Pradeep Reddy and Rokem, Ariel and Rutherford, Saige and Sharan, Malvika and Shaw, Thomas B and Syeda, Warda T and Testerman, Meghan M and Toro, Roberto and Valk, Sofie L and Van Den Bossche, Sofie and Varoquaux, Ga{\"e}l and V{\'a}{\v s}a, Franti{\v s}ek and Veldsman, Michele and Vohryzek, Jakub and Wagner, Adina S and Walsh, Reubs J and White, Tonya and Wong, Fu-Te and Xie, Xihe and Yan, Chao-Gan and Yang, Yu-Fang and Yee, Yohan and Zanitti, Gaston E and Van Gulick, Ana E and Duff, Eugene and Maumet, Camille},
	journal = {GigaScience},
	keywords = {collaborative events; diversity; inclusivity; online conferences; open science},
	language = {en},
	month = aug,
	number = 8,
	title = {Centering inclusivity in the design of online conferences: An {OHBM-Open} Science perspective},
	volume = 10,
	year = 2021}

@article{Van_Essen2012-jg,
	abstract = {The Human Connectome Project (HCP) is an ambitious 5-year effort
              to characterize brain connectivity and function and their
              variability in healthy adults. This review summarizes the data
              acquisition plans being implemented by a consortium of HCP
              investigators who will study a population of 1200 subjects (twins
              and their non-twin siblings) using multiple imaging modalities
              along with extensive behavioral and genetic data. The imaging
              modalities will include diffusion imaging (dMRI), resting-state
              fMRI (R-fMRI), task-evoked fMRI (T-fMRI), T1- and T2-weighted MRI
              for structural and myelin mapping, plus combined
              magnetoencephalography and electroencephalography (MEG/EEG).
              Given the importance of obtaining the best possible data quality,
              we discuss the efforts underway during the first two years of the
              grant (Phase I) to refine and optimize many aspects of HCP data
              acquisition, including a new 7T scanner, a customized 3T scanner,
              and improved MR pulse sequences.},
	author = {Van Essen, D C and Ugurbil, K and Auerbach, E and Barch, D and Behrens, T E J and Bucholz, R and Chang, A and Chen, L and Corbetta, M and Curtiss, S W and Della Penna, S and Feinberg, D and Glasser, M F and Harel, N and Heath, A C and Larson-Prior, L and Marcus, D and Michalareas, G and Moeller, S and Oostenveld, R and Petersen, S E and Prior, F and Schlaggar, B L and Smith, S M and Snyder, A Z and Xu, J and Yacoub, E and {WU-Minn HCP Consortium}},
	journal = {Neuroimage},
	language = {en},
	month = oct,
	number = 4,
	pages = {2222--2231},
	title = {The Human Connectome Project: A data acquisition perspective},
	volume = 62,
	year = 2012}

@article{Peirce2019-cs,
	abstract = {PsychoPy is an application for the creation of experiments in
              behavioral science (psychology, neuroscience, linguistics, etc.)
              with precise spatial control and timing of stimuli. It now
              provides a choice of interface; users can write scripts in Python
              if they choose, while those who prefer to construct experiments
              graphically can use the new Builder interface. Here we describe
              the features that have been added over the last 10 years of its
              development. The most notable addition has been that Builder
              interface, allowing users to create studies with minimal or no
              programming, while also allowing the insertion of Python code for
              maximal flexibility. We also present some of the other new
              features, including further stimulus options, asynchronous
              time-stamped hardware polling, and better support for open
              science and reproducibility. Tens of thousands of users now
              launch PsychoPy every month, and more than 90 people have
              contributed to the code. We discuss the current state of the
              project, as well as plans for the future.},
	author = {Peirce, Jonathan and Gray, Jeremy R and Simpson, Sol and MacAskill, Michael and H{\"o}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
	journal = {Behavior Research Methods},
	keywords = {Experiment; Open science; Open-source; Psychology; Reaction time; Software; Timing},
	language = {en},
	month = feb,
	number = 1,
	pages = {195--203},
	title = {{PsychoPy2}: Experiments in behavior made easy},
	volume = 51,
	year = 2019}

@misc{Gau2022-ob,
	author = {Gau, R{\'e}mi and Flandin, Guillaume and Janke, Andrew and {tanguyduval} and Oostenveld, Robert and Madan, Christopher and Niso Gal{\'a}n, Guiomar and Szczepanik, Micha{\l} and Mutsaerts, Henk and Beliy, Nikita and Norgaard, Martin and Pernet, Cyril and Chrisophe, Phillips},
	month = jan,
	title = {bids-matlab},
	year = 2022}

@article{Wicherts2011-gd,
	abstract = {BACKGROUND: The widespread reluctance to share published research
              data is often hypothesized to be due to the authors' fear that
              reanalysis may expose errors in their work or may produce
              conclusions that contradict their own. However, these hypotheses
              have not previously been studied systematically. METHODS AND
              FINDINGS: We related the reluctance to share research data for
              reanalysis to 1148 statistically significant results reported in
              49 papers published in two major psychology journals. We found
              the reluctance to share data to be associated with weaker
              evidence (against the null hypothesis of no effect) and a higher
              prevalence of apparent errors in the reporting of statistical
              results. The unwillingness to share data was particularly clear
              when reporting errors had a bearing on statistical significance.
              CONCLUSIONS: Our findings on the basis of psychological papers
              suggest that statistical results are particularly hard to verify
              when reanalysis is more likely to lead to contrasting
              conclusions. This highlights the importance of establishing
              mandatory data archiving policies.},
	author = {Wicherts, Jelte M and Bakker, Marjan and Molenaar, Dylan},
	journal = {PLoS One},
	language = {en},
	month = nov,
	number = 11,
	pages = {e26828},
	title = {Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results},
	volume = 6,
	year = 2011}

@article{Poldrack2011-bv,
	abstract = {Cognitive neuroscience aims to map mental processes onto brain
              function, which begs the question of what ``mental processes''
              exist and how they relate to the tasks that are used to
              manipulate and measure them. This topic has been addressed
              informally in prior work, but we propose that cumulative progress
              in cognitive neuroscience requires a more systematic approach to
              representing the mental entities that are being mapped to brain
              function and the tasks used to manipulate and measure mental
              processes. We describe a new open collaborative project that aims
              to provide a knowledge base for cognitive neuroscience, called
              the Cognitive Atlas (accessible online at
              http://www.cognitiveatlas.org), and outline how this project has
              the potential to drive novel discoveries about both mind and
              brain.},
	author = {Poldrack, Russell A and Kittur, Aniket and Kalar, Donald and Miller, Eric and Seppa, Christian and Gil, Yolanda and Parker, D Stott and Sabb, Fred W and Bilder, Robert M},
	journal = {Frontiers in Neuroinformatics},
	keywords = {cognitive science; informatics; neuroimaging; ontology},
	language = {en},
	month = sep,
	pages = {17},
	title = {The cognitive atlas: Toward a knowledge foundation for cognitive neuroscience},
	volume = 5,
	year = 2011}

@article{Pernet2020-zo,
	abstract = {Reproducibility is a cornerstone of scientific communication
              without which one cannot build upon each other's work. Because
              modern human brain imaging relies on many integrated steps with a
              variety of possible algorithms, it has, however, become
              impossible to report every detail of a data processing workflow.
              In response to this analytical complexity, community
              recommendations are to share data analysis pipelines (scripts
              that implement workflows). Here we show that this can easily be
              done using EEGLAB and tools built around it. BIDS tools allow
              importing all the necessary information and create a study from
              electroencephalography (EEG)-Brain Imaging Data Structure
              compliant data. From there preprocessing can be carried out in
              only a few steps using EEGLAB and statistical analyses performed
              using the LIMO EEG plug-in. Using Wakeman and Henson (2015) face
              dataset, we illustrate how to prepare data and build different
              statistical models, a standard factorial design (faces ∗
              repetition), and a more modern trial-based regression approach
              for the stimulus repetition effect, all in a few reproducible
              command lines.},
	author = {Pernet, Cyril R and Martinez-Cancino, Ramon and Truong, Dung and Makeig, Scott and Delorme, Arnaud},
	journal = {Front. Neurosci.},
	keywords = {EEGLAB toolbox; LIMO EEG; brain imaging data structure; linear models; preprocessing algorithm; reproducibility and tools},
	language = {en},
	pages = {610388},
	title = {From {BIDS-Formatted} {EEG} Data to {Sensor-Space} Group Results: A Fully Reproducible Workflow With {EEGLAB} and {LIMO} {EEG}},
	volume = 14,
	year = 2020}

@article{Devezer2021-lk,
	abstract = {Current attempts at methodological reform in sciences come in
              response to an overall lack of rigor in methodological and
              scientific practices in experimental sciences. However, most
              methodological reform attempts suffer from similar mistakes and
              over-generalizations to the ones they aim to address. We argue
              that this can be attributed in part to lack of formalism and
              first principles. Considering the costs of allowing false claims
              to become canonized, we argue for formal statistical rigor and
              scientific nuance in methodological reform. To attain this rigor
              and nuance, we propose a five-step formal approach for solving
              methodological problems. To illustrate the use and benefits of
              such formalism, we present a formal statistical analysis of three
              popular claims in the metascientific literature: (i) that
              reproducibility is the cornerstone of science; (ii) that data
              must not be used twice in any analysis; and (iii) that
              exploratory projects imply poor statistical practice. We show how
              our formal approach can inform and shape debates about such
              methodological claims.},
	author = {Devezer, Berna and Navarro, Danielle J and Vandekerckhove, Joachim and Ozge Buzbas, Erkan},
	journal = {Royal Society Open Science},
	keywords = {double-dipping; exploratory research; replication; reproducibility; scientific reform},
	language = {en},
	month = mar,
	number = 3,
	pages = {200805},
	title = {The case for formal methodology in scientific reform},
	volume = 8,
	year = 2021}

@book{Penny2011-nd,
	abstract = {In an age where the amount of data collected from brain imaging
               is increasing constantly, it is of critical importance to
               analyse those data within an accepted framework to ensure proper
               integration and comparison of the information collected. This
               book describes the ideas and procedures that underlie the
               analysis of signals produced by the brain. The aim is to
               understand how the brain works, in terms of its functional
               architecture and dynamics. This book provides the background and
               methodology for the analysis of all types of brain imaging data,
               from functional magnetic resonance imaging to
               magnetoencephalography. Critically, Statistical Parametric
               Mapping provides a widely accepted conceptual framework which
               allows treatment of all these different modalities. This rests
               on an understanding of the brain's functional anatomy and the
               way that measured signals are caused experimentally. The book
               takes the reader from the basic concepts underlying the analysis
               of neuroimaging data to cutting edge approaches that would be
               difficult to find in any other source. Critically, the material
               is presented in an incremental way so that the reader can
               understand the precedents for each new development. This book
               will be particularly useful to neuroscientists engaged in any
               form of brain mapping; who have to contend with the real-world
               problems of data analysis and understanding the techniques they
               are using. It is primarily a scientific treatment and a didactic
               introduction to the analysis of brain imaging data. It can be
               used as both a textbook for students and scientists starting to
               use the techniques, as well as a reference for practicing
               neuroscientists. The book also serves as a companion to the
               software packages that have been developed for brain imaging
               data analysis. An essential reference and companion for users of
               the SPM software Provides a complete description of the concepts
               and procedures entailed by the analysis of brain images Offers
               full didactic treatment of the basic mathematics behind the
               analysis of brain imaging data Stands as a compendium of all the
               advances in neuroimaging data analysis over the past decade
               Adopts an easy to understand and incremental approach that takes
               the reader from basic statistics to state of the art approaches
               such as Variational Bayes Structured treatment of data analysis
               issues that links different modalities and models Includes a
               series of appendices and tutorial-style chapters that makes even
               the most sophisticated approaches accessible},
	author = {Penny, William D and Friston, Karl J and Ashburner, John T and Kiebel, Stefan J and Nichols, Thomas E},
	language = {en},
	month = apr,
	publisher = {Elsevier},
	title = {Statistical Parametric Mapping: The analysis of functional brain images},
	year = 2011}

@misc{Shafto2014-pe,
	author = {Shafto, Meredith A and {Cam-CAN} and Tyler, Lorraine K and Dixon, Marie and Taylor, Jason R and Rowe, James B and Cusack, Rhodri and Calder, Andrew J and Marslen-Wilson, William D and Duncan, John and Dalgleish, Tim and Henson, Richard N and Brayne, Carol and Matthews, Fiona E},
	journal = {BMC Neurology},
	number = 1,
	title = {The Cambridge Centre for Ageing and Neuroscience ({Cam-CAN}) study protocol: a cross-sectional, lifespan, multidisciplinary examination of healthy cognitive ageing},
	volume = 14,
	year = 2014}

@article{Kiar2020-bh,
	abstract = {With an increase in awareness regarding a troubling lack of
              reproducibility in analytical software tools, the degree of
              validity in scientific derivatives and their downstream results
              has become unclear. The nature of reproducibility issues may vary
              across domains, tools, data sets, and computational
              infrastructures, but numerical instabilities are thought to be a
              core contributor. In neuroimaging, unexpected deviations have
              been observed when varying operating systems, software
              implementations, or adding negligible quantities of noise. In the
              field of numerical analysis, these issues have recently been
              explored through Monte Carlo Arithmetic, a method involving the
              instrumentation of floating-point operations with probabilistic
              noise injections at a target precision. Exploring multiple
              simulations in this context allows the characterization of the
              result space for a given tool or operation. In this article, we
              compare various perturbation models to introduce instabilities
              within a typical neuroimaging pipeline, including (i) targeted
              noise, (ii) Monte Carlo Arithmetic, and (iii) operating system
              variation, to identify the significance and quality of their
              impact on the resulting derivatives. We demonstrate that even
              low-order models in neuroimaging such as the structural
              connectome estimation pipeline evaluated here are sensitive to
              numerical instabilities, suggesting that stability is a relevant
              axis upon which tools are compared, alongside more traditional
              criteria such as biological feasibility, computational
              efficiency, or, when possible, accuracy. Heterogeneity was
              observed across participants which clearly illustrates a strong
              interaction between the tool and data set being processed,
              requiring that the stability of a given tool be evaluated with
              respect to a given cohort. We identify use cases for each
              perturbation method tested, including quality assurance, pipeline
              error detection, and local sensitivity analysis, and make
              recommendations for the evaluation of stability in a practical
              and analytically focused setting. Identifying how these
              relationships and recommendations scale to higher order
              computational tools, distinct data sets, and their implication on
              biological feasibility remain exciting avenues for future work.},
	author = {Kiar, Gregory and de Oliveira Castro, Pablo and Rioux, Pierre and Petit, Eric and Brown, Shawn T and Evans, Alan C and Glatard, Tristan},
	journal = {The International Journal of High Performance Computing Applications},
	keywords = {Monte Carlo Arithmetic; Neuroimaging; diffusion MRI; stability},
	language = {en},
	month = sep,
	number = 5,
	pages = {491--501},
	title = {Comparing perturbation models for evaluating stability of neuroimaging pipelines},
	volume = 34,
	year = 2020}

@article{Simmons2021-tc,
	abstract = {In this article, we (1) discuss the reasons why pre-registration
               is a good idea, both for the field and individual researchers,
               (2) respond to arguments against pre-registration, (3) describe
               how to best write and review a pre-registration, and (4) comment
               on pre-registration?s rapidly accelerating popularity. Along the
               way, we describe the (big) problem that pre-registration can
               solve (i.e., false positives caused by p-hacking), while also
               offering viable solutions to the problems that pre-registration
               cannot solve (e.g., hidden confounds or fraud). Pre-registration
               does not guarantee that every published finding will be true,
               but without it you can safely bet that many more will be false.
               It is time for our field to embrace pre-registration, while
               taking steps to ensure that it is done right.},
	author = {Simmons, Joseph P and Nelson, Leif and Simonsohn, Uri},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	journal = {Journal of Consumer Psychology},
	language = {en},
	month = jan,
	number = 1,
	pages = {151--162},
	publisher = {Wiley},
	title = {Pre‐registration: Why and how},
	volume = 31,
	year = 2021}

@misc{Gelman2013-dw,
	author = {Gelman, A and Loken, E},
	howpublished = {\url{http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf}},
	note = {Accessed: NA-NA-NA},
	title = {The Garden of Forking Paths: Why Multiple Comparisons Can Be a Problem, Even When There Is No `fishing Expedition' or `p-Hacking' and the Research Hypothesis Was Posited ahead of Time},
	year = 2013}

@article{Jenkinson2012-az,
	abstract = {FSL (the FMRIB Software Library) is a comprehensive library of
              analysis tools for functional, structural and diffusion MRI brain
              imaging data, written mainly by members of the Analysis Group,
              FMRIB, Oxford. For this NeuroImage special issue on ``20 years of
              fMRI'' we have been asked to write about the history,
              developments and current status of FSL. We also include some
              descriptions of parts of FSL that are not well covered in the
              existing literature. We hope that some of this content might be
              of interest to users of FSL, and also maybe to new research
              groups considering creating, releasing and supporting new
              software packages for brain image analysis.},
	author = {Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E J and Woolrich, Mark W and Smith, Stephen M},
	journal = {Neuroimage},
	language = {en},
	month = aug,
	number = 2,
	pages = {782--790},
	title = {{FSL}},
	volume = 62,
	year = 2012}

@article{Markowetz2015-la,
	abstract = {And so, my fellow scientists: ask not what you can do for
              reproducibility; ask what reproducibility can do for you! Here, I
              present five reasons why working reproducibly pays off in the
              long run and is in the self-interest of every ambitious,
              career-oriented scientist.},
	author = {Markowetz, Florian},
	journal = {Genome Biol.},
	language = {en},
	month = dec,
	pages = {274},
	title = {Five selfish reasons to work reproducibly},
	volume = 16,
	year = 2015}

@article{Esteban2019-pm,
	abstract = {Preprocessing of functional magnetic resonance imaging (fMRI)
              involves numerous steps to clean and standardize the data before
              statistical analysis. Generally, researchers create ad hoc
              preprocessing workflows for each dataset, building upon a large
              inventory of available tools. The complexity of these workflows
              has snowballed with rapid advances in acquisition and processing.
              We introduce fMRIPrep, an analysis-agnostic tool that addresses
              the challenge of robust and reproducible preprocessing for fMRI
              data. fMRIPrep automatically adapts a best-in-breed workflow to
              the idiosyncrasies of virtually any dataset, ensuring
              high-quality preprocessing without manual intervention. By
              introducing visual assessment checkpoints into an iterative
              integration framework for software testing, we show that fMRIPrep
              robustly produces high-quality results on a diverse fMRI data
              collection. Additionally, fMRIPrep introduces less uncontrolled
              spatial smoothness than observed with commonly used preprocessing
              tools. fMRIPrep equips neuroscientists with an easy-to-use and
              transparent preprocessing workflow, which can help ensure the
              validity of inference and the interpretability of results.},
	author = {Esteban, Oscar and Markiewicz, Christopher J and Blair, Ross W and Moodie, Craig A and Isik, A Ilkay and Erramuzpe, Asier and Kent, James D and Goncalves, Mathias and DuPre, Elizabeth and Snyder, Madeleine and Oya, Hiroyuki and Ghosh, Satrajit S and Wright, Jessey and Durnez, Joke and Poldrack, Russell A and Gorgolewski, Krzysztof J},
	journal = {Nature Methods},
	language = {en},
	month = jan,
	number = 1,
	pages = {111--116},
	title = {{fMRIPrep}: A robust preprocessing pipeline for functional {MRI}},
	volume = 16,
	year = 2019}

@article{Nosek2022-hh,
	abstract = {Replication-an important, uncommon, and misunderstood practice-is
              gaining appreciation in psychology. Achieving replicability is
              important for making research progress. If findings are not
              replicable, then prediction and theory development are stifled.
              If findings are replicable, then interrogation of their meaning
              and validity can advance knowledge. Assessing replicability can
              be productive for generating and testing hypotheses by actively
              confronting current understandings to identify weaknesses and
              spur innovation. For psychology, the 2010s might be characterized
              as a decade of active confrontation. Systematic and multi-site
              replication projects assessed current understandings and observed
              surprising failures to replicate many published findings.
              Replication efforts highlighted sociocultural challenges such as
              disincentives to conduct replications and a tendency to frame
              replication as a personal attack rather than a healthy scientific
              practice, and they raised awareness that replication contributes
              to self-correction. Nevertheless, innovation in doing and
              understanding replication and its cousins, reproducibility and
              robustness, has positioned psychology to improve research
              practices and accelerate progress.},
	author = {Nosek, Brian A and Hardwicke, Tom E and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B and Rohrer, Julia M and Romero, Felipe and Scheel, Anne M and Scherer, Laura D and Sch{\"o}nbrodt, Felix D and Vazire, Simine},
	journal = {Annu. Rev. Psychol.},
	keywords = {generalizability; metascience; replication; reproducibility; research methods; robustness; statistical inference; theory; validity},
	language = {en},
	month = jan,
	pages = {719--748},
	title = {Replicability, Robustness, and Reproducibility in Psychological Science},
	volume = 73,
	year = 2022}

@article{Pernet2020-ep,
	abstract = {The Organization for Human Brain Mapping (OHBM) has been active
              in advocating for the instantiation of best practices in
              neuroimaging data acquisition, analysis, reporting and sharing of
              both data and analysis code to deal with issues in science
              related to reproducibility and replicability. Here we summarize
              recommendations for such practices in magnetoencephalographic
              (MEG) and electroencephalographic (EEG) research, recently
              developed by the OHBM neuroimaging community known by the
              abbreviated name of COBIDAS MEEG. We discuss the rationale for
              the guidelines and their general content, which encompass many
              topics under active discussion in the field. We highlight future
              opportunities and challenges to maximizing the sharing and
              exploitation of MEG and EEG data, and we also discuss how this
              'living' set of guidelines will evolve to continually address new
              developments in neurophysiological assessment methods and
              multimodal integration of neurophysiological data with other data
              types.},
	author = {Pernet, Cyril R and Garrido, Marta I and Gramfort, Alexandre and Maurits, Natasha and Michel, Christoph M and Pang, Elizabeth and Salmelin, Riitta and Schoffelen, Jan Mathijs and Valdes-Sosa, Pedro A and Puce, Aina},
	journal = {Nature Neuroscience},
	language = {en},
	month = dec,
	number = 12,
	pages = {1473--1483},
	title = {Issues and recommendations from the {OHBM} {COBIDAS} {MEEG} committee for reproducible {EEG} and {MEG} research},
	volume = 23,
	year = 2020}

@article{Feldstein_Ewing2018-og,
	author = {Feldstein Ewing, S and Luciana, M},
	journal = {Developmental Cognitive Neuroscience},
	pages = {1--164},
	title = {The Adolescent Brain Cognitive Development ({ABCD}) Consortium: Rationale, aims, and assessment strategy [Special Issue]},
	volume = 32,
	year = 2018}

@article{Kappenman2021-xs,
	abstract = {Event-related potentials (ERPs) are noninvasive measures of human
              brain activity that index a range of sensory, cognitive,
              affective, and motor processes. Despite their broad application
              across basic and clinical research, there is little
              standardization of ERP paradigms and analysis protocols across
              studies. To address this, we created ERP CORE (Compendium of Open
              Resources and Experiments), a set of optimized paradigms,
              experiment control scripts, data processing pipelines, and sample
              data (N = 40 neurotypical young adults) for seven widely used ERP
              components: N170, mismatch negativity (MMN), N2pc, N400, P3,
              lateralized readiness potential (LRP), and error-related
              negativity (ERN). This resource makes it possible for researchers
              to 1) employ standardized ERP paradigms in their research, 2)
              apply carefully designed analysis pipelines and use a priori
              selected parameters for data processing, 3) rigorously assess the
              quality of their data, and 4) test new analytic techniques with
              standardized data from a wide range of paradigms.},
	author = {Kappenman, Emily S and Farrens, Jaclyn L and Zhang, Wendy and Stewart, Andrew X and Luck, Steven J},
	journal = {Neuroimage},
	keywords = {Data quality; EEG; Event-related potentials; Open science; Reproducibility},
	language = {en},
	month = jan,
	pages = {117465},
	title = {{ERP} {CORE}: An open resource for human event-related potential research},
	volume = 225,
	year = 2021}

@article{Andersen2018-re,
	abstract = {An important aim of an analysis pipeline for
              magnetoencephalographic (MEG) data is that it allows for the
              researcher spending maximal effort on making the statistical
              comparisons that will answer his or her questions. The example
              question being answered here is whether the so-called beta
              rebound differs between novel and repeated stimulations. Two
              analyses are presented: going from individual sensor space
              representations to, respectively, an across-group sensor space
              representation and an across-group source space representation.
              The data analyzed are neural responses to tactile stimulations of
              the right index finger in a group of 20 healthy participants
              acquired from an Elekta Neuromag System. The processing steps
              covered for the first analysis are MaxFiltering the raw data,
              defining, preprocessing and epoching the data, cleaning the data,
              finding and removing independent components related to eye
              blinks, eye movements and heart beats, calculating participants'
              individual evoked responses by averaging over epoched data and
              subsequently removing the average response from single epochs,
              calculating a time-frequency representation and baselining it
              with non-stimulation trials and finally calculating a grand
              average, an across-group sensor space representation. The second
              analysis starts from the grand average sensor space
              representation and after identification of the beta rebound the
              neural origin is imaged using beamformer source reconstruction.
              This analysis covers reading in co-registered magnetic resonance
              images, segmenting the data, creating a volume conductor,
              creating a forward model, cutting out MEG data of interest in the
              time and frequency domains, getting Fourier transforms and
              estimating source activity with a beamformer model where power is
              expressed relative to MEG data measured during periods of
              non-stimulation. Finally, morphing the source estimates onto a
              common template and performing group-level statistics on the data
              are covered. Functions for saving relevant figures in an
              automated and structured manner are also included. The protocol
              presented here can be applied to any research protocol where the
              emphasis is on source reconstruction of induced responses where
              the underlying sources are not coherent.},
	author = {Andersen, Lau M},
	journal = {Front. Neurosci.},
	keywords = {MEG; analysis pipeline; beamformer; fieldtrip; good practice; group analysis; tactile expectations},
	language = {en},
	month = may,
	pages = {261},
	title = {Group Analysis in {FieldTrip} of {Time-Frequency} Responses: A Pipeline for Reproducibility at Every Step of Processing, Going From Individual Sensor Space Representations to an {Across-Group} Source Space Representation},
	volume = 12,
	year = 2018}

@article{Nature2015-cq,
	author = {{Nature}},
	journal = {Nature},
	language = {en},
	month = oct,
	number = 7572,
	pages = {163},
	title = {Let's think about cognitive bias},
	volume = 526,
	year = 2015}

@article{Schafer2019-us,
	abstract = {Effect sizes are the currency of psychological research. They
              quantify the results of a study to answer the research question
              and are used to calculate statistical power. The interpretation
              of effect sizes-when is an effect small, medium, or large?-has
              been guided by the recommendations Jacob Cohen gave in his
              pioneering writings starting in 1962: Either compare an effect
              with the effects found in past research or use certain
              conventional benchmarks. The present analysis shows that neither
              of these recommendations is currently applicable. From past
              publications without pre-registration, 900 effects were randomly
              drawn and compared with 93 effects from publications with
              pre-registration, revealing a large difference: Effects from the
              former (median = 0.36) were much larger than effects from the
              latter (median = 0.16). That is, certain biases, such as
              publication bias or questionable research practices, have caused
              a dramatic inflation in published effects, making it difficult to
              compare an actual effect with the real population effects (as
              these are unknown). In addition, there were very large
              differences in the mean effects between psychological
              sub-disciplines and between different study designs, making it
              impossible to apply any global benchmarks. Many more
              pre-registered studies are needed in the future to derive a
              reliable picture of real population effects.},
	author = {Sch{\"a}fer, Thomas and Schwarz, Marcus A},
	journal = {Frontiers in Psychology},
	keywords = {Cohen; effect size; replicability; sample size; statistical power},
	language = {en},
	month = apr,
	pages = {813},
	title = {The meaningfulness of effect sizes in psychological research: Differences between sub-disciplines and the impact of potential biases},
	volume = 10,
	year = 2019}

@article{Amunts2019-nk,
	abstract = {The Human Brain Project (HBP) is a European flagship project with
              a 10-year horizon aiming to understand the human brain and to
              translate neuroscience knowledge into medicine and technology. To
              achieve such aims, the HBP explores the multilevel complexity of
              the brain in space and time; transfers the acquired knowledge to
              brain-derived applications in health, computing, and technology;
              and provides shared and open computing tools and data through the
              HBP European brain research infrastructure. We discuss how the
              HBP creates a transdisciplinary community of researchers united
              by the quest to understand the brain, with fascinating
              perspectives on societal benefits.},
	author = {Amunts, Katrin and Knoll, Alois C and Lippert, Thomas and Pennartz, Cyriel M A and Ryvlin, Philippe and Destexhe, Alain and Jirsa, Viktor K and D'Angelo, Egidio and Bjaalie, Jan G},
	journal = {PLoS Biology},
	language = {en},
	month = jul,
	number = 7,
	pages = {e3000344},
	title = {The Human Brain Project: Synergy between neuroscience, computing, informatics, and brain-inspired technologies},
	volume = 17,
	year = 2019}

@article{Styles2021-wj,
	abstract = {As the number of EEG papers increases, so too do the number of
               guidelines for how to report what has been done. However,
               current guidelines and checkl{\ldots}},
	author = {Styles, Suzy J and Kovi{\'c}, Vanja and Ke, Han and {\v S}o{\v s}ki{\'c}, An{\dj}ela},
	journal = {Neuroimage},
	month = dec,
	pages = {118721},
	publisher = {Academic Press},
	title = {Towards {ARTEM-IS}: Design guidelines for evidence-based {EEG} methodology reporting tools},
	volume = 245,
	year = 2021}

@article{Milham2018-pr,
	abstract = {Data sharing is increasingly recommended as a means of
              accelerating science by facilitating collaboration, transparency,
              and reproducibility. While few oppose data sharing
              philosophically, a range of barriers deter most researchers from
              implementing it in practice. To justify the significant effort
              required for sharing data, funding agencies, institutions, and
              investigators need clear evidence of benefit. Here, using the
              International Neuroimaging Data-sharing Initiative, we present a
              case study that provides direct evidence of the impact of open
              sharing on brain imaging data use and resulting peer-reviewed
              publications. We demonstrate that openly shared data can increase
              the scale of scientific studies conducted by data contributors,
              and can recruit scientists from a broader range of disciplines.
              These findings dispel the myth that scientific findings using
              shared data cannot be published in high-impact journals, suggest
              the transformative power of data sharing for accelerating
              science, and underscore the need for implementing data sharing
              universally.},
	author = {Milham, Michael P and Craddock, R Cameron and Son, Jake J and Fleischmann, Michael and Clucas, Jon and Xu, Helen and Koo, Bonhwang and Krishnakumar, Anirudh and Biswal, Bharat B and Castellanos, F Xavier and Colcombe, Stan and Di Martino, Adriana and Zuo, Xi-Nian and Klein, Arno},
	journal = {Nat. Commun.},
	language = {en},
	month = jul,
	number = 1,
	pages = {2818},
	title = {Assessment of the impact of shared brain imaging data on the scientific literature},
	volume = 9,
	year = 2018}

@article{Pernet2019-wa,
	author = {Pernet, Cyril R and Appelhoff, Stefan and Gorgolewski, Krzysztof J and Flandin, Guillaume and Phillips, Christophe and Delorme, Arnaud and Oostenveld, Robert},
	journal = {Scientific data},
	language = {en},
	month = jun,
	number = 1,
	pages = {103},
	title = {{EEG-BIDS}: An extension to the brain imaging data structure for electroencephalography},
	volume = 6,
	year = 2019}

@article{Maier2021-rj,
	abstract = {PURPOSE: The aim of this work is to shed light on the issue of
              reproducibility in MR image reconstruction in the context of a
              challenge. Participants had to recreate the results of ``Advances
              in sensitivity encoding with arbitrary k-space trajectories'' by
              Pruessmann et al. METHODS: The task of the challenge was to
              reconstruct radially acquired multicoil k-space data
              (brain/heart) following the method in the original paper,
              reproducing its key figures. Results were compared to
              consolidated reference implementations created after the
              challenge, accounting for the two most common programming
              languages used in the submissions (Matlab/Python). RESULTS:
              Visually, differences between submissions were small. Pixel-wise
              differences originated from image orientation, assumed
              field-of-view, or resolution. The reference implementations were
              in good agreement, both visually and in terms of image similarity
              metrics. DISCUSSION AND CONCLUSION: While the description level
              of the published algorithm enabled participants to reproduce
              CG-SENSE in general, details of the implementation varied, for
              example, density compensation or Tikhonov regularization.
              Implicit assumptions about the data lead to further differences,
              emphasizing the importance of sufficient metadata accompanying
              open datasets. Defining reproducibility quantitatively turned out
              to be nontrivial for this image reconstruction challenge, in the
              absence of ground-truth results. Typical similarity measures like
              NMSE of SSIM were misled by image intensity scaling and outlier
              pixels. Thus, to facilitate reproducibility, researchers are
              encouraged to publish code and data alongside the original paper.
              Future methodological papers on MR image reconstruction might
              benefit from the consolidated reference implementations of
              CG-SENSE presented here, as a benchmark for methods comparison.},
	author = {Maier, Oliver and Baete, Steven Hubert and Fyrdahl, Alexander and Hammernik, Kerstin and Harrevelt, Seb and Kasper, Lars and Karakuzu, Agah and Loecher, Michael and Patzig, Franz and Tian, Ye and Wang, Ke and Gallichan, Daniel and Uecker, Martin and Knoll, Florian},
	journal = {Magnetic Resonance in Medicine},
	keywords = {CG-SENSE; MRI; NUFFT; image reconstruction; nonuniform sampling; reproducibility},
	language = {en},
	month = apr,
	number = 4,
	pages = {1821--1839},
	title = {{CG-SENSE} revisited: Results from the first {ISMRM} reproducibility challenge},
	volume = 85,
	year = 2021}

@unpublished{Dafflon2020-cn,
	abstract = {For most neuroimaging questions the huge range of possible
              analytic choices leads to the possibility that conclusions from
              any single analytic approach may be misleading. Examples of
              possible choices include the motion regression approach used and
              smoothing and {\ldots}},
	author = {Dafflon, J and Da Costa, P F and V{\'a}{\v s}a, F and Monti, R P and Bzdok, D and {others}},
	journal = {bioRxiv},
	title = {Neuroimaging: Into the multiverse},
	year = 2020}

@article{Flandin2008-hr,
	author = {Flandin, Guillaume and Friston, Karl},
	journal = {Scholarpedia J.},
	number = 4,
	pages = {6232},
	publisher = {Scholarpedia},
	title = {Statistical parametric mapping ({SPM})},
	volume = 3,
	year = 2008}

@article{Gramfort2013-ok,
	abstract = {Magnetoencephalography and electroencephalography (M/EEG) measure
              the weak electromagnetic signals generated by neuronal activity
              in the brain. Using these signals to characterize and locate
              neural activation in the brain is a challenge that requires
              expertise in physics, signal processing, statistics, and
              numerical methods. As part of the MNE software suite, MNE-Python
              is an open-source software package that addresses this challenge
              by providing state-of-the-art algorithms implemented in Python
              that cover multiple methods of data preprocessing, source
              localization, statistical analysis, and estimation of functional
              connectivity between distributed brain regions. All algorithms
              and utility functions are implemented in a consistent manner with
              well-documented interfaces, enabling users to create M/EEG data
              analysis pipelines by writing Python scripts. Moreover,
              MNE-Python is tightly integrated with the core Python libraries
              for scientific comptutation (NumPy, SciPy) and visualization
              (matplotlib and Mayavi), as well as the greater neuroimaging
              ecosystem in Python via the Nibabel package. The code is provided
              under the new BSD license allowing code reuse, even in commercial
              products. Although MNE-Python has only been under heavy
              development for a couple of years, it has rapidly evolved with
              expanded analysis capabilities and pedagogical tutorials because
              multiple labs have collaborated during code development to help
              share best practices. MNE-Python also gives easy access to
              preprocessed datasets, helping users to get started quickly and
              facilitating reproducibility of methods by other researchers.
              Full documentation, including dozens of examples, is available at
              http://martinos.org/mne.},
	author = {Gramfort, Alexandre and Luessi, Martin and Larson, Eric and Engemann, Denis A and Strohmeier, Daniel and Brodbeck, Christian and Goj, Roman and Jas, Mainak and Brooks, Teon and Parkkonen, Lauri and H{\"a}m{\"a}l{\"a}inen, Matti},
	journal = {Frontiers in Neuroscience},
	keywords = {electroencephalography (EEG); magnetoencephalography (MEG); neuroimaging; open-source; python; software},
	language = {en},
	month = dec,
	pages = {267},
	title = {{MEG} and {EEG} data analysis with {MNE-Python}},
	volume = 7,
	year = 2013}

@article{Esteban2019-nx,
	abstract = {The neuroimaging community is steering towards increasingly large
              sample sizes, which are highly heterogeneous because they can
              only be acquired by multi-site consortia. The visual assessment
              of every imaging scan is a necessary quality control step, yet
              arduous and time-consuming. A sizeable body of evidence shows
              that images of low quality are a source of variability that may
              be comparable to the effect size under study. We present the
              MRIQC Web-API, an open crowdsourced database that collects image
              quality metrics extracted from MR images and corresponding manual
              assessments by experts. The database is rapidly growing, and
              currently contains over 100,000 records of image quality metrics
              of functional and anatomical MRIs of the human brain, and over
              200 expert ratings. The resource is designed for researchers to
              share image quality metrics and annotations that can readily be
              reused in training human experts and machine learning algorithms.
              The ultimate goal of the database is to allow the development of
              fully automated quality control tools that outperform expert
              ratings in identifying subpar images.},
	author = {Esteban, Oscar and Blair, Ross W and Nielson, Dylan M and Varada, Jan C and Marrett, Sean and Thomas, Adam G and Poldrack, Russell A and Gorgolewski, Krzysztof J},
	journal = {Sci Data},
	language = {en},
	month = apr,
	number = 1,
	pages = {30},
	title = {Crowdsourced {MRI} quality metrics and expert quality annotations for training of humans and machines},
	volume = 6,
	year = 2019}

@article{Serra-Garcia2021-pf,
	abstract = {We use publicly available data to show that published papers in
              top psychology, economics, and general interest journals that
              fail to replicate are cited more than those that replicate. This
              difference in citation does not change after the publication of
              the failure to replicate. Only 12\% of postreplication citations
              of nonreplicable findings acknowledge the replication failure.
              Existing evidence also shows that experts predict well which
              papers will be replicated. Given this prediction, why are
              nonreplicable papers accepted for publication in the first place?
              A possible answer is that the review team faces a trade-off. When
              the results are more ``interesting,'' they apply lower standards
              regarding their reproducibility.},
	author = {Serra-Garcia, Marta and Gneezy, Uri},
	journal = {Science Advances},
	language = {en},
	month = may,
	number = 21,
	title = {Nonreplicable publications are cited more than replicable ones},
	volume = 7,
	year = 2021}

@unpublished{Bowring2021-pw,
	abstract = {AbstractWhile the development of tools and techniques has
                 broadened our horizons for comprehending the complexities of
                 the human brain, a growing body of research has highlighted
                 the pitfalls of such methodological plurality. In a recent
                 study, we found that the choice of software package used to
                 run the analysis pipeline can have a considerable impact on
                 the final group-level results of a task-fMRI investigation
                 (Bowring et al., 2019, BMN). Here we revisit our work, seeking
                 to identify the stages of the pipeline where the greatest
                 variation between analysis software is induced. We carry out
                 further analyses on the three datasets evaluated in BMN,
                 employing a common processing strategy across parts of the
                 analysis workflow and then utilizing procedures from three
                 software packages (AFNI, FSL and SPM) across the remaining
                 steps of the pipeline. We use quantitative methods to compare
                 the statistical maps and isolate the main stages of the
                 workflow where the three packages diverge. Across all
                 datasets, we find that variation between the packages' results
                 is largely attributable to a handful of individual analysis
                 stages, and that these sources of variability were
                 heterogeneous across the datasets (e.g. choice of first-level
                 signal model had the most impact for the ds000001 dataset,
                 while first-level noise model was more influential for
                 ds000109 dataset). We also observe areas of the analysis
                 workflow where changing the software package causes minimal
                 differences in the final results, finding that the group-level
                 results were largely unaffected by which software package is
                 used to model the low-frequency fMRI drifts.},
	author = {Bowring, Alexander and Nichols, Thomas E and Maumet, Camille},
	institution = {bioRxiv},
	journal = {bioRxiv},
	language = {en},
	month = jul,
	title = {Isolating the sources of pipeline-variability in group-level {task-fMRI} results},
	year = 2021}

@article{Kaplan2015-ul,
	abstract = {BACKGROUND: We explore whether the number of null results in
              large National Heart Lung, and Blood Institute (NHLBI) funded
              trials has increased over time. METHODS: We identified all large
              NHLBI supported RCTs between 1970 and 2012 evaluating drugs or
              dietary supplements for the treatment or prevention of
              cardiovascular disease. Trials were included if direct costs
              >\$500,000/year, participants were adult humans, and the primary
              outcome was cardiovascular risk, disease or death. The 55 trials
              meeting these criteria were coded for whether they were published
              prior to or after the year 2000, whether they registered in
              clinicaltrials.gov prior to publication, used active or placebo
              comparator, and whether or not the trial had industry
              co-sponsorship. We tabulated whether the study reported a
              positive, negative, or null result on the primary outcome
              variable and for total mortality. RESULTS: 17 of 30 studies
              (57\%) published prior to 2000 showed a significant benefit of
              intervention on the primary outcome in comparison to only 2 among
              the 25 (8\%) trials published after 2000 ($\chi$2=12.2,df= 1,
              p=0.0005). There has been no change in the proportion of trials
              that compared treatment to placebo versus active comparator.
              Industry co-sponsorship was unrelated to the probability of
              reporting a significant benefit. Pre-registration in clinical
              trials.gov was strongly associated with the trend toward null
              findings. CONCLUSIONS: The number NHLBI trials reporting positive
              results declined after the year 2000. Prospective declaration of
              outcomes in RCTs, and the adoption of transparent reporting
              standards, as required by clinicaltrials.gov, may have
              contributed to the trend toward null findings.},
	author = {Kaplan, Robert M and Irvin, Veronica L},
	journal = {PLoS One},
	language = {en},
	month = aug,
	number = 8,
	pages = {e0132382},
	title = {Likelihood of null effects of large {NHLBI} clinical trials has increased over time},
	volume = 10,
	year = 2015}

@article{Nosek2018-yr,
	abstract = {Progress in science relies in part on generating hypotheses with
              existing observations and testing hypotheses with new
              observations. This distinction between postdiction and prediction
              is appreciated conceptually but is not respected in practice.
              Mistaking generation of postdictions with testing of predictions
              reduces the credibility of research findings. However, ordinary
              biases in human reasoning, such as hindsight bias, make it hard
              to avoid this mistake. An effective solution is to define the
              research questions and analysis plan before observing the
              research outcomes-a process called preregistration.
              Preregistration distinguishes analyses and outcomes that result
              from predictions from those that result from postdictions. A
              variety of practical strategies are available to make the best
              possible use of preregistration in circumstances that fall short
              of the ideal application, such as when the data are preexisting.
              Services are now available for preregistration across all
              disciplines, facilitating a rapid increase in the practice.
              Widespread adoption of preregistration will increase
              distinctiveness between hypothesis generation and hypothesis
              testing and will improve the credibility of research findings.},
	author = {Nosek, Brian A and Ebersole, Charles R and DeHaven, Alexander C and Mellor, David T},
	journal = {Proceedings of the National Academy of Sciences},
	number = 15,
	pages = {201708274},
	title = {The preregistration revolution},
	volume = 2017,
	year = 2018}

@article{Avesani2019-vi,
	abstract = {We describe the Open Diffusion Data Derivatives (O3D) repository:
              an integrated collection of preserved brain data derivatives and
              processing pipelines, published together using a single
              digital-object-identifier. The data derivatives were generated
              using modern diffusion-weighted magnetic resonance imaging data
              (dMRI) with diverse properties of resolution and signal-to-noise
              ratio. In addition to the data, we publish all processing
              pipelines (also referred to as open cloud services). The
              pipelines utilize modern methods for neuroimaging data processing
              (diffusion-signal modelling, fiber tracking, tractography
              evaluation, white matter segmentation, and structural connectome
              construction). The O3D open services can allow cognitive and
              clinical neuroscientists to run the connectome mapping algorithms
              on new, user-uploaded, data. Open source code implementing all
              O3D services is also provided to allow computational and computer
              scientists to reuse and extend the processing methods. Publishing
              both data-derivatives and integrated processing pipeline promotes
              practices for scientific reproducibility and data upcycling by
              providing open access to the research assets for utilization by
              multiple scientific communities.},
	author = {Avesani, Paolo and McPherson, Brent and Hayashi, Soichi and Caiafa, Cesar F and Henschel, Robert and Garyfallidis, Eleftherios and Kitchell, Lindsey and Bullock, Daniel and Patterson, Andrew and Olivetti, Emanuele and Sporns, Olaf and Saykin, Andrew J and Wang, Lei and Dinov, Ivo and Hancock, David and Caron, Bradley and Qian, Yiming and Pestilli, Franco},
	journal = {Scientific Data},
	language = {en},
	month = may,
	number = 1,
	pages = {69},
	title = {The open diffusion data derivatives, brain data upcycling via integrated publishing of derivatives and reproducible open cloud services},
	volume = 6,
	year = 2019}

@article{Moreau2015-ia,
	abstract = {The prov family of documents are the final output of the World
               Wide Web Consortium Provenance Working Group, chartered to
               specify a representation of provenance to facilitate its
               exchange over the Web. This article reflects upon the key
               requirements, guiding principles, and design decisions that
               influenced the prov family of documents. A broad range of
               requirements were found, relating to the key concepts necessary
               for describing provenance, such as resources, activities, agents
               and events, and to balancing prov's ease of use with the
               facility to check its validity. By this retrospective
               requirement analysis, the article aims to provide some insights
               into how prov turned out as it did and why. Benefits of this
               insight include better inter-operability, a roadmap for
               alternate investigations and improvements, and solid foundations
               for future standardization activities.},
	author = {Moreau, Luc and Groth, Paul and Cheney, James and Lebo, Timothy and Miles, Simon},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	journal = {Web Semantics},
	language = {en},
	month = dec,
	pages = {235--257},
	publisher = {Elsevier BV},
	title = {The rationale of {PROV}},
	volume = 35,
	year = 2015}

@article{Schmitt2021-id,
	abstract = {The performance of MRI head coils together with the influence of
               the prescan normalize filter in different brain regions was
               evaluated. Functional and structural data were recorded from 26
               participants performing a motor, an auditory, and a visual task
               in different conditions: with the 20- and the 64-channel Siemens
               head/neck coil and the prescan normalize filter turned ON or
               OFF. Data were analyzed with the MRIQC tool to evaluate data
               quality differences. The functional data were statistically
               evaluated by comparison of the $\beta$ estimates and the
               time-course SNR in four regions of interest, i.e., the auditory,
               the visual, the motor cortex and the thalamus. The MRIQC tool
               indicated a better data quality for both functional and
               structural data with the prescan normalize filter, with an
               advantage for the 20-channel head coil in functional and an
               advantage for the 64-channel head coil in structural
               measurements. Nevertheless, recommendations for the functional
               data regarding choice of head coils and prescan normalize filter
               depend on the brain regions of interest. Higher $\beta$
               estimates and tSNR values occurred in the auditory cortex and
               thalamus with the prescan normalize filter, whereas the contrary
               was true for the visual and the motor cortex. Due to higher
               $\beta$ estimates in the visual cortex in the 64-channel head
               coil, this head coil is recommended for studies investigating
               the visual cortex. For most of the research questions the
               20-channel is better suited for functional experiments, with the
               prescan normalize filter, especially when investigating deep
               brain areas. For anatomical studies, the 64-channel head coil
               seemed to be the better choice.},
	author = {Schmitt, Tina and Rieger, Jochem W},
	journal = {Frontiers in Neuroscience},
	keywords = {20-channel head coil; 64-channel head coil; prescan normalize; Echo-Planar Imaging; Anatomical images; MRIQC},
	language = {en},
	publisher = {Frontiers},
	title = {Recommendations of choice of head coil and prescan normalize filter depend on region of interest and task},
	volume = 0,
	year = 2021}

@misc{DuPre2019-is,
	author = {DuPre, Elizabeth and Hanke, Michael and Poline, Jean-Baptiste},
	title = {Nature abhors a paywall: How open science can realize the potential of naturalistic stimuli},
	year = 2019}

@article{Jas2018-qg,
	abstract = {Cognitive neuroscience questions are commonly tested with
              experiments that involve a cohort of subjects. The cohort can
              consist of a handful of subjects for small studies to hundreds or
              thousands of subjects in open datasets. While there exist various
              online resources to get started with the analysis of
              magnetoencephalography (MEG) or electroencephalography (EEG)
              data, such educational materials are usually restricted to the
              analysis of a single subject. This is in part because data from
              larger group studies are harder to share, but also analyses of
              such data often require subject-specific decisions which are hard
              to document. This work presents the results obtained by the
              reanalysis of an open dataset from Wakeman and Henson (2015)
              using the MNE software package. The analysis covers preprocessing
              steps, quality assurance steps, sensor space analysis of evoked
              responses, source localization, and statistics in both sensor and
              source space. Results with possible alternative strategies are
              presented and discussed at different stages such as the use of
              high-pass filtering versus baseline correction, tSSS vs. SSS, the
              use of a minimum norm inverse vs. LCMV beamformer, and the use of
              univariate or multivariate statistics. This aims to provide a
              comparative study of different stages of M/EEG analysis pipeline
              on the same dataset, with open access to all of the scripts
              necessary to reproduce this analysis.},
	author = {Jas, Mainak and Larson, Eric and Engemann, Denis A and Lepp{\"a}kangas, Jaakko and Taulu, Samu and H{\"a}m{\"a}l{\"a}inen, Matti and Gramfort, Alexandre},
	journal = {Frontiers in Neuroscience},
	keywords = {Python; electroencephalography (EEG); magnetoencephalography (MEG); neuroimaging; open-source; software},
	language = {en},
	month = aug,
	pages = {530},
	title = {A reproducible {MEG/EEG} group study with the {MNE} Software: Recommendations, quality assessments, and good practices},
	volume = 12,
	year = 2018}

@article{Ding2019-nw,
	abstract = {Quality control (QC) of brain magnetic resonance images (MRI) is
              an important process requiring a significant amount of manual
              inspection. Major artifacts, such as severe subject motion, are
              easy to identify to na{\"\i}ve observers but lack automated
              identification tools. Clinical trials involving motion-prone
              neonates typically pool data to obtain sufficient power, and
              automated quality control protocols are especially important to
              safeguard data quality. Current study tested an open source
              method to detect major artifacts among 2D neonatal MRI via
              supervised machine learning. A total of 1,020 two-dimensional
              transverse T2-weighted MRI images of preterm newborns were
              examined and classified as either QC Pass or QC Fail. Then 70
              features across focus, texture, noise, and natural scene
              statistics categories were extracted from each image. Several
              different classifiers were trained and their performance was
              compared with subjective rating as the gold standard. We repeated
              the rating process again to examine the stability of the rating
              and classification. When tested via 10-fold cross validation, the
              random undersampling and adaboost ensemble (RUSBoost) method
              achieved the best overall performance for QC Fail images with
              85\% positive predictive value along with 75\% sensitivity.
              Similar classification performance was observed in the analyses
              of the repeated subjective rating. Current results served as a
              proof of concept for predicting images that fail quality control
              using no-reference objective image features. We also highlighted
              the importance of evaluating results beyond mere accuracy as a
              performance measure for machine learning in imbalanced group
              settings due to larger proportion of QC Pass quality images.},
	author = {Ding, Yang and Suffren, Sabrina and Bellec, Pierre and Lodygensky, Gregory A},
	journal = {Hum. Brain Mapp.},
	keywords = {Canadian Neonatal Brain Platform; T2w; brain imaging; motion detection; neonatal; open source; quality control},
	language = {en},
	month = mar,
	number = 4,
	pages = {1290--1297},
	title = {Supervised machine learning quality control for magnetic resonance artifacts in neonatal data sets},
	volume = 40,
	year = 2019}

@article{Blischak2016-th,
	author = {Blischak, John D and Davenport, Emily R and Wilson, Greg},
	journal = {PLoS Computational Biology},
	language = {en},
	month = jan,
	number = 1,
	pages = {e1004668},
	title = {A quick introduction to version control with Git and {GitHub}},
	volume = 12,
	year = 2016}

@misc{De_San_Roman2021-xr,
	author = {de San Rom{\'a}n, Alea L{\'o}pez},
	month = apr,
	publisher = {Zenodo},
	title = {Open Science in Horizon Europe},
	year = 2021}

@misc{Brett2020-hz,
	abstract = {Most work on NiBabel so far has been by Matthew Brett (MB),
                  Chris Markiewicz (CM), Michael Hanke (MH), Marc-Alexandre
                  C{\^o}t{\'e} (MC), Ben Cipollini (BC), Paul McCarthy (PM),
                  Chris Cheng (CC), Yaroslav Halchenko (YOH), Satra Ghosh (SG),
                  Eric Larson (EL), Demian Wassermann, Stephan Gerhard and Ross
                  Markello (RM). References like ``pr/298'' refer to github
                  pull request numbers. 3.2.1 (Saturday 28 November 2020) Bug
                  fix release in the 3.2.x series. Maintenance Drop references
                  to builtin types in Numpy namespace like np.float (pr/964)
                  (EL, reviewed by CM) Ensure compatibility with Python 3.9
                  (pr/963) (CM)},
	author = {Brett, Matthew and Markiewicz, Christopher J and Hanke, Michael and C{\^o}t{\'e}, Marc-Alexandre and Cipollini, Ben and McCarthy, Paul and Jarecka, Dorota and Cheng, Christopher P and Halchenko, Yaroslav O and Cottaar, Michiel and Larson, Eric and Ghosh, Satrajit and Wassermann, Demian and Gerhard, Stephan and Lee, Gregory R and Wang, Hao-Ting and Kastman, Erik and Kaczmarzyk, Jakub and Guidotti, Roberto and Duek, Or and Daniel, Jonathan and Rokem, Ariel and Madison, Cindee and Moloney, Brendan and Morency, F{\'e}lix C and Goncalves, Mathias and Markello, Ross and Riddell, Cameron and Burns, Christopher and Millman, Jarrod and Gramfort, Alexandre and Lepp{\"a}kangas, Jaakko and S{\'o}lon, Anibal and van den Bosch, Jasper J F and Vincent, Robert D and Braun, Henry and Subramaniam, Krish and Gorgolewski, Krzysztof J and Raamana, Pradeep Reddy and Klug, Julian and Nichols, B Nolan and Baker, Eric M and Hayashi, Soichi and Pinsard, Basile and Haselgrove, Christian and Hymers, Mark and Esteban, Oscar and Koudoro, Serge and P{\'e}rez-Garc{\'\i}a, Fernando and Oosterhof, Nikolaas N and Amirbekian, Bago and Nimmo-Smith, Ian and Nguyen, Ly and Reddigari, Samir and St-Jean, Samuel and Panfilov, Egor and Garyfallidis, Eleftherios and Varoquaux, Gael and Legarreta, Jon Haitz and Hahn, Kevin S and Hinds, Oliver P and Fauber, Bennet and Poline, Jean-Baptiste and Stutters, Jon and Jordan, Kesshi and Cieslak, Matthew and Moreno, Miguel Estevan and Haenel, Valentin and Schwartz, Yannick and Baratz, Zvi and Darwin, Benjamin C and Thirion, Bertrand and Gauthier, Carl and Papadopoulos Orfanos, Dimitri and Solovey, Igor and Gonzalez, Ivan and Palasubramaniam, Jath and Lecher, Justin and Leinweber, Katrin and Raktivan, Konstantinos and Cal{\'a}bkov{\'a}, Mark{\'e}ta and Fischer, Peter and Gervais, Philippe and Gadde, Syam and Ballinger, Thomas and Roos, Thomas and Reddam, Venkateswara Reddy and {freec}},
	howpublished = {Zenodo},
	keywords = {neuroimaging},
	month = nov,
	publisher = {Zenodo},
	title = {nipy/nibabel: 3.2.1},
	year = 2020}

@article{Benning2019-te,
	abstract = {Clinical scientists can use a continuum of registration efforts
              that vary in their disclosure and timing relative to data
              collection and analysis. Broadly speaking, registration benefits
              investigators by offering stronger, more powerful tests of theory
              with particular methods in tandem with better control of long-run
              false positive error rates. Registration helps clinical
              researchers in thinking through tensions between bandwidth and
              fidelity that surround recruiting participants, defining clinical
              phenotypes, handling comorbidity, treating missing data, and
              analyzing rich and complex data. In particular, registration
              helps record and justify the reasons behind specific study design
              decisions, though it also provides the opportunity to register
              entire decision trees with specific endpoints. Creating ever more
              faithful registrations and standard operating procedures may
              offer alternative methods of judging a clinical investigator's
              scientific skill and eminence because study registration
              increases the transparency of clinical researchers' work.
              (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
	author = {Benning, Stephen D and Bachrach, Rachel L and Smith, Edward A and Freeman, Andrew J and Wright, Aidan G C},
	journal = {J. Abnorm. Psychol.},
	language = {en},
	month = aug,
	number = 6,
	pages = {528--540},
	title = {The registration continuum in clinical science: A guide toward transparent practices},
	volume = 128,
	year = 2019}

@misc{Wagner2021-aq,
	abstract = {The DataLad Handbook (http:/handbook.datalad.org) is a
               comprehensive educational resource for data management with
               DataLad.},
	author = {Wagner, Adina S and Waite, Laura K and Meyer, Kyle and Heckner, Marisa K and Kadelka, Tobias and Reuter, Niels and Waite, Alexander Q and Poldrack, Benjamin and Markiewicz, Christopher J and Halchenko, Yaroslav O and Vavra, Peter and Chormai, Pattarawat and Poline, Jean-Baptiste and Paas, Lya K and Herholz, Peer and Mochalski, Lisa N and Kraljevic, Nevena and Wiersch, Lisa and Hutton, Alexandre and Pustina, Dorian and Baagil, Hamzah Hamid and Glatard, Tristan and Oliveira, Sarah and Ippoliti, Giulia and M{\"o}nch, Christian and Huijser, Dorien and Togaru, Surya T and Rokem, Ariel and Gau, R{\'e}mi and Bomba, Judith and Wierzba, Ma{\l}gorzata and Appelhoff, Stefan and Joseph, Michael and Hanke, Michael},
	publisher = {Zenodo},
	title = {The {DataLad} Handbook},
	year = 2021}

@article{Lee2019-mb,
	abstract = {PURPOSE: Parametric imaging methods (e.g., T relaxation time
              mapping) have been shown to be more reproducible across time and
              vendors than weighted (e.g., T -weighted) images. The purpose of
              this work was to more extensively evaluate the validity of this
              assertion. METHODS: Seven volunteers underwent twice-repeated
              acquisitions of variable flip-angle T mapping, including B
              calibration, on a 3T Philips Achieva and 3T Siemens Trio scanner.
              Intra-scanner and inter-vendor T variability were calculated. To
              determine T reproducibility levels in longitudinal settings, or
              after changing hardware or software, four additional data sets
              were acquired from two of the participants; one participant was
              scanned on a different 3T Siemens Trio scanner and another on the
              same 3T Philips Achieva scanner but after a software upgrade.
              RESULTS: Intra-scanner variability of voxel-wise T values was
              consistent between the two vendors, averaging 0.7/0.7/1.3/1.4\%
              in white matter/cortical gray matter/subcortical gray
              matter/cerebellum, respectively. We observed, however, a
              systematic bias between the two vendors of
              https://doi.org/10.0/7.8/8.6/10.0\%, respectively. The T bias
              across two scanners of the same model was greater than
              intra-scanner variability, although still only at
              1.4/1.0/1.9/2.3\%, respectively. A greater bias was identified
              for data sets acquired before/after software upgrade in white
              matter/cortical gray matter (3.6/2.7\%) whereas variability in
              subcortical gray matter/cerebellum was comparable (1.7/1.9\%).
              CONCLUSION: We established intra- and inter-vendor
              reproducibility levels for a widely used T mapping protocol. We
              anticipate that these results will guide the design of
              multi-center studies, particularly those encompassing multiple
              vendors. Furthermore, this baseline level of reproducibility
              should be established or surpassed during the piloting phase of
              such studies.},
	author = {Lee, Yoojin and Callaghan, Martina F and Acosta-Cabronero, Julio and Lutti, Antoine and Nagy, Zoltan},
	journal = {Magnetic Resonance in Medicine},
	keywords = {3T; T1 relaxation; bias; multi-vendor; parametric imaging; reproducibility},
	language = {en},
	month = jan,
	number = 1,
	pages = {454--465},
	title = {Establishing intra- and inter-vendor reproducibility of {T} relaxation time measurements with {3T} {MRI}},
	volume = 81,
	year = 2019}

@article{Steegen2016-cr,
	abstract = {Empirical research inevitably includes constructing a data set by
              processing raw data into a form ready for statistical analysis.
              Data processing often involves choices among several reasonable
              options for excluding, transforming, and coding data. We suggest
              that instead of performing only one analysis, researchers could
              perform a multiverse analysis, which involves performing all
              analyses across the whole set of alternatively processed data
              sets corresponding to a large set of reasonable scenarios. Using
              an example focusing on the effect of fertility on religiosity and
              political attitudes, we show that analyzing a single data set can
              be misleading and propose a multiverse analysis as an alternative
              practice. A multiverse analysis offers an idea of how much the
              conclusions change because of arbitrary choices in data
              construction and gives pointers as to which choices are most
              consequential in the fragility of the result.},
	author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
	journal = {Perspectives on Psychological Science},
	keywords = {arbitrary choices; data processing; good research practices; multiverse analysis; selective reporting; transparency},
	number = 5,
	pages = {702--712},
	title = {Increasing transparency through a multiverse analysis},
	volume = 11,
	year = 2016}

@article{Leslie2015-rv,
	abstract = {The gender imbalance in STEM subjects dominates current debates
              about women's underrepresentation in academia. However, women are
              well represented at the Ph.D. level in some sciences and poorly
              represented in some humanities (e.g., in 2011, 54\% of U.S.
              Ph.D.'s in molecular biology were women versus only 31\% in
              philosophy). We hypothesize that, across the academic spectrum,
              women are underrepresented in fields whose practitioners believe
              that raw, innate talent is the main requirement for success,
              because women are stereotyped as not possessing such talent. This
              hypothesis extends to African Americans' underrepresentation as
              well, as this group is subject to similar stereotypes. Results
              from a nationwide survey of academics support our hypothesis
              (termed the field-specific ability beliefs hypothesis) over three
              competing hypotheses.},
	author = {Leslie, Sarah-Jane and Cimpian, Andrei and Meyer, Meredith and Freeland, Edward},
	journal = {Science},
	language = {en},
	month = jan,
	number = 6219,
	pages = {262--265},
	title = {Expectations of brilliance underlie gender distributions across academic disciplines},
	volume = 347,
	year = 2015}

@incollection{Kollada2021-ot,
	abstract = {Over the last twenty five years, advances in the collection and
               analysis of functional magnetic resonance imaging (fMRI) data
               have enabled new insights into the brain basis of human health
               and disease. Individual behavioral variation can now be
               visualized at a neural level as patterns of connectivity among
               brain regions. As such, functional brain imaging is enhancing
               our understanding of clinical psychiatric disorders by revealing
               ties between regional and network abnormalities and psychiatric
               symptoms. Initial success in this arena has recently motivated
               collection of larger datasets which are needed to leverage fMRI
               to generate brain-based biomarkers to support the development of
               precision medicines. Despite numerous methodological advances
               and enhanced computational power, evaluating the quality of fMRI
               scans remains a critical step in the analytical framework.
               Before analysis can be performed, expert reviewers visually
               inspect individual raw scans and preprocessed derivatives to
               determine viability of the data. This Quality Control (QC)
               process is labor intensive, and the inability to adequately
               automate at large scale has proven to be a limiting factor in
               clinical neuroscience fMRI research. In this paper, we present a
               novel method for automating the QC of fMRI scans. We train
               machine learning classifiers using features derived from brain
               MR images to predict the ``quality'' of those images, which is
               based on the ground truth of an expert's opinion. Specifically,
               we emphasize the importance of these classifiers' ability to
               generalize their predictions across data collected in different
               studies. To address this, we propose a novel approach entitled
               ``FMRI preprocessing Log mining for Automated, Generalizable
               Quality Control'' (FLAG-QC), in which features derived from
               mining runtime logs are used to train the classifier. We show
               that classifiers trained on FLAG-QC features perform much better
               (AUC = 0.79) than previously proposed feature sets (AUC = 0.56)
               when testing their ability to generalize across studies.},
	address = {Cham},
	author = {Kollada, Matthew and Gao, Qingzhu and Mellem, Monika S and Banerjee, Tathagata and Martin, William J},
	booktitle = {Explainable {AI} in Healthcare and Medicine: Building a Culture of Transparency and Accountability},
	editor = {Shaban-Nejad, Arash and Michalowski, Martin and Buckeridge, David L},
	pages = {55--68},
	publisher = {Springer International Publishing},
	title = {A Generalizable Method for Automated Quality Control of Functional Neuroimaging Datasets},
	year = 2021}

@article{Van_Essen2013-ty,
	abstract = {The Human Connectome Project consortium led by Washington
              University, University of Minnesota, and Oxford University is
              undertaking a systematic effort to map macroscopic human brain
              circuits and their relationship to behavior in a large population
              of healthy adults. This overview article focuses on progress made
              during the first half of the 5-year project in refining the
              methods for data acquisition and analysis. Preliminary analyses
              based on a finalized set of acquisition and preprocessing
              protocols demonstrate the exceptionally high quality of the data
              from each modality. The first quarterly release of imaging and
              behavioral data via the ConnectomeDB database demonstrates the
              commitment to making HCP datasets freely accessible. Altogether,
              the progress to date provides grounds for optimism that the HCP
              datasets and associated methods and software will become
              increasingly valuable resources for characterizing human brain
              connectivity and function, their relationship to behavior, and
              their heritability and genetic underpinnings.},
	author = {Van Essen, D C and Smith, Stephen M and Barch, Deanna M and Behrens, Timothy E J and Yacoub, Essa and Ugurbil, Kamil and {WU-Minn HCP Consortium}},
	journal = {Neuroimage},
	language = {en},
	month = oct,
	pages = {62--79},
	title = {The {WU-Minn} Human Connectome Project: An overview},
	volume = 80,
	year = 2013}

@unpublished{Ciric2021-uw,
	abstract = {Reference anatomies of the brain and corresponding atlases play a
              central role in experimental neuroimaging workflows and are the
              foundation for reporting standardized results. The choice of such
              references ---i.e., templates--- and atlases is one relevant
              source of methodological variability across studies, which has
              recently been brought to attention as an important challenge to
              reproducibility in neuroscience. TemplateFlow is a publicly
              available framework for human and nonhuman brain models. The
              framework combines an open database with software for access,
              management, and vetting, allowing scientists to distribute their
              resources under FAIR ---findable, accessible, interoperable,
              reusable--- principles. TemplateFlow supports a multifaceted
              insight into brains across species, and enables multiverse
              analyses testing whether results generalize across standard
              references, scales, and in the long term, species, thereby
              contributing to increasing the reliability of neuroimaging
              results. \#\#\# Competing Interest Statement The authors have
              declared no competing interest.},
	author = {Ciric, Rastko and Thompson, William H and Lorenz, Romy and Goncalves, Mathias and MacNicol, Eilidh and Markiewicz, Christopher J and Halchenko, Yaroslav O and Ghosh, Satrajit S and Gorgolewski, Krzysztof J and Poldrack, Russell A and Esteban, Oscar},
	journal = {bioRxiv},
	language = {en},
	month = aug,
	pages = {2021.02.10.430678},
	title = {{TemplateFlow}: {FAIR-sharing} of multi-scale, multi-species brain models},
	year = 2021}

@article{Karakuzu2021-wy,
	abstract = {The Brain Imaging Data Structure (BIDS) established community
              consensus on the organization of data and metadata for several
              neuroimaging modalities. Traditionally, BIDS had a strong focus
              on functional magnetic resonance imaging (MRI) datasets and
              lacked guidance on how to store multimodal structural MRI
              datasets. Here, we present and describe the BIDS Extension
              Proposal 001 (BEP001), which adds a range of quantitative MRI
              (qMRI) applications to the BIDS. In general, the aim of qMRI is
              to characterize brain microstructure by quantifying the physical
              MR parameters of the tissue via computational, biophysical
              models. By proposing this new standard, we envision
              standardization of qMRI which makes multicenter dissemination of
              interoperable data possible. As a result, BIDS can act as a
              catalyst of convergence between qMRI methods development and
              application-driven neuroimaging studies that can help develop
              quantitative biomarkers for neural tissue characterization.
              Finally, our BIDS extension offers a common ground for developers
              to exchange novel imaging data and tools, reducing the practical
              barriers to standardization that is currently lacking in the
              field of neuroimaging. \#\#\# Competing Interest Statement The
              authors have declared no competing interest. \#\#\# Funding
              Statement Biotechnology and Biological Sciences Research Council,
              London (BB/S008314/1). F.R.S.-FNRS, Belgium. Rubicon grant from
              the Dutch Research Council (NWO. Canada First Research Excellence
              Fund through the TransMedTech Institute, Canadian Open
              Neuroscience Platform (CONP) and International Society for
              Magnetic Resonance in Medicine (ISMRM). \#\#\# Author
              Declarations I confirm all relevant ethical guidelines have been
              followed, and any necessary IRB and/or ethics committee approvals
              have been obtained. Yes The details of the IRB/oversight body
              that provided approval or exemption for the research described
              are given below: N/A I confirm that all necessary
              patient/participant consent has been obtained and the appropriate
              institutional forms have been archived, and that any
              patient/participant/sample identifiers included were not known to
              anyone (e.g., hospital staff, patients or participants
              themselves) outside the research group so cannot be used to
              identify individuals. Yes I understand that all clinical trials
              and any other prospective interventional studies must be
              registered with an ICMJE-approved registry, such as
              ClinicalTrials.gov. I confirm that any such study reported in the
              manuscript has been registered and the trial registration ID is
              provided (note: if posting a prospective study registered
              retrospectively, please provide a statement in the trial ID field
              explaining why the study was not registered in advance). Yes I
              have followed all appropriate research reporting guidelines and
              uploaded the relevant EQUATOR Network research reporting
              checklist(s) and other pertinent material as supplementary files,
              if applicable. Yes https://github.com/bids-standard/bids-examples},
	author = {Karakuzu, Agah and Appelhoff, Stefan and Auer, Tibor and Boudreau, Mathieu and Feingold, Franklin and Khan, Ali R and Lazari, Alberto and Markiewicz, Christopher and Mulder, Martjin j and Phillips, Christophe and Salo, Taylor and Stikov, Nikola and Whitaker, Kirstie and Hollander, Gilles},
	date-modified = {2022-08-01 14:11:54 +0300},
	journal = {medRxiv},
	language = {en},
	month = oct,
	pages = {2021.10.22.21265382},
	title = {{qMRI-BIDS}: An extension to the brain imaging data structure for quantitative magnetic resonance imaging data},
	year = 2021}

@article{Chambers2019-ns,
	author = {Chambers, Chris},
	journal = {Nature},
	keywords = {Peer review; Publishing},
	language = {en},
	month = sep,
	number = 7773,
	pages = {187--189},
	title = {What's next for Registered Reports?},
	volume = 573,
	year = 2019}

@article{Niso2022-ng,
	abstract = {Good Scientific Practice (GSP) refers to both explicit and
              implicit rules, recommendations, and guidelines that help
              scientists to produce work that is of the highest quality at any
              given time, and to efficiently share that work with the community
              for further scrutiny or utilization. For experimental research
              using magneto- and electroencephalography (MEEG), GSP includes
              specific standards and guidelines for technical competence, which
              are periodically updated and adapted to new findings. However,
              GSP also needs to be periodically revisited in a broader light.
              At the LiveMEEG 2020 conference, a reflection on GSP was fostered
              that included explicitly documented guidelines and technical
              advances, but also emphasized intangible GSP: a general awareness
              of personal, organizational, and societal realities and how they
              can influence MEEG research. This article provides an extensive
              report on most of the LiveMEEG contributions and new literature,
              with the additional aim to synthesize ongoing cultural changes in
              GSP. It first covers GSP with respect to cognitive biases and
              logical fallacies, pre-registration as a tool to avoid those and
              other early pitfalls, and a number of resources to enable
              collaborative and reproducible research as a general approach to
              minimize misconceptions. Second, it covers GSP with respect to
              data acquisition, analysis, reporting, and sharing, including new
              tools and frameworks to support collaborative work. Finally, GSP
              is considered in light of ethical implications of MEEG research
              and the resulting responsibility that scientists have to engage
              with societal challenges. Considering among other things the
              benefits of peer review and open access at all stages, the need
              to coordinate larger international projects, the complexity of
              MEEG subject matter, and today's prioritization of fairness,
              privacy, and the environment, we find that current GSP tends to
              favor collective and cooperative work, for both scientific and
              for societal reasons.},
	author = {Niso, Guiomar and Krol, Laurens R and Combrisson, Etienne and Dubarry, A-Sophie and Elliott, Madison A and Fran{\c c}ois, Cl{\'e}ment and H{\'e}jja-Brichard, Yseult and Herbst, Sophie K and Jerbi, Karim and Kovic, Vanja and Lehongre, Katia and Luck, Steven J and Mercier, Manuel and Mosher, John C and Pavlov, Yuri G and Puce, Aina and Schettino, Antonio and Sch{\"o}n, Daniele and Sinnott-Armstrong, Walter and Somon, Bertille and {\v S}o{\v s}ki{\'c}, An{\dj}ela and Styles, Suzy J and Tibon, Roni and Vilas, Martina G and van Vliet, Marijn and Chaumon, Maximilien},
	journal = {Neuroimage},
	keywords = {Electroencephalography (EEG); Good Scientific Practice; Magnetoencephalography (MEG)},
	language = {en},
	month = mar,
	pages = {119056},
	title = {Good scientific practice in {MEEG} research: Progress and perspectives},
	year = 2022}

@article{Holdgraf2019-bb,
	author = {Holdgraf, Christopher and Appelhoff, Stefan and Bickel, Stephan and Bouchard, Kristofer and D'Ambrosio, Sasha and David, Olivier and Devinsky, Orrin and Dichter, Benjamin and Flinker, Adeen and Foster, Brett L and Gorgolewski, Krzysztof J and Groen, Iris and Groppe, David and Gunduz, Aysegul and Hamilton, Liberty and Honey, Christopher J and Jas, Mainak and Knight, Robert and Lachaux, Jean-Philippe and Lau, Jonathan C and Lee-Messer, Christopher and Lundstrom, Brian N and Miller, Kai J and Ojemann, Jeffrey G and Oostenveld, Robert and Petridou, Natalia and Piantoni, Gio and Pigorini, Andrea and Pouratian, Nader and Ramsey, Nick F and Stolk, Arjen and Swann, Nicole C and Tadel, Fran{\c c}ois and Voytek, Bradley and Wandell, Brian A and Winawer, Jonathan and Whitaker, Kirstie and Zehl, Lyuba and Hermes, Dora},
	journal = {Scientific Data},
	language = {en},
	month = jun,
	number = 1,
	pages = {102},
	title = {{iEEG-BIDS}: Extending the Brain Imaging Data Structure specification to human intracranial electrophysiology},
	volume = 6,
	year = 2019}

@article{Norgaard2020-ak,
	abstract = {Positron emission tomography (PET) neuroimaging provides unique
               possibilities to study biological processes in vivo under basal
               and interventional conditions. For quantification of PET data,
               researchers commonly apply different arrays of sequential data
               analytic methods (?preprocessing pipeline?), but it is often
               unknown how the choice of preprocessing affects the final
               outcome. Here, we use an available data set from a double-blind,
               randomized, placebo-controlled [11C]DASB-PET study as a case to
               evaluate how the choice of preprocessing affects the outcome of
               the study. We tested the impact of 384 commonly used
               preprocessing strategies on a previously reported positive
               association between the change from baseline in neocortical
               serotonin transporter binding determined with [11C]DASB-PET, and
               change in depressive symptoms, following a pharmacological sex
               hormone manipulation intervention in 30 women. The two
               preprocessing steps that were most critical for the outcome were
               motion correction and kinetic modeling of the dynamic PET data.
               We found that 36\% of the applied preprocessing strategies
               replicated the originally reported finding (p?<?0.05). For
               preprocessing strategies with motion correction, the replication
               percentage was 72\%, whereas it was 0\% for strategies without
               motion correction. In conclusion, the choice of preprocessing
               strategy can have a major impact on a study outcome.},
	author = {N{\o}rgaard, Martin and Ganz, Melanie and Svarer, Claus and Frokjaer, Vibe G and Greve, Douglas N and Strother, Stephen C and Knudsen, Gitte M},
	journal = {Journal of Cerebral Blood Flow and Metabolism},
	month = sep,
	number = 9,
	pages = {1902--1911},
	publisher = {SAGE Publications Ltd STM},
	title = {Different preprocessing strategies lead to different conclusions: A [{11C]DASB-PET} reproducibility study},
	volume = 40,
	year = 2020}

@article{Nielsen2018-wy,
	abstract = {PURPOSE: To introduce a framework for rapid prototyping of MR
              pulse sequences. METHODS: We propose a simple file format, called
              ``TOPPE'', for specifying all details of an MR imaging
              experiment, such as gradient and radiofrequency waveforms and the
              complete scan loop. In addition, we provide a TOPPE file
              ``interpreter'' for GE scanners, which is a binary executable
              that loads TOPPE files and executes the sequence on the scanner.
              We also provide MATLAB scripts for reading and writing TOPPE
              files and previewing the sequence prior to hardware execution.
              With this setup, the task of the pulse sequence programmer is
              reduced to creating TOPPE files, eliminating the need for
              hardware-specific programming. No sequence-specific compilation
              is necessary; the interpreter only needs to be compiled once (for
              every scanner software upgrade). We demonstrate TOPPE in three
              different applications: k-space mapping, non-Cartesian PRESTO
              whole-brain dynamic imaging, and myelin mapping in the brain
              using inhomogeneous magnetization transfer. RESULTS: We
              successfully implemented and executed the three example
              sequences. By simply changing the various TOPPE sequence files, a
              single binary executable (interpreter) was used to execute
              several different sequences. CONCLUSION: The TOPPE file format is
              a complete specification of an MR imaging experiment, based on
              arbitrary sequences of a (typically small) number of unique
              modules. Along with the GE interpreter, TOPPE comprises a modular
              and flexible platform for rapid prototyping of new pulse
              sequences. Magn Reson Med 79:3128-3134, 2018. \copyright{} 2017
              International Society for Magnetic Resonance in Medicine.},
	author = {Nielsen, Jon-Fredrik and Noll, Douglas C},
	journal = {Magnetic Resonance in Medicine},
	keywords = {open MRI; open source; platform-independent; pulse sequence programming; pulse sequence prototyping},
	language = {en},
	month = jun,
	number = 6,
	pages = {3128--3134},
	title = {{TOPPE}: A framework for rapid prototyping of {MR} pulse sequences},
	volume = 79,
	year = 2018}

@article{Botvinik-Nezer2020-hx,
	abstract = {Data analysis workflows in many scientific domains have become
              increasingly complex and flexible. Here we assess the effect of
              this flexibility on the results of functional magnetic resonance
              imaging by asking 70 independent teams to analyse the same
              dataset, testing the same 9 ex-ante hypotheses. The flexibility
              of analytical approaches is exemplified by the fact that no two
              teams chose identical workflows to analyse the data. This
              flexibility resulted in sizeable variation in the results of
              hypothesis tests, even for teams whose statistical maps were
              highly correlated at intermediate stages of the analysis
              pipeline. Variation in reported results was related to several
              aspects of analysis methodology. Notably, a meta-analytical
              approach that aggregated information across teams yielded a
              significant consensus in activated regions. Furthermore,
              prediction markets of researchers in the field revealed an
              overestimation of the likelihood of significant findings, even by
              researchers with direct knowledge of the dataset. Our findings
              show that analytical flexibility can have substantial effects on
              scientific conclusions, and identify factors that may be related
              to variability in the analysis of functional magnetic resonance
              imaging. The results emphasize the importance of validating and
              sharing complex analysis workflows, and demonstrate the need for
              performing and reporting multiple analyses of the same data.
              Potential approaches that could be used to mitigate issues
              related to analytical variability are discussed.},
	author = {Botvinik-Nezer, Rotem and Holzmeister, Felix and Camerer, Colin F and Dreber, Anna and Huber, Juergen and Johannesson, Magnus and Kirchler, Michael and Iwanir, Roni and Mumford, Jeanette A and Adcock, R Alison and Avesani, Paolo and Baczkowski, Blazej M and Bajracharya, Aahana and Bakst, Leah and Ball, Sheryl and Barilari, Marco and Bault, Nad{\`e}ge and Beaton, Derek and Beitner, Julia and Benoit, Roland G and Berkers, Ruud M W J and Bhanji, Jamil P and Biswal, Bharat B and Bobadilla-Suarez, Sebastian and Bortolini, Tiago and Bottenhorn, Katherine L and Bowring, Alexander and Braem, Senne and Brooks, Hayley R and Brudner, Emily G and Calderon, Cristian B and Camilleri, Julia A and Castrellon, Jaime J and Cecchetti, Luca and Cieslik, Edna C and Cole, Zachary J and Collignon, Olivier and Cox, Robert W and Cunningham, William A and Czoschke, Stefan and Dadi, Kamalaker and Davis, Charles P and Luca, Alberto De and Delgado, Mauricio R and Demetriou, Lysia and Dennison, Jeffrey B and Di, Xin and Dickie, Erin W and Dobryakova, Ekaterina and Donnat, Claire L and Dukart, Juergen and Duncan, Niall W and Durnez, Joke and Eed, Amr and Eickhoff, Simon B and Erhart, Andrew and Fontanesi, Laura and Fricke, G Matthew and Fu, Shiguang and Galv{\'a}n, Adriana and Gau, Remi and Genon, Sarah and Glatard, Tristan and Glerean, Enrico and Goeman, Jelle J and Golowin, Sergej A E and Gonz{\'a}lez-Garc{\'\i}a, Carlos and Gorgolewski, Krzysztof J and Grady, Cheryl L and Green, Mikella A and Guassi Moreira, Jo{\~a}o F and Guest, Olivia and Hakimi, Shabnam and Hamilton, J Paul and Hancock, Roeland and Handjaras, Giacomo and Harry, Bronson B and Hawco, Colin and Herholz, Peer and Herman, Gabrielle and Heunis, Stephan and Hoffstaedter, Felix and Hogeveen, Jeremy and Holmes, Susan and Hu, Chuan-Peng and Huettel, Scott A and Hughes, Matthew E and Iacovella, Vittorio and Iordan, Alexandru D and Isager, Peder M and Isik, Ayse I and Jahn, Andrew and Johnson, Matthew R and Johnstone, Tom and Joseph, Michael J E and Juliano, Anthony C and Kable, Joseph W and Kassinopoulos, Michalis and Koba, Cemal and Kong, Xiang-Zhen and Koscik, Timothy R and Kucukboyaci, Nuri Erkut and Kuhl, Brice A and Kupek, Sebastian and Laird, Angela R and Lamm, Claus and Langner, Robert and Lauharatanahirun, Nina and Lee, Hongmi and Lee, Sangil and Leemans, Alexander and Leo, Andrea and Lesage, Elise and Li, Flora and Li, Monica Y C and Lim, Phui Cheng and Lintz, Evan N and Liphardt, Schuyler W and Losecaat Vermeer, Annabel B and Love, Bradley C and Mack, Michael L and Malpica, Norberto and Marins, Theo and Maumet, Camille and McDonald, Kelsey and McGuire, Joseph T and Melero, Helena and M{\'e}ndez Leal, Adriana S and Meyer, Benjamin and Meyer, Kristin N and Mihai, Glad and Mitsis, Georgios D and Moll, Jorge and Nielson, Dylan M and Nilsonne, Gustav and Notter, Michael P and Olivetti, Emanuele and Onicas, Adrian I and Papale, Paolo and Patil, Kaustubh R and Peelle, Jonathan E and P{\'e}rez, Alexandre and Pischedda, Doris and Poline, Jean-Baptiste and Prystauka, Yanina and Ray, Shruti and Reuter-Lorenz, Patricia A and Reynolds, Richard C and Ricciardi, Emiliano and Rieck, Jenny R and Rodriguez-Thompson, Anais M and Romyn, Anthony and Salo, Taylor and Samanez-Larkin, Gregory R and Sanz-Morales, Emilio and Schlichting, Margaret L and Schultz, Douglas H and Shen, Qiang and Sheridan, Margaret A and Silvers, Jennifer A and Skagerlund, Kenny and Smith, Alec and Smith, David V and Sokol-Hessner, Peter and Steinkamp, Simon R and Tashjian, Sarah M and Thirion, Bertrand and Thorp, John N and Tingh{\"o}g, Gustav and Tisdall, Loreen and Tompson, Steven H and Toro-Serey, Claudio and Torre Tresols, Juan Jesus and Tozzi, Leonardo and Truong, Vuong and Turella, Luca and van 't Veer, Anna E and Verguts, Tom and Vettel, Jean M and Vijayarajah, Sagana and Vo, Khoi and Wall, Matthew B and Weeda, Wouter D and Weis, Susanne and White, David J and Wisniewski, David and Xifra-Porxas, Alba and Yearling, Emily A and Yoon, Sangsuk and Yuan, Rui and Yuen, Kenneth S L and Zhang, Lei and Zhang, Xu and Zosky, Joshua E and Nichols, Thomas E and Poldrack, Russell A and Schonberg, Tom},
	journal = {Nature},
	language = {en},
	month = jun,
	number = 7810,
	pages = {84--88},
	title = {Variability in the analysis of a single neuroimaging dataset by many teams},
	volume = 582,
	year = 2020}

@article{Markiewicz2021-mi,
	abstract = {The sharing of research data is essential to ensure
              reproducibility and maximize the impact of public investments in
              scientific research. Here, we describe OpenNeuro, a BRAIN
              Initiative data archive that provides the ability to openly share
              data from a broad range of brain imaging data types following the
              FAIR principles for data sharing. We highlight the importance of
              the Brain Imaging Data Structure standard for enabling effective
              curation, sharing, and reuse of data. The archive presently
              shares more than 600 datasets including data from more than
              20,000 participants, comprising multiple species and measurement
              modalities and a broad range of phenotypes. The impact of the
              shared data is evident in a growing number of published reuses,
              currently totalling more than 150 publications. We conclude by
              describing plans for future development and integration with
              other ongoing open science efforts.},
	author = {Markiewicz, Christopher J and Gorgolewski, Krzysztof J and Feingold, Franklin and Blair, Ross and Halchenko, Yaroslav O and Miller, Eric and Hardcastle, Nell and Wexler, Joe and Esteban, Oscar and Goncavles, Mathias and Jwa, Anita and Poldrack, Russell},
	journal = {Elife},
	keywords = {EEG; MEG; MRI; data sharing; human; mouse; neuroimaging; neuroscience; open science; rat},
	language = {en},
	month = oct,
	title = {The {OpenNeuro} resource for sharing of neuroscience data},
	volume = 10,
	year = 2021}

@article{Hardwicke2018-fi,
	author = {Hardwicke, Tom E and Ioannidis, John P A},
	journal = {Nature Human Behaviour},
	language = {en},
	month = nov,
	number = 11,
	pages = {793--796},
	title = {Mapping the universe of registered reports},
	volume = 2,
	year = 2018}

@article{Gorgolewski2015-if,
	abstract = {Here we present NeuroVault-a web based repository that allows
              researchers to store, share, visualize, and decode statistical
              maps of the human brain. NeuroVault is easy to use and employs
              modern web technologies to provide informative visualization of
              data without the need to install additional software. In
              addition, it leverages the power of the Neurosynth database to
              provide cognitive decoding of deposited maps. The data are
              exposed through a public REST API enabling other services and
              tools to take advantage of it. NeuroVault is a new resource for
              researchers interested in conducting meta- and coactivation
              analyses.},
	author = {Gorgolewski, Krzysztof J and Varoquaux, Gael and Rivera, Gabriel and Schwarz, Yannick and Ghosh, Satrajit S and Maumet, Camille and Sochat, Vanessa V and Nichols, Thomas E and Poldrack, Russell A and Poline, Jean-Baptiste and Yarkoni, Tal and Margulies, Daniel S},
	journal = {Frontiers in Neuroinformatics},
	keywords = {data sharing; database; meta-analysis; repository; statistical parameter mapping (SPM)},
	language = {en},
	month = apr,
	pages = {8},
	title = {{NeuroVault.org}: A web-based repository for collecting and sharing unthresholded statistical maps of the human brain},
	volume = 9,
	year = 2015}

@article{Piwowar2018-ty,
	abstract = {Despite growing interest in Open Access (OA) to scholarly
              literature, there is an unmet need for large-scale, up-to-date,
              and reproducible studies assessing the prevalence and
              characteristics of OA. We address this need using oaDOI, an open
              online service that determines OA status for 67 million articles.
              We use three samples, each of 100,000 articles, to investigate OA
              in three populations: (1) all journal articles assigned a
              Crossref DOI, (2) recent journal articles indexed in Web of
              Science, and (3) articles viewed by users of Unpaywall, an
              open-source browser extension that lets users find OA articles
              using oaDOI. We estimate that at least 28\% of the scholarly
              literature is OA (19M in total) and that this proportion is
              growing, driven particularly by growth in Gold and Hybrid. The
              most recent year analyzed (2015) also has the highest percentage
              of OA (45\%). Because of this growth, and the fact that readers
              disproportionately access newer articles, we find that Unpaywall
              users encounter OA quite frequently: 47\% of articles they view
              are OA. Notably, the most common mechanism for OA is not Gold,
              Green, or Hybrid OA, but rather an under-discussed category we
              dub Bronze: articles made free-to-read on the publisher website,
              without an explicit Open license. We also examine the citation
              impact of OA articles, corroborating the so-called open-access
              citation advantage: accounting for age and discipline, OA
              articles receive 18\% more citations than average, an effect
              driven primarily by Green and Hybrid OA. We encourage further
              research using the free oaDOI service, as a way to inform OA
              policy and practice.},
	author = {Piwowar, Heather A and Priem, Jason and Larivi{\`e}re, Vincent and Alperin, Juan Pablo and Matthias, Lisa and Norlander, Bree and Farley, Ashley and West, Jevin and Haustein, Stefanie},
	journal = {PeerJ},
	keywords = {Bibliometrics; Libraries; Open access; Open science; Publishing; Scholarly communication; Science policy; Scientometrics},
	language = {en},
	month = feb,
	pages = {e4375},
	title = {The state of {OA}: A large-scale analysis of the prevalence and impact of Open Access articles},
	volume = 6,
	year = 2018}

@article{Soderberg2021-gd,
	abstract = {In registered reports (RRs), initial peer review and in-principle
              acceptance occur before knowing the research outcomes. This
              combats publication bias and distinguishes planned from unplanned
              research. How RRs could improve the credibility of research
              findings is straightforward, but there is little empirical
              evidence. Also, there could be unintended costs such as reducing
              novelty. Here, 353 researchers peer reviewed a pair of papers
              from 29 published RRs from psychology and neuroscience and 57
              non-RR comparison papers. RRs numerically outperformed comparison
              papers on all 19 criteria (mean difference 0.46, scale range -4
              to +4) with effects ranging from RRs being statistically
              indistinguishable from comparison papers in novelty (0.13, 95\%
              credible interval [-0.24, 0.49]) and creativity (0.22, [-0.14,
              0.58]) to sizeable improvements in rigour of methodology (0.99,
              [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper
              quality (0.66, [0.30, 1.02]). RRs could improve research quality
              while reducing publication bias and ultimately improve the
              credibility of the published literature.},
	author = {Soderberg, Courtney K and Errington, Timothy M and Schiavone, Sarah R and Bottesini, Julia and Thorn, Felix Singleton and Vazire, Simine and Esterling, Kevin M and Nosek, Brian A},
	journal = {Nature Human Behaviour},
	language = {en},
	month = aug,
	number = 8,
	pages = {990--997},
	title = {Initial evidence of research quality of registered reports compared with the standard publishing model},
	volume = 5,
	year = 2021}

@article{Niso2018-wq,
	abstract = {We present a significant extension of the Brain Imaging Data
              Structure (BIDS) to support the specific aspects of
              magnetoencephalography (MEG) data. MEG measures brain activity
              with millisecond temporal resolution and unique source imaging
              capabilities. So far, BIDS was a solution to organise magnetic
              resonance imaging (MRI) data. The nature and acquisition
              parameters of MRI and MEG data are strongly dissimilar. Although
              there is no standard data format for MEG, we propose MEG-BIDS as
              a principled solution to store, organise, process and share the
              multidimensional data volumes produced by the modality. The
              standard also includes well-defined metadata, to facilitate
              future data harmonisation and sharing efforts. This responds to
              unmet needs from the multimodal neuroimaging community and paves
              the way to further integration of other techniques in
              electrophysiology. MEG-BIDS builds on MRI-BIDS, extending BIDS to
              a multimodal data structure. We feature several data-analytics
              software that have adopted MEG-BIDS, and a diverse sample of open
              MEG-BIDS data resources available to everyone.},
	author = {Niso, Guiomar and Gorgolewski, Krzysztof J and Bock, Elizabeth and Brooks, Teon L and Flandin, Guillaume and Gramfort, Alexandre and Henson, Richard N and Jas, Mainak and Litvak, Vladimir and T Moreau, Jeremy and Oostenveld, Robert and Schoffelen, Jan-Mathijs and Tadel, Francois and Wexler, Joseph and Baillet, Sylvain},
	journal = {Scientific Data},
	language = {en},
	month = jun,
	pages = {180110},
	title = {{MEG-BIDS}: The brain imaging data structure extended to magnetoencephalography},
	volume = 5,
	year = 2018}

@article{Allen2019-cb,
	abstract = {The movement towards open science is a consequence of seemingly
              pervasive failures to replicate previous research. This
              transition comes with great benefits but also significant
              challenges that are likely to affect those who carry out the
              research, usually early career researchers (ECRs). Here, we
              describe key benefits, including reputational gains, increased
              chances of publication, and a broader increase in the reliability
              of research. The increased chances of publication are supported
              by exploratory analyses indicating null findings are
              substantially more likely to be published via open registered
              reports in comparison to more conventional methods. These
              benefits are balanced by challenges that we have encountered and
              that involve increased costs in terms of flexibility, time, and
              issues with the current incentive structure, all of which seem to
              affect ECRs acutely. Although there are major obstacles to the
              early adoption of open science, overall open science practices
              should benefit both the ECR and improve the quality of research.
              We review 3 benefits and 3 challenges and provide suggestions
              from the perspective of ECRs for moving towards open science
              practices, which we believe scientists and institutions at all
              levels would do well to consider.},
	author = {Allen, Christopher and Mehler, David M A},
	journal = {PLoS Biology},
	language = {en},
	month = may,
	number = 5,
	pages = {e3000246},
	title = {Open science challenges, benefits and tips in early career and beyond},
	volume = 17,
	year = 2019}

@article{Boos2021-az,
	abstract = {Speech comprehension in natural soundscapes rests on the ability
              of the auditory system to extract speech information from a
              complex acoustic signal with overlapping contributions from many
              sound sources. Here we reveal the canonical processing of speech
              in natural soundscapes on multiple scales by using data-driven
              modeling approaches to characterize sounds to analyze ultra high
              field fMRI recorded while participants listened to the audio
              soundtrack of a movie. We show that at the functional level the
              neuronal processing of speech in natural soundscapes can be
              surprisingly low dimensional in the human cortex, highlighting
              the functional efficiency of the auditory system for a seemingly
              complex task. Particularly, we find that a model comprising three
              functional dimensions of auditory processing in the temporal
              lobes is shared across participants' fMRI activity. We further
              demonstrate that the three functional dimensions are implemented
              in anatomically overlapping networks that process different
              aspects of speech in natural soundscapes. One is most sensitive
              to complex auditory features present in speech, another to
              complex auditory features and fast temporal modulations, that are
              not specific to speech, and one codes mainly sound level. These
              results were derived with few a-priori assumptions and provide a
              detailed and computationally reproducible account of the cortical
              activity in the temporal lobe elicited by the processing of
              speech in natural soundscapes.},
	author = {Boos, Moritz and L{\"u}cke, J{\"o}rg and Rieger, Jochem W},
	journal = {Neuroimage},
	keywords = {Auditory; Encoding models; Natural soundscapes; Speech processing; Ultra high field fMRI; Unsupervised learning},
	language = {en},
	month = aug,
	pages = {118106},
	title = {Generalizable dimensions of human cortical auditory processing of speech in natural soundscapes: A data-driven ultra high field {fMRI} approach},
	volume = 237,
	year = 2021}

@article{Simmons2011-zo,
	abstract = {In this article, we accomplish two things. First, we show that
              despite empirical psychologists' nominal endorsement of a low
              rate of false-positive findings ($\leq$ .05), flexibility in data
              collection, analysis, and reporting dramatically increases actual
              false-positive rates. In many cases, a researcher is more likely
              to falsely find evidence that an effect exists than to correctly
              find evidence that it does not. We present computer simulations
              and a pair of actual experiments that demonstrate how
              unacceptably easy it is to accumulate (and report) statistically
              significant evidence for a false hypothesis. Second, we suggest a
              simple, low-cost, and straightforwardly effective
              disclosure-based solution to this problem. The solution involves
              six concrete requirements for authors and four guidelines for
              reviewers, all of which impose a minimal burden on the
              publication process.},
	author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
	journal = {Psychological Science},
	keywords = {disclosure; methodology; motivated reasoning; publication},
	number = 11,
	pages = {1359--1366},
	title = {False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant},
	volume = 22,
	year = 2011}

@inproceedings{Dragicevic2019-mj,
	abstract = {We present explorable multiverse analysis reports, a new
                approach to statistical reporting where readers of research
                papers can explore alternative analysis options by interacting
                with the paper itself. This approach draws from two recent
                ideas: i) multiverse analysis, a philosophy of statistical
                reporting where paper authors report the outcomes of many
                different statistical analyses in order to show how fragile or
                robust their findings are; and ii) explorable explanations,
                narratives that can be read as normal explanations but where
                the reader can also become active by dynamically changing some
                elements of the explanation. Based on five examples and a
                design space analysis, we show how combining those two ideas
                can complement existing reporting approaches and constitute a
                step towards more transparent research papers.},
	address = {New York, NY, USA},
	author = {Dragicevic, Pierre and Jansen, Yvonne and Sarma, Abhraneel and Kay, Matthew and Chevalier, Fanny},
	conference = {Conference on Human Factors in Computing Systems},
	month = may,
	pages = {1--15},
	publisher = {Association for Computing Machinery},
	title = {Increasing the transparency of research papers with Explorable Multiverse Analyses},
	year = 2019}

@article{Open_Science_Collaboration2015-rc,
	abstract = {Reproducibility is a defining feature of science, but the extent
              to which it characterizes current research is unknown. We
              conducted replications of 100 experimental and correlational
              studies published in three psychology journals using high-powered
              designs and original materials when available. Replication
              effects were half the magnitude of original effects, representing
              a substantial decline. Ninety-seven percent of original studies
              had statistically significant results. Thirty-six percent of
              replications had statistically significant results; 47\% of
              original effect sizes were in the 95\% confidence interval of the
              replication effect size; 39\% of effects were subjectively rated
              to have replicated the original result; and if no bias in
              original results is assumed, combining original and replication
              results left 68\% with statistically significant effects.
              Correlational tests suggest that replication success was better
              predicted by the strength of original evidence than by
              characteristics of the original and replication teams.},
	author = {{Open Science Collaboration}},
	journal = {Science},
	language = {en},
	month = aug,
	number = 6251,
	pages = {aac4716},
	title = {{PSYCHOLOGY}. Estimating the reproducibility of psychological science},
	volume = 349,
	year = 2015}

@misc{Halchenko2021-ue,
	author = {Halchenko, Yaroslav and Goncalves, Mathias and di Oleggio Castello, Matteo Visconti and Ghosh, Satrajit and Salo, Taylor and Hanke, Michael and Velasco, Pablo and {Dae} and Kent, James and Brett, Matthew and Amlien, Inge and Gorgolewski, Chris and Lukas, Darren Christopher and Markiewicz, Chris and Tilley, Steven and Kaczmarzyk, Jakub and Stadler, Joerg and Kim, Sin and Kahn, Ari and Poldrack, Benjamin and Melo, Bruno and Braun, Henry and Pellman, John and Lurie, Daniel and Lee, John and Wagner, Adina and Feingold, Franklin and Carlin, Johan and Samuels, Kalle and Meyer, Kyle},
	month = oct,
	title = {nipy/heudiconv:},
	year = 2021}

@article{Nissen2016-bl,
	abstract = {Science is facing a ``replication crisis'' in which many
              experimental findings cannot be replicated and are likely to be
              false. Does this imply that many scientific facts are false as
              well? To find out, we explore the process by which a claim
              becomes fact. We model the community's confidence in a claim as a
              Markov process with successive published results shifting the
              degree of belief. Publication bias in favor of positive findings
              influences the distribution of published results. We find that
              unless a sufficient fraction of negative results are published,
              false claims frequently can become canonized as fact.
              Data-dredging, p-hacking, and similar behaviors exacerbate the
              problem. Should negative results become easier to publish as a
              claim approaches acceptance as a fact, however, true and false
              claims would be more readily distinguished. To the degree that
              the model reflects the real world, there may be serious concerns
              about the validity of purported facts in some disciplines.},
	author = {Nissen, Silas Boye and Magidson, Tali and Gross, Kevin and Bergstrom, Carl T},
	journal = {eLife},
	keywords = {false positive; hypothesis testing; none; publication bias; replication crisis},
	language = {en},
	month = dec,
	title = {Publication bias and the canonization of false facts},
	volume = 5,
	year = 2016}

@article{Tadel2019-te,
	abstract = {Brainstorm is a free, open-source Matlab and Java application for
              multimodal electrophysiology data analytics and source imaging
              [primarily MEG, EEG and depth recordings, and integration with
              MRI and functional near infrared spectroscopy (fNIRS)]. We also
              provide a free, platform-independent executable version to users
              without a commercial Matlab license. Brainstorm has a rich and
              intuitive graphical user interface, which facilitates learning
              and augments productivity for a wider range of neuroscience users
              with little or no knowledge of scientific coding and scripting.
              Yet, it can also be used as a powerful scripting tool for
              reproducible and shareable batch processing of (large) data
              volumes. This article describes these Brainstorm interactive and
              scripted features via illustration through the complete analysis
              of group data from 16 participants in a MEG vision study.},
	author = {Tadel, Fran{\c c}ois and Bock, Elizabeth and Niso, Guiomar and Mosher, John C and Cousineau, Martin and Pantazis, Dimitrios and Leahy, Richard M and Baillet, Sylvain},
	journal = {Frontiers in Neuroscience},
	keywords = {brain imaging data structure (BIDS); electroencephalography (EEG); good practice; group analysis; magnetoencephalography (MEG); open data; open source; reproducibility},
	language = {en},
	month = feb,
	pages = {76},
	title = {{MEG/EEG} group analysis with brainstorm},
	volume = 13,
	year = 2019}

@article{Schilling2021-hd,
	abstract = {White matter bundle segmentation using diffusion MRI fiber
              tractography has become the method of choice to identify white
              matter fiber pathways in vivo in human brains. However, like
              other analyses of complex data, there is considerable variability
              in segmentation protocols and techniques. This can result in
              different reconstructions of the same intended white matter
              pathways, which directly affects tractography results,
              quantification, and interpretation. In this study, we aim to
              evaluate and quantify the variability that arises from different
              protocols for bundle segmentation. Through an open call to users
              of fiber tractography, including anatomists, clinicians, and
              algorithm developers, 42 independent teams were given processed
              sets of human whole-brain streamlines and asked to segment 14
              white matter fascicles on six subjects. In total, we received 57
              different bundle segmentation protocols, which enabled detailed
              volume-based and streamline-based analyses of agreement and
              disagreement among protocols for each fiber pathway. Results show
              that even when given the exact same sets of underlying
              streamlines, the variability across protocols for bundle
              segmentation is greater than all other sources of variability in
              the virtual dissection process, including variability within
              protocols and variability across subjects. In order to foster the
              use of tractography bundle dissection in routine clinical
              settings, and as a fundamental analytical tool, future endeavors
              must aim to resolve and reduce this heterogeneity. Although
              external validation is needed to verify the anatomical accuracy
              of bundle dissections, reducing heterogeneity is a step towards
              reproducible research and may be achieved through the use of
              standard nomenclature and definitions of white matter bundles and
              well-chosen constraints and decisions in the dissection process.},
	author = {Schilling, Kurt G and Rheault, Fran{\c c}ois and Petit, Laurent and Hansen, Colin B and Nath, Vishwesh and Yeh, Fang-Cheng and Girard, Gabriel and Barakovic, Muhamed and Rafael-Patino, Jonathan and Yu, Thomas and Fischi-Gomez, Elda and Pizzolato, Marco and Ocampo-Pineda, Mario and Schiavi, Simona and Canales-Rodr{\'\i}guez, Erick J and Daducci, Alessandro and Granziera, Cristina and Innocenti, Giorgio and Thiran, Jean-Philippe and Mancini, Laura and Wastling, Stephen and Cocozza, Sirio and Petracca, Maria and Pontillo, Giuseppe and Mancini, Matteo and Vos, Sjoerd B and Vakharia, Vejay N and Duncan, John S and Melero, Helena and Manzanedo, Lidia and Sanz-Morales, Emilio and Pe{\~n}a-Meli{\'a}n, {\'A}ngel and Calamante, Fernando and Atty{\'e}, Arnaud and Cabeen, Ryan P and Korobova, Laura and Toga, Arthur W and Vijayakumari, Anupa Ambili and Parker, Drew and Verma, Ragini and Radwan, Ahmed and Sunaert, Stefan and Emsell, Louise and De Luca, Alberto and Leemans, Alexander and Bajada, Claude J and Haroon, Hamied and Azadbakht, Hojjatollah and Chamberland, Maxime and Genc, Sila and Tax, Chantal M W and Yeh, Ping-Hong and Srikanchana, Rujirutana and Mcknight, Colin D and Yang, Joseph Yuan-Mou and Chen, Jian and Kelly, Claire E and Yeh, Chun-Hung and Cochereau, Jerome and Maller, Jerome J and Welton, Thomas and Almairac, Fabien and Seunarine, Kiran K and Clark, Chris A and Zhang, Fan and Makris, Nikos and Golby, Alexandra and Rathi, Yogesh and O'Donnell, Lauren J and Xia, Yihao and Aydogan, Dogu Baran and Shi, Yonggang and Fernandes, Francisco Guerreiro and Raemaekers, Mathijs and Warrington, Shaun and Michielse, Stijn and Ram{\'\i}rez-Manzanares, Alonso and Concha, Luis and Aranda, Ram{\'o}n and Meraz, Mariano Rivera and Lerma-Usabiaga, Garikoitz and Roitman, Lucas and Fekonja, Lucius S and Calarco, Navona and Joseph, Michael and Nakua, Hajer and Voineskos, Aristotle N and Karan, Philippe and Grenier, Gabrielle and Legarreta, Jon Haitz and Adluru, Nagesh and Nair, Veena A and Prabhakaran, Vivek and Alexander, Andrew L and Kamagata, Koji and Saito, Yuya and Uchida, Wataru and Andica, Christina and Abe, Masahiro and Bayrak, Roza G and Wheeler-Kingshott, Claudia A M Gandini and D'Angelo, Egidio and Palesi, Fulvia and Savini, Giovanni and Rolandi, Nicol{\`o} and Guevara, Pamela and Houenou, Josselin and L{\'o}pez-L{\'o}pez, Narciso and Mangin, Jean-Fran{\c c}ois and Poupon, Cyril and Rom{\'a}n, Claudio and V{\'a}zquez, Andrea and Maffei, Chiara and Arantes, Mavilde and Andrade, Jos{\'e} Paulo and Silva, Susana Maria and Calhoun, Vince D and Caverzasi, Eduardo and Sacco, Simone and Lauricella, Michael and Pestilli, Franco and Bullock, Daniel and Zhan, Yang and Brignoni-Perez, Edith and Lebel, Catherine and Reynolds, Jess E and Nestrasil, Igor and Labounek, Ren{\'e} and Lenglet, Christophe and Paulson, Amy and Aulicka, Stefania and Heilbronner, Sarah R and Heuer, Katja and Chandio, Bramsh Qamar and Guaje, Javier and Tang, Wei and Garyfallidis, Eleftherios and Raja, Rajikha and Anderson, Adam W and Landman, Bennett A and Descoteaux, Maxime},
	journal = {Neuroimage},
	keywords = {Bundle segmentation; Dissection; Fiber pathways; Tractography; White matter},
	language = {en},
	month = nov,
	pages = {118502},
	title = {Tractography dissection variability: What happens when 42 groups dissect 14 white matter bundles on the same dataset?},
	volume = 243,
	year = 2021}

@article{Glasser2013-qt,
	abstract = {The Human Connectome Project (HCP) faces the challenging task of
              bringing multiple magnetic resonance imaging (MRI) modalities
              together in a common automated preprocessing framework across a
              large cohort of subjects. The MRI data acquired by the HCP differ
              in many ways from data acquired on conventional 3 Tesla scanners
              and often require newly developed preprocessing methods. We
              describe the minimal preprocessing pipelines for structural,
              functional, and diffusion MRI that were developed by the HCP to
              accomplish many low level tasks, including spatial
              artifact/distortion removal, surface generation, cross-modal
              registration, and alignment to standard space. These pipelines
              are specially designed to capitalize on the high quality data
              offered by the HCP. The final standard space makes use of a
              recently introduced CIFTI file format and the associated
              grayordinate spatial coordinate system. This allows for combined
              cortical surface and subcortical volume analyses while reducing
              the storage and processing requirements for high spatial and
              temporal resolution data. Here, we provide the minimum image
              acquisition requirements for the HCP minimal preprocessing
              pipelines and additional advice for investigators interested in
              replicating the HCP's acquisition protocols or using these
              pipelines. Finally, we discuss some potential future improvements
              to the pipelines.},
	author = {Glasser, Matthew F and Sotiropoulos, Stamatios N and Wilson, J Anthony and Coalson, Timothy S and Fischl, Bruce and Andersson, Jesper L and Xu, Junqian and Jbabdi, Saad and Webster, Matthew and Polimeni, Jonathan R and Van Essen, David C and Jenkinson, Mark and {WU-Minn HCP Consortium}},
	journal = {Neuroimage},
	keywords = {CIFTI; Grayordinates; Human Connectome Project; Image analysis pipeline; Multi-modal data integration; Surface-based analysis},
	language = {en},
	month = oct,
	pages = {105--124},
	title = {The minimal preprocessing pipelines for the Human Connectome Project},
	volume = 80,
	year = 2013}

@article{Nosek2012-lx,
	abstract = {An academic scientist's professional success depends on
              publishing. Publishing norms emphasize novel, positive results.
              As such, disciplinary incentives encourage design, analysis, and
              reporting decisions that elicit positive results and ignore
              negative results. Prior reports demonstrate how these incentives
              inflate the rate of false effects in published science. When
              incentives favor novelty over replication, false results persist
              in the literature unchallenged, reducing efficiency in knowledge
              accumulation. Previous suggestions to address this problem are
              unlikely to be effective. For example, a journal of negative
              results publishes otherwise unpublishable reports. This enshrines
              the low status of the journal and its content. The persistence of
              false findings can be meliorated with strategies that make the
              fundamental but abstract accuracy motive-getting it
              right-competitive with the more tangible and concrete
              incentive-getting it published. This article develops strategies
              for improving scientific practices and knowledge accumulation
              that account for ordinary human motivations and biases.},
	author = {Nosek, Brian A and Spies, Jeffrey R and Motyl, Matt},
	journal = {Perspect. Psychol. Sci.},
	keywords = {false positives; incentives; methodology; motivated reasoning; replication},
	language = {en},
	month = nov,
	number = 6,
	pages = {615--631},
	title = {Scientific Utopia: {II}. Restructuring Incentives and Practices to Promote Truth Over Publishability},
	volume = 7,
	year = 2012}

@article{Carp2012-kj,
	abstract = {Replication of research findings is critical to the progress of
              scientific understanding. Accordingly, most scientific journals
              require authors to report experimental procedures in sufficient
              detail for independent researchers to replicate their work. To
              what extent do research reports in the functional neuroimaging
              literature live up to this standard? The present study evaluated
              methods reporting and methodological choices across 241 recent
              fMRI articles. Many studies did not report critical
              methodological details with regard to experimental design, data
              acquisition, and analysis. Further, many studies were
              underpowered to detect any but the largest statistical effects.
              Finally, data collection and analysis methods were highly
              flexible across studies, with nearly as many unique analysis
              pipelines as there were studies in the sample. Because the rate
              of false positive results is thought to increase with the
              flexibility of experimental designs, the field of functional
              neuroimaging may be particularly vulnerable to false positives.
              In sum, the present study documented significant gaps in methods
              reporting among fMRI studies. Improved methodological
              descriptions in research reports would yield significant benefits
              for the field.},
	author = {Carp, Joshua},
	journal = {Neuroimage},
	language = {en},
	month = oct,
	number = 1,
	pages = {289--300},
	title = {The secret lives of experiments: methods reporting in the {fMRI} literature},
	volume = 63,
	year = 2012}

@article{Poldrack2020-ht,
	author = {Poldrack, Russell A and Whitaker, Kirstie and Kennedy, David},
	journal = {Neuroimage},
	language = {en},
	month = sep,
	pages = {116357},
	title = {Introduction to the special issue on reproducibility in neuroimaging},
	volume = 218,
	year = 2020}

@article{Nosek2019-zt,
	abstract = {Preregistration clarifies the distinction between planned and
              unplanned research by reducing unnoticed flexibility. This
              improves credibility of findings and calibration of uncertainty.
              However, making decisions before conducting analyses requires
              practice. During report writing, respecting both what was planned
              and what actually happened requires good judgment and humility in
              making claims.},
	author = {Nosek, Brian A and Beck, Emorie D and Campbell, Lorne and Flake, Jessica K and Hardwicke, Tom E and Mellor, David T and van 't Veer, Anna E and Vazire, Simine},
	journal = {Trends in Cognitive Sciences},
	keywords = {confirmatory research; exploratory research; preregistration; reproducibility; transparency},
	language = {en},
	month = oct,
	number = 10,
	pages = {815--818},
	title = {Preregistration is hard, and worthwhile},
	volume = 23,
	year = 2019}

@inproceedings{Delorme2021-ue,
	author = {Delorme, Arnaud and Truong, Dung and Martinez-Cancino, Ramon and Pernet, Cyril R and Sivagnanam, Subha and Yoshimoto, Kenneth and Poldrack, Russ and Majumdar, Amit and Makeig, Scott},
	conference = {10th International IEEE/EMBS Conference on Neural Engineering (NER)},
	title = {Tools for importing and evaluating {BIDS-EEG} formatted data},
	year = 2021}

@article{Stikov2019-xd,
	author = {Stikov, Nikola and Trzasko, Joshua D and Bernstein, Matt A},
	journal = {Magnetic Resonance in Medicine},
	language = {en},
	month = dec,
	number = 6,
	pages = {1981--1983},
	title = {Reproducibility and the future of {MRI} research},
	volume = 82,
	year = 2019}

@article{Errington2021-mw,
	author = {Errington, Timothy M and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
	journal = {eLife},
	title = {Challenges for assessing replicability in preclinical cancer biology},
	volume = 10,
	year = 2021}

@article{Abraham2014-cr,
	abstract = {Statistical machine learning methods are increasingly used for
              neuroimaging data analysis. Their main virtue is their ability to
              model high-dimensional datasets, e.g., multivariate analysis of
              activation images or resting-state time series. Supervised
              learning is typically used in decoding or encoding settings to
              relate brain images to behavioral or clinical observations, while
              unsupervised learning can uncover hidden structures in sets of
              images (e.g., resting state functional MRI) or find
              sub-populations in large cohorts. By considering different
              functional neuroimaging applications, we illustrate how
              scikit-learn, a Python machine learning library, can be used to
              perform some key analysis steps. Scikit-learn contains a very
              large set of statistical learning algorithms, both supervised and
              unsupervised, and its application to neuroimaging data provides a
              versatile tool to study the brain.},
	author = {Abraham, Alexandre and Pedregosa, Fabian and Eickenberg, Michael and Gervais, Philippe and Mueller, Andreas and Kossaifi, Jean and Gramfort, Alexandre and Thirion, Bertrand and Varoquaux, Ga{\"e}l},
	journal = {Frontiers in Neuroinformatics},
	keywords = {Python; machine learning; neuroimaging; scikit-learn; statistical learning},
	language = {en},
	month = feb,
	pages = {14},
	title = {Machine learning for neuroimaging with scikit-learn},
	volume = 8,
	year = 2014}

@article{Hanke2021-un,
	abstract = {Decentralized research data management (dRDM) systems handle
               digital research objects across participating nodes without
               critically relying on central services. We present four
               perspectives in defense of dRDM, illustrating that, in contrast
               to centralized or federated research data management solutions,
               a dRDM system based on heterogeneous but interoperable
               components can offer a sustainable, resilient, inclusive, and
               adaptive infrastructure for scientific stakeholders: An
               individual scientist or laboratory, a research institute, a
               domain data archive or cloud computing platform, and a
               collaborative multisite consortium. All perspectives share the
               use of a common, self-contained, portable data structure as an
               abstraction from current technology and service choices. In
               conjunction, the four perspectives review how varying
               requirements of independent scientific stakeholders can be
               addressed by a scalable, uniform dRDM solution and present a
               working system as an exemplary implementation.},
	author = {Hanke, Michael and Pestilli, Franco and Wagner, Adina S and Markiewicz, Christopher J and Poline, Jean-Baptiste and Halchenko, Yaroslav O},
	journal = {Neuroforum},
	keywords = {BrainLife; Canadian Open Neuroscience Platform; DataLad; Interoperability; OpenNeuro},
	language = {en},
	month = feb,
	number = 1,
	pages = {17--25},
	publisher = {De Gruyter},
	title = {In defense of decentralized research data management},
	volume = 27,
	year = 2021}

@article{Poldrack2017-xt,
	abstract = {Functional neuroimaging techniques have transformed our ability
              to probe the neurobiological basis of behaviour and are
              increasingly being applied by the wider neuroscience community.
              However, concerns have recently been raised that the conclusions
              drawn from some human neuroimaging studies are either spurious or
              not generalizable. Problems such as low statistical power,
              flexibility in data analysis, software errors, and lack of direct
              replication apply to many fields, but perhaps particularly to
              fMRI. Here we discuss these problems, outline current and
              suggested best practices, and describe how we think the field
              should evolve to produce the most meaningful answers to
              neuroscientific questions.},
	author = {Poldrack, Russell A and Baker, Chris I and Durnez, Joke and Gorgolewski, Krzysztof J and Matthews, Paul M and Munaf{\`o}, Marcus R and Nichols, Thomas E and Poline, Jean Baptiste and Vul, Edward and Yarkoni, Tal},
	journal = {Nat. Rev. Neurosci.},
	number = 2,
	pages = {115--126},
	title = {Scanning the horizon: Towards transparent and reproducible neuroimaging research},
	volume = 18,
	year = 2017}

@article{Nichols2017-qb,
	abstract = {Given concerns about the reproducibility of scientific findings,
              neuroimaging must define best practices for data analysis,
              results reporting, and algorithm and data sharing to promote
              transparency, reliability and collaboration. We describe insights
              from developing a set of recommendations on behalf of the
              Organization for Human Brain Mapping and identify barriers that
              impede these practices, including how the discipline must change
              to fully exploit the potential of the world's neuroimaging data.},
	author = {Nichols, Thomas E and Das, Samir and Eickhoff, Simon B and Evans, Alan C and Glatard, Tristan and Hanke, Michael and Kriegeskorte, Nikolaus and Milham, Michael P and Poldrack, Russell A and Poline, Jean-Baptiste and Proal, Erika and Thirion, Bertrand and Van Essen, David C and White, Tonya and Yeo, B T Thomas},
	journal = {Nature Neuroscience},
	language = {en},
	month = feb,
	number = 3,
	pages = {299--303},
	title = {Best practices in data analysis and sharing in neuroimaging using {MRI}},
	volume = 20,
	year = 2017}

@article{Meyer2021-ze,
	abstract = {Developmental research using electroencephalography (EEG) offers
              valuable insights in brain processes early in life, but at the
              same time, applying this sensitive technique to young children
              who are often non-compliant and have short attention spans comes
              with practical limitations. It is thus of particular importance
              to optimally use the limited resources to advance our
              understanding of development through reproducible and replicable
              research practices. Here, we describe methodological approaches
              that help maximize the reproducibility of developmental EEG
              research. We discuss how to transform EEG data into the
              standardized Brain Imaging Data Structure (BIDS) which organizes
              data according to the FAIR data sharing principles. We provide a
              tutorial on how to use cluster-based permutation testing to
              analyze developmental EEG data. This versatile test statistic
              solves the multiple comparison problem omnipresent in EEG
              analysis and thereby substantially decreases the risk of
              reporting false discoveries. Finally, we describe how to quantify
              effect sizes, in particular of cluster-based permutation results.
              Reporting effect sizes conveys a finding's impact and robustness
              which in turn informs future research. To demonstrate these
              methodological approaches to data organization, analysis and
              report, we use a publicly accessible infant EEG dataset and
              provide a complete copy of the analysis code.},
	author = {Meyer, Marlene and Lamers, Didi and Kayhan, Ezgi and Hunnius, Sabine and Oostenveld, Robert},
	journal = {Dev. Cogn. Neurosci.},
	keywords = {BIDS; EEG; cluster-based permutation test; effect size; reproducibility},
	language = {en},
	month = dec,
	pages = {101036},
	title = {Enhancing reproducibility in developmental {EEG} research: {BIDS}, cluster-based permutation tests, and effect sizes},
	volume = 52,
	year = 2021}

@article{Camerer2018-cb,
	abstract = {Being able to replicate scientific findings is crucial for
              scientific progress. We replicate 21 systematically selected
              experimental studies in the social sciences published in Nature
              and Science between 2010 and 2015. The replications follow
              analysis plans reviewed by the original authors and
              pre-registered prior to the replications. The replications are
              high powered, with sample sizes on average about five times
              higher than in the original studies. We find a significant effect
              in the same direction as the original study for 13 (62\%)
              studies, and the effect size of the replications is on average
              about 50\% of the original effect size. Replicability varies
              between 12 (57\%) and 14 (67\%) studies for complementary
              replicability indicators. Consistent with these results, the
              estimated true-positive rate is 67\% in a Bayesian analysis. The
              relative effect size of true positives is estimated to be 71\%,
              suggesting that both false positives and inflated effect sizes of
              true positives contribute to imperfect reproducibility.
              Furthermore, we find that peer beliefs of replicability are
              strongly related to replicability, suggesting that the research
              community could predict which results would replicate and that
              failures to replicate were not the result of chance alone.},
	author = {Camerer, Colin F and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
	journal = {Nature Human Behaviour},
	language = {en},
	month = sep,
	number = 9,
	pages = {637--644},
	title = {Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015},
	volume = 2,
	year = 2018}

@article{Poldrack2014-lr,
	abstract = {In the last decade, major advances have been made in the
              availability of shared neuroimaging data, such that there are
              more than 8,000 shared MRI (magnetic resonance imaging) data sets
              available online. Here we outline the state of data sharing for
              task-based functional MRI (fMRI) data, with a focus on various
              forms of data and their relative utility for subsequent analyses.
              We also discuss challenges to the future success of data sharing
              and highlight the ethical argument that data sharing may be
              necessary to maximize the contribution of human subjects.},
	author = {Poldrack, Russell A and Gorgolewski, Krzysztof J},
	journal = {Nature Neuroscience},
	language = {en},
	month = nov,
	number = 11,
	pages = {1510--1517},
	title = {Making big data open: Data sharing in neuroimaging},
	volume = 17,
	year = 2014}

@article{Jellus2014-jy,
	author = {Jell{\'u}s, V and Kannengiesser, S A R},
	date-modified = {2022-08-01 14:12:22 +0300},
	journal = {Joint Annual Meeting ISMRM-ESMRMB},
	pages = {4406},
	title = {Adaptive Coil Combination Using a Body Coil Scan as Phase Reference},
	year = 2014}

@article{Van_Vliet2018-ob,
	abstract = {Communication between brain regions is thought to be facilitated
              by the synchronization of oscillatory activity. Hence,
              large-scale functional networks within the brain may be estimated
              by measuring synchronicity between regions. Neurophysiological
              recordings, such as magnetoencephalography (MEG) and
              electroencephalography (EEG), provide a direct measure of
              oscillatory neural activity with millisecond temporal resolution.
              In this paper, we describe a full data analysis pipeline for
              functional connectivity analysis based on dynamic imaging of
              coherent sources (DICS) of MEG data. DICS is a beamforming
              technique in the frequency-domain that enables the study of the
              cortical sources of oscillatory activity and synchronization
              between brain regions. All the analysis steps, starting from the
              raw MEG data up to publication-ready group-level statistics and
              visualization, are discussed in depth, including methodological
              considerations, rules of thumb and tradeoffs. We start by
              computing cross-spectral density (CSD) matrices using a wavelet
              approach in several frequency bands (alpha, theta, beta, gamma).
              We then provide a way to create comparable source spaces across
              subjects and discuss the cortical mapping of spectral power. For
              connectivity analysis, we present a canonical computation of
              coherence that facilitates a stable estimation of all-to-all
              connectivity. Finally, we use group-level statistics to limit the
              network to cortical regions for which significant differences
              between experimental conditions are detected and produce vertex-
              and parcel-level visualizations of the different brain networks.
              Code examples using the MNE-Python package are provided at each
              step, guiding the reader through a complete analysis of the
              freely available openfMRI ds000117 ``familiar vs. unfamiliar vs.
              scrambled faces'' dataset. The goal is to educate both novice and
              experienced data analysts with the ``tricks of the trade''
              necessary to successfully perform this type of analysis on their
              own data.},
	author = {van Vliet, Marijn and Liljestr{\"o}m, Mia and Aro, Susanna and Salmelin, Riitta and Kujala, Jan},
	journal = {Front. Neurosci.},
	keywords = {DICS; MEG; brain rhythms; coherence; tutorial; workflow},
	language = {en},
	month = sep,
	pages = {586},
	title = {Analysis of Functional Connectivity and Oscillatory Power Using {DICS}: From Raw {MEG} Data to {Group-Level} Statistics in Python},
	volume = 12,
	year = 2018}

@article{Delorme2004-ah,
	abstract = {We have developed a toolbox and graphic user interface, EEGLAB,
              running under the crossplatform MATLAB environment (The
              Mathworks, Inc.) for processing collections of single-trial
              and/or averaged EEG data of any number of channels. Available
              functions include EEG data, channel and event information
              importing, data visualization (scrolling, scalp map and dipole
              model plotting, plus multi-trial ERP-image plots), preprocessing
              (including artifact rejection, filtering, epoch selection, and
              averaging), independent component analysis (ICA) and
              time/frequency decompositions including channel and component
              cross-coherence supported by bootstrap statistical methods based
              on data resampling. EEGLAB functions are organized into three
              layers. Top-layer functions allow users to interact with the data
              through the graphic interface without needing to use MATLAB
              syntax. Menu options allow users to tune the behavior of EEGLAB
              to available memory. Middle-layer functions allow users to
              customize data processing using command history and interactive
              'pop' functions. Experienced MATLAB users can use EEGLAB data
              structures and stand-alone signal processing functions to write
              custom and/or batch analysis scripts. Extensive function help and
              tutorial information are included. A 'plug-in' facility allows
              easy incorporation of new EEG modules into the main menu. EEGLAB
              is freely available (http://www.sccn.ucsd.edu/eeglab/) under the
              GNU public license for noncommercial use and open source
              development, together with sample data, user tutorial and
              extensive documentation.},
	author = {Delorme, Arnaud and Makeig, Scott},
	journal = {Journal of Neuroscience Methods},
	language = {en},
	month = mar,
	number = 1,
	pages = {9--21},
	title = {{EEGLAB}: An open source toolbox for analysis of single-trial {EEG} dynamics including independent component analysis},
	volume = 134,
	year = 2004}

@article{Henrich2010-ew,
	abstract = {Behavioral scientists routinely publish broad claims about human
               psychology and behavior in the world's top journals based on
               samples drawn entirely from Western, Educated, Industrialized,
               Rich, and Democratic (WEIRD) societies. Researchers -- often
               implicitly -- assume that either there is little variation
               across human populations, or that these ``standard subjects''
               are as representative of the species as any other population.
               Are these assumptions justified? Here, our review of the
               comparative database from across the behavioral sciences
               suggests both that there is substantial variability in
               experimental results across populations and that WEIRD subjects
               are particularly unusual compared with the rest of the species
               -- frequent outliers. The domains reviewed include visual
               perception, fairness, cooperation, spatial reasoning,
               categorization and inferential induction, moral reasoning,
               reasoning styles, self-concepts and related motivations, and the
               heritability of IQ. The findings suggest that members of WEIRD
               societies, including young children, are among the least
               representative populations one could find for generalizing about
               humans. Many of these findings involve domains that are
               associated with fundamental aspects of psychology, motivation,
               and behavior -- hence, there are no obvious a priori grounds for
               claiming that a particular behavioral phenomenon is universal
               based on sampling from a single subpopulation. Overall, these
               empirical patterns suggests that we need to be less cavalier in
               addressing questions of human nature on the basis of data drawn
               from this particularly thin, and rather unusual, slice of
               humanity. We close by proposing ways to structurally re-organize
               the behavioral sciences to best tackle these challenges.},
	author = {Henrich, Joseph and Heine, Steven J and Norenzayan, Ara},
	journal = {Behavioral and Brain Sciences},
	keywords = {behavioral economics; cross-cultural research; cultural psychology; culture; evolutionary psychology; experiments; external validity; generalizability; human universals; population variability},
	month = jun,
	number = {2-3},
	pages = {61--83},
	publisher = {Cambridge University Press},
	title = {The weirdest people in the world?},
	volume = 33,
	year = 2010}

@article{Amano2021-wl,
	author = {Amano, Tatsuya and Rios Rojas, Clarissa and Boum, Ii, Yap and Calvo, Margarita and Misra, Biswapriya B},
	journal = {Nature Human Behaviour},
	language = {en},
	month = sep,
	number = 9,
	pages = {1119--1122},
	title = {Ten tips for overcoming language barriers in science},
	volume = 5,
	year = 2021}

@article{Tong2021-aj,
	abstract = {Open-source pulse sequence programs offer an accessible and
              transparent approach to sequence development and deployment.
              However, a common framework for testing, documenting, and sharing
              open-source sequences is still needed to ensure sequence
              usability and repeatability. We propose and demonstrate such a
              framework by implementing two sequences, Inversion Recovery Spin
              Echo (IRSE) and Turbo Spin Echo (TSE), with PyPulseq, and testing
              them on a commercial 3 T scanner. We used the ACR and ISMRM/NIST
              phantoms for qualitative imaging and T1/T2 mapping, respectively.
              The qualitative sequences show good agreement with
              vendor-provided counterparts (mean Structural Similarity Index
              Measure (SSIM) = 0.810 for IRSE and 0.826 for TSE). Both
              sequences passed five out of the seven standard ACR tests,
              performing at similar levels to vendor counterparts. Compared to
              reference values, the coefficient of determination R2 was 0.9946
              for IRSE T1 mapping and 0.9331 for TSE T2 mapping. All sequences
              passed the scanner safety check for a 70 kg, 175 cm subject. The
              framework was demonstrated by packaging the sequences and sharing
              them on GitHub with data and documentation on the file
              generation, acquisition, reconstruction, and post-processing
              steps. The same sequences were tested at a second site using a
              1.5 T scanner with the information shared. PDF templates for both
              sequence developers and users were created and filled.},
	author = {Tong, Gehua and Gaspar, Andreia S and Qian, Enlin and Ravi, Keerthi Sravan and Vaughan, Jr, John Thomas and Nunes, Rita G and Geethanath, Sairam},
	journal = {Magnetic Resonance Imaging},
	keywords = {Pulse sequence programming; Pulse sequence simulation; Repeatability; Reproducible research},
	language = {en},
	month = nov,
	pages = {7--18},
	title = {A framework for validating open-source pulse sequences},
	volume = 87,
	year = 2021}

@article{Strother2004-nk,
	abstract = {We argue that published results demonstrate that new insights
              into human brain function may be obscured by poor and/or limited
              choices in the data-processing pipeline, and review the work on
              performance metrics for optimizing pipelines: prediction,
              reproducibility, and related empirical Receiver Operating
              Characteristic (ROC) curve metrics. Using the NPAIRS split-half
              resampling framework for estimating prediction/reproducibility
              metrics (Strother et al., 2002), we illustrate its use by testing
              the relative importance of selected pipeline components
              (interpolation, in-plane spatial smoothing, temporal detrending,
              and between-subject alignment) in a group analysis of BOLD-fMRI
              scans from 16 subjects performing a block-design,
              parametric-static-force task. Large-scale brain networks were
              detected using a multivariate linear discriminant analysis
              (canonical variates analysis, CVA) that was tuned to fit the
              data. We found that tuning the CVA model and spatial smoothing
              were the most important processing parameters. Temporal
              detrending was essential to remove low-frequency, reproducing
              time trends; the number of cosine basis functions for detrending
              was optimized by assuming that separate epochs of baseline scans
              have constant, equal means, and this assumption was assessed with
              prediction metrics. Higher-order polynomial warps compared to
              affine alignment had only a minor impact on the performance
              metrics. We found that both prediction and reproducibility
              metrics were required for optimizing the pipeline and give
              somewhat different results. Moreover, the parameter settings of
              components in the pipeline interact so that the current practice
              of reporting the optimization of components tested in relative
              isolation is unlikely to lead to fully optimized processing
              pipelines.},
	author = {Strother, Stephen C and La Conte, Stephen and Kai Hansen, Lars and Anderson, Jon and Zhang, Jin and Pulapura, Sujit and Rottenberg, David},
	journal = {Neuroimage},
	language = {en},
	pages = {S196--207},
	title = {Optimizing the {fMRI} data-processing pipeline using prediction and reproducibility performance metrics: I. A preliminary group analysis},
	volume = {23 Suppl 1},
	year = 2004}

@article{The_Open_Brain_Consent_working_group2021-pp,
	author = {{The Open Brain Consent working group}},
	journal = {Human Brain Mapping},
	number = 7,
	pages = {1945--1951},
	title = {The Open Brain Consent: Informing research participants and obtaining consent to share brain imaging data},
	volume = 42,
	year = 2021}

@article{Paret2021-ky,
	abstract = {Replicability and reproducibility of scientific findings is
              paramount for sustainable progress in neuroscience.
              Preregistration of the hypotheses and methods of an empirical
              study before analysis, the sharing of primary research data, and
              compliance with data standards such as the Brain Imaging Data
              Structure (BIDS), are considered effective practices to secure
              progress and to substantiate quality of research. We investigated
              the current level of adoption of open science practices in
              neuroimaging and the difficulties that prevent researchers from
              using them. Email invitations to participate in the survey were
              sent to addresses received through a PubMed search of human
              functional magnetic resonance imaging studies between 2010 and
              2020. 283 persons completed the questionnaire. Although half of
              the participants were experienced with preregistration, the
              willingness to preregister studies in the future was modest. The
              majority of participants had experience with the sharing of
              primary neuroimaging data. Most of the participants were
              interested in implementing a standardized data structure such as
              BIDS in their labs. Based on demographic variables, we compared
              participants on seven subscales, which had been generated through
              factor analysis. It was found that experienced researchers at
              lower career level had higher fear of being transparent,
              researchers with residence in the EU had a higher need for data
              governance, and researchers at medical faculties as compared to
              other university faculties reported a higher need for data
              governance and a more unsupportive environment. The results
              suggest growing adoption of open science practices but also
              highlight a number of important impediments. \#\#\# Competing
              Interest Statement The authors have declared no competing
              interest.},
	author = {Paret, Christian and Unverhau, Nike and Feingold, Franklin and Poldrack, Russell A and Stirner, Madita and Schmahl, Christian and Sicorello, Maurizio},
	date-modified = {2022-07-28 01:16:42 +0300},
	journal = {bioRxiv},
	language = {en},
	month = nov,
	pages = {2021.11.26.470115},
	title = {Survey on open science practices in functional neuroimaging},
	year = 2021}

@article{Clayson2022-gh,
	abstract = {Open science practices are gaining momentum in
              psychophysiological research, but at the nascent stage of this
              special issue in the International Journal of Psychophysiology
              there was no systematic collection of resources to support the
              adoption of open science practices specific to studies of human
              electrophysiology (EEG). The purpose of this special issue was to
              gather and provide resources that identify the idiosyncratic
              considerations and implications of open science practices
              specifically for studies of human EEG and event-related
              potentials (ERPs). Papers also show the importance of promoting
              good scientific practices in the application of open science
              principles to EEG and ERPs. This introduction to the special
              issue provides a roadmap for identifying the resources necessary
              to begin and improve the application of open science principles
              to EEG and ERP research. We are optimistic that open science
              practices will help increase the robustness, rigor, and
              replicability of EEG/ERP research and ultimately become the norm
              in studies of EEG and ERPs.},
	author = {Clayson, Peter E and Keil, Andreas and Larson, Michael J},
	journal = {Int. J. Psychophysiol.},
	language = {en},
	month = apr,
	pages = {43--46},
	title = {Open science in human electrophysiology},
	volume = 174,
	year = 2022}

@article{Sasaki2008-cs,
	abstract = {PURPOSE: To determine whether and to what degree absolute
              apparent diffusion coefficient (ADC) values vary between
              different imagers, vendors, field strengths, and intraimager
              conditions. MATERIALS AND METHODS: Informed consent and
              institutional review board approval were obtained.
              Diffusion-weighted (DW) images with nearly identical parameters
              were obtained at 1.5 and 3.0 T from 12 healthy volunteers at
              seven institutions by using 10 magnetic resonance (MR) imagers
              provided by four different vendors. ADC maps were generated from
              isotropic DW maps, and images with a b value of 0 sec/mm(2) were
              generated by using in-house software. The mean pixel values for
              the brain tissues were calculated for evaluating the differences
              among coil systems, imagers, vendors, and magnetic field
              strengths. RESULTS: The absolute ADC values of gray and white
              matter from the same vendor varied substantially: 4\%-9\% at 1.5
              and 3.0 T. With the exception of one vendor, the intervendor
              variability at 1.5 T was as high as 7\%. Moreover, there was
              substantial intraimager variability, up to 8\%, depending on the
              coil systems in certain imagers. CONCLUSION: There is significant
              variability in ADC values depending on the coil systems, imagers,
              vendors, and field strengths used for MR imaging. The relative
              ADC values may be more suitable than absolute ADC values for
              evaluating diffusion abnormalities in patients enrolled in
              multicenter acute ischemic stroke trials.},
	author = {Sasaki, Makoto and Yamada, Kei and Watanabe, Yoshiyuki and Matsui, Mieko and Ida, Masahiro and Fujiwara, Shunrou and Shibata, Eri and {Acute Stroke Imaging Standardization Group-Japan (ASIST-Japan) Investigators}},
	journal = {Radiology},
	language = {en},
	month = nov,
	number = 2,
	pages = {624--630},
	title = {Variability in absolute apparent diffusion coefficient values across different platforms may be substantial: A multivendor, multi-institutional comparison study},
	volume = 249,
	year = 2008}

@inproceedings{Cox2004-rn,
	author = {Cox, R W and Ashburner, J and Breman, H and Fissell, K and Haselgrove, C and Holmes, C J and Lancaster, J L and Rex, D E and Smith, S M and Woodward, J B and Strother, And S C},
	conference = {Human Brain Mapping},
	location = {Budapest, Hungary},
	title = {A (sort of) new image data format standard: {NiFTI-1}},
	year = 2004}

@article{Desjardins2021-lh,
	abstract = {The methods available for pre-processing EEG data are rapidly
               evolving as researchers gain access to vast computational
               resources; however, the field {\ldots}},
	author = {Desjardins, J A and van Noordt, S and Huberty, S and Segalowitz, S J and Elsabbagh, M},
	journal = {Journal of Neuroscience Methods},
	month = jan,
	pages = {108961},
	publisher = {Elsevier},
	title = {{EEG} Integrated Platform Lossless ({EEG-IP-L}) pre-processing pipeline for objective signal quality assessment incorporating data annotation and blind source separation},
	volume = 347,
	year = 2021}

@article{Kerr1998-wf,
	abstract = {This article considers a practice in scientific communication
              termed HARKing (Hypothesizing After the Results are Known).
              HARKing is defined as presenting a post hoc hypothesis (i.e., one
              based on or informed by one's results) in one's research report
              as i f it were, in fact, an a priori hypotheses. Several forms of
              HARKing are identified and survey data are presented that
              suggests that at least some forms of HARKing are widely practiced
              and widely seen as inappropriate. I identify several reasons why
              scientists might HARK. Then I discuss several reasons why
              scientists ought not to HARK. It is conceded that the question of
              whether HARKing ' s costs exceed its benefits is a complex one
              that ought to be addressed through research, open discussion, and
              debate. To help stimulate such discussion (and for those such as
              myself who suspect that HARKing's costs do exceed its benefits),
              I conclude the article with some suggestions for deterring
              HARKing.},
	author = {Kerr, N L},
	journal = {Personality and Social Psychology Review},
	language = {en},
	number = 3,
	pages = {196--217},
	title = {{HARKing}: Hypothesizing after the results are known},
	volume = 2,
	year = 1998}

@article{Hunt2019-sd,
	author = {Hunt, Laurence T},
	journal = {Nat Hum Behav},
	language = {en},
	month = apr,
	number = 4,
	pages = {312--315},
	title = {The life-changing magic of sharing your data},
	volume = 3,
	year = 2019}

@misc{Robbins2021-qt,
	author = {Robbins, K and Truong, Dung and Jones, Alexander and Callanan, Ian and Makeig, Scott},
	publisher = {OSF},
	title = {Building {FAIR} functionality: Annotating events in time series data using Hierarchical Event Descriptors ({HED})},
	year = 2021}

@article{Kennedy2019-jy,
	abstract = {There has been a recent major upsurge in the concerns about
              reproducibility in many areas of science. Within the neuroimaging
              domain, one approach is to promote reproducibility is to target
              the re-executability of the publication. The information
              supporting such re-executability can enable the detailed
              examination of how an initial finding generalizes across changes
              in the processing approach, and sampled population, in a
              controlled scientific fashion. ReproNim: A Center for
              Reproducible Neuroimaging Computation is a recently funded
              initiative that seeks to facilitate the ``last mile''
              implementations of core re-executability tools in order to reduce
              the accessibility barrier and increase adoption of standards and
              best practices at the neuroimaging research laboratory level. In
              this report, we summarize the overall approach and tools we have
              developed in this domain.},
	author = {Kennedy, David N and Abraham, Sanu A and Bates, Julianna F and Crowley, Albert and Ghosh, Satrajit and Gillespie, Tom and Goncalves, Mathias and Grethe, Jeffrey S and Halchenko, Yaroslav O and Hanke, Michael and Haselgrove, Christian and Hodge, Steven M and Jarecka, Dorota and Kaczmarzyk, Jakub and Keator, David B and Meyer, Kyle and Martone, Maryann E and Padhy, Smruti and Poline, Jean-Baptiste and Preuss, Nina and Sincomb, Troy and Travers, Matt},
	journal = {Front. Neuroinform.},
	keywords = {data model; neuroimaging; publication; re-executability; reproducibility},
	language = {en},
	month = feb,
	pages = {1},
	title = {Everything Matters: The {ReproNim} Perspective on Reproducible Neuroimaging},
	volume = 13,
	year = 2019}

@article{Li2021-pk,
	author = {Li, X and Ai, L and Giavasis, S and Jin, H and Feczko, E and Xu, T and Clucas, J and Franco, A and Heinsfeld, A S and Adebimpe, A and {Others}},
	date-modified = {2022-08-01 13:41:13 +0300},
	journal = {bioRxiv},
	title = {Moving beyond processing and analysis-related variation in neuroscience},
	year = 2021}

@article{Carp2012-zu,
	abstract = {How likely are published findings in the functional neuroimaging
              literature to be false? According to a recent mathematical model,
              the potential for false positives increases with the flexibility
              of analysis methods. Functional MRI (fMRI) experiments can be
              analyzed using a large number of commonly used tools, with little
              consensus on how, when, or whether to apply each one. This
              situation may lead to substantial variability in analysis
              outcomes. Thus, the present study sought to estimate the
              flexibility of neuroimaging analysis by submitting a single
              event-related fMRI experiment to a large number of unique
              analysis procedures. Ten analysis steps for which multiple
              strategies appear in the literature were identified, and two to
              four strategies were enumerated for each step. Considering all
              possible combinations of these strategies yielded 6,912 unique
              analysis pipelines. Activation maps from each pipeline were
              corrected for multiple comparisons using five thresholding
              approaches, yielding 34,560 significance maps. While some
              outcomes were relatively consistent across pipelines, others
              showed substantial methods-related variability in activation
              strength, location, and extent. Some analysis decisions
              contributed to this variability more than others, and different
              decisions were associated with distinct patterns of variability
              across the brain. Qualitative outcomes also varied with analysis
              parameters: many contrasts yielded significant activation under
              some pipelines but not others. Altogether, these results reveal
              considerable flexibility in the analysis of fMRI experiments.
              This observation, when combined with mathematical simulations
              linking analytic flexibility with elevated false positive rates,
              suggests that false positive results may be more prevalent than
              expected in the literature. This risk of inflated false positive
              rates may be mitigated by constraining the flexibility of
              analytic choices or by abstaining from selective analysis
              reporting.},
	author = {Carp, Joshua},
	journal = {Front. Neurosci.},
	keywords = {Analysis flexibility; Data analysis; False positive results; Selective reporting; fMRI},
	number = {OCT},
	pages = {1--13},
	title = {On the plurality of (methodological) worlds: Estimating the analytic flexibility of fmri experiments},
	volume = 6,
	year = 2012}

@article{Borghi2018-nt,
	abstract = {Neuroimaging methods such as magnetic resonance imaging (MRI)
              involve complex data collection and analysis protocols, which
              necessitate the establishment of good research data management
              (RDM). Despite efforts within the field to address issues related
              to rigor and reproducibility, information about the RDM-related
              practices and perceptions of neuroimaging researchers remains
              largely anecdotal. To inform such efforts, we conducted an online
              survey of active MRI researchers that covered a range of
              RDM-related topics. Survey questions addressed the type(s) of
              data collected, tools used for data storage, organization, and
              analysis, and the degree to which practices are defined and
              standardized within a research group. Our results demonstrate
              that neuroimaging data is acquired in multifarious forms,
              transformed and analyzed using a wide variety of software tools,
              and that RDM practices and perceptions vary considerably both
              within and between research groups, with trainees reporting less
              consistency than faculty. Ratings of the maturity of RDM
              practices from ad-hoc to refined were relatively high during the
              data collection and analysis phases of a project and
              significantly lower during the data sharing phase. Perceptions of
              emerging practices including open access publishing and
              preregistration were largely positive, but demonstrated little
              adoption into current practice.},
	author = {Borghi, John A and Van Gulick, Ana E},
	journal = {PLoS One},
	language = {en},
	month = jul,
	number = 7,
	pages = {e0200562},
	title = {Data management and sharing in neuroimaging: Practices and perceptions of {MRI} researchers},
	volume = 13,
	year = 2018}

@article{Budzinski2020-co,
	author = {Budzinski, Oliver and Grebel, Thomas and Wolling, Jens and Zhang, Xijie},
	journal = {SSRN Electronic Journal},
	pages = {2185--2206},
	title = {Drivers of article processing charges in open access},
	volume = 124,
	year = 2020}

@article{Heunis2020-ie,
	abstract = {Neurofeedback training using real-time functional magnetic
              resonance imaging (rtfMRI-NF) allows subjects voluntary control
              of localised and distributed brain activity. It has sparked
              increased interest as a promising non-invasive treatment option
              in neuropsychiatric and neurocognitive disorders, although its
              efficacy and clinical significance are yet to be determined. In
              this work, we present the first extensive review of acquisition,
              processing and quality control methods available to improve the
              quality of the neurofeedback signal. Furthermore, we investigate
              the state of denoising and quality control practices in 128
              recently published rtfMRI-NF studies. We found: (a) that less
              than a third of the studies reported implementing standard
              real-time fMRI denoising steps, (b) significant room for
              improvement with regards to methods reporting and (c) the need
              for methodological studies quantifying and comparing the
              contribution of denoising steps to the neurofeedback signal
              quality. Advances in rtfMRI-NF research depend on reproducibility
              of methods and results. Notably, a systematic effort is needed to
              build up evidence that disentangles the various mechanisms
              influencing neurofeedback effects. To this end, we recommend that
              future rtfMRI-NF studies: (a) report implementation of a set of
              standard real-time fMRI denoising steps according to a proposed
              COBIDAS-style checklist (https://osf.io/kjwhf/), (b) ensure the
              quality of the neurofeedback signal by calculating and reporting
              community-informed quality metrics and applying offline control
              checks and (c) strive to adopt transparent principles in the form
              of methods and data sharing and support of open-source rtfMRI-NF
              software. Code and data for reproducibility, as well as an
              interactive environment to explore the study data, can be
              accessed at
              https://github.com/jsheunis/quality-and-denoising-in-rtfmri-nf.},
	author = {Heunis, Stephan and Lamerichs, Rolf and Zinger, Svitlana and Caballero-Gaudes, Cesar and Jansen, Jacobus F A and Aldenkamp, Bert and Breeuwer, Marcel},
	journal = {Human Brain Mapping},
	keywords = {denoising; fMRI; neurofeedback; quality; real-time; reproducibility},
	language = {en},
	month = aug,
	number = 12,
	pages = {3439--3467},
	title = {Quality and denoising in real-time functional magnetic resonance imaging neurofeedback: A methods review},
	volume = 41,
	year = 2020}

@article{Gramfort2014-kz,
	abstract = {Magnetoencephalography and electroencephalography (M/EEG) measure
              the weak electromagnetic signals originating from neural currents
              in the brain. Using these signals to characterize and locate
              brain activity is a challenging task, as evidenced by several
              decades of methodological contributions. MNE, whose name stems
              from its capability to compute cortically-constrained
              minimum-norm current estimates from M/EEG data, is a software
              package that provides comprehensive analysis tools and workflows
              including preprocessing, source estimation, time-frequency
              analysis, statistical analysis, and several methods to estimate
              functional connectivity between distributed brain regions. The
              present paper gives detailed information about the MNE package
              and describes typical use cases while also warning about
              potential caveats in analysis. The MNE package is a collaborative
              effort of multiple institutes striving to implement and share
              best methods and to facilitate distribution of analysis pipelines
              to advance reproducibility of research. Full documentation is
              available at http://martinos.org/mne.},
	author = {Gramfort, Alexandre and Luessi, Martin and Larson, Eric and Engemann, Denis A and Strohmeier, Daniel and Brodbeck, Christian and Parkkonen, Lauri and H{\"a}m{\"a}l{\"a}inen, Matti S},
	journal = {Neuroimage},
	keywords = {Connectivity; Electroencephalography (EEG); Inverse problem; Magnetoencephalography (MEG); Non-parametric statistics; Software; Time--frequency analysis},
	language = {en},
	month = feb,
	pages = {446--460},
	title = {{MNE} software for processing {MEG} and {EEG} data},
	volume = 86,
	year = 2014}

@article{Hofstra2020-cx,
	abstract = {By analyzing data from nearly all US PhD recipients and their
              dissertations across three decades, this paper finds
              demographically underrepresented students innovate at higher
              rates than majority students, but their novel contributions are
              ...Prior work finds a diversity paradox: Diversity breeds
              innovation, yet underrepresented groups that diversify
              organizations have less successful careers within them. Does the
              diversity paradox hold for scientists as well? We study this by
              utilizing a near-...},
	author = {Hofstra, B and Kulkarni, V V and Munoz-Najar, Galvez S and He, B and Jurafsky, D and McFarland, D A},
	journal = {Proceedings of the National Academy of Sciences (PNAS)},
	language = {en},
	number = 17,
	pages = {9284--9291},
	title = {The diversity--innovation paradox in science},
	volume = 117,
	year = 2020}

@article{DuPre2022-sh,
	author = {DuPre, Elizabeth and Holdgraf, Chris and Karakuzu, Agah and Tetrel, Lo{\"\i}c and Bellec, Pierre and Stikov, Nikola and Poline, Jean-Baptiste},
	journal = {PLoS Computational Biology},
	language = {en},
	month = jan,
	number = 1,
	pages = {e1009651},
	title = {Beyond advertising: New infrastructures for publishing integrated research objects},
	volume = 18,
	year = 2022}

@article{Knopp2021-wp,
	abstract = {PURPOSE: The aim of this work is to develop a high-performance,
              flexible, and easy-to-use MRI reconstruction framework using the
              scientific programming language Julia. METHODS: Julia is a
              modern, general purpose programming language with strong features
              in the area of signal/image processing and numerical computing.
              It has a high-level syntax but still generates efficient machine
              code that is usually as fast as comparable C/C++ applications. In
              addition to the language features itself, Julia has a
              sophisticated package management system that makes proper
              modularization of functionality across different packages
              feasible. Our developed MRI reconstruction framework MRIReco.jl
              can therefore reuse existing functionality from other Julia
              packages and concentrate on the MRI-related parts. This includes
              common imaging operators and support for MRI raw data formats.
              RESULTS: MRIReco.jl is a simple to use framework with a high
              degree of accessibility. While providing a simple-to-use
              interface, many of its components can easily be extended and
              customized. The performance of MRIReco.jl is compared to the
              Berkeley Advanced Reconstruction Toolbox (BART) and we show that
              the Julia framework achieves comparable reconstruction speed as
              the popular C/C++ library. CONCLUSIONS: Modern programming
              languages can bridge the gap between high performance and
              accessible implementations. MRIReco.jl leverages this fact and
              contributes a promising environment for future algorithmic
              development in MRI reconstruction.},
	author = {Knopp, Tobias and Grosser, Mirco},
	journal = {Magnetic Resonance in Medicine},
	keywords = {Julia; image reconstruction; magnetic resonance imaging; numerical computing; open source},
	language = {en},
	month = sep,
	number = 3,
	pages = {1633--1646},
	title = {{MRIReco.jl}: An {MRI} reconstruction framework written in Julia},
	volume = 86,
	year = 2021}

@unpublished{Borghi2021-go,
	author = {Borghi, John A and Van Gulick, A E},
	journal = {arXiv},
	title = {Promoting open science through research data management},
	year = 2021}

@article{Van_Vliet2020-xd,
	author = {van Vliet, Marijn},
	journal = {PLoS Computational Biology},
	language = {en},
	month = mar,
	number = 3,
	pages = {e1007358},
	title = {Seven quick tips for analysis scripts in neuroimaging},
	volume = 16,
	year = 2020}

@article{Dale1999-ku,
	abstract = {Several properties of the cerebral cortex, including its columnar
              and laminar organization, as well as the topographic organization
              of cortical areas, can only be properly understood in the context
              of the intrinsic two-dimensional structure of the cortical
              surface. In order to study such cortical properties in humans, it
              is necessary to obtain an accurate and explicit representation of
              the cortical surface in individual subjects. Here we describe a
              set of automated procedures for obtaining accurate
              reconstructions of the cortical surface, which have been applied
              to data from more than 100 subjects, requiring little or no
              manual intervention. Automated routines for unfolding and
              flattening the cortical surface are described in a companion
              paper. These procedures allow for the routine use of cortical
              surface-based analysis and visualization methods in functional
              brain imaging.},
	author = {Dale, A M and Fischl, B and Sereno, M I},
	journal = {Neuroimage},
	language = {en},
	month = feb,
	number = 2,
	pages = {179--194},
	title = {Cortical surface-based analysis. I. Segmentation and surface reconstruction},
	volume = 9,
	year = 1999}

@article{Munafo2017-sm,
	abstract = {Improving the reliability and efficiency of scientific research
               will increase the credibility of the published scientific
               literature and accelerate discovery. Here we argue for the
               adoption of measures to optimize key elements of the scientific
               process: methods, reporting and dissemination, reproducibility,
               evaluation and incentives. There is some evidence from both
               simulations and empirical studies supporting the likely
               effectiveness of these measures, but their broad adoption by
               researchers, institutions, funders and journals will require
               iterative evaluation and improvement. We discuss the goals of
               these measures, and how they can be implemented, in the hope
               that this will facilitate action toward improving the
               transparency, reproducibility and efficiency of scientific
               research.},
	author = {Munaf{\`o}, Marcus R and Nosek, Brian A and Bishop, Dorothy V M and Button, Katherine S and Chambers, Christopher D and Percie Du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric Jan and Ware, Jennifer J and Ioannidis, John P A},
	journal = {Nature Human Behaviour},
	number = 1,
	pages = {1--9},
	publisher = {Macmillan Publishers Limited},
	title = {A manifesto for reproducible science},
	volume = 1,
	year = 2017}

@article{Di_Tommaso2017-ov,
	author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
	journal = {Nature Biotechnology},
	language = {en},
	month = apr,
	number = 4,
	pages = {316--319},
	publisher = {Nature Publishing Group},
	title = {Nextflow enables reproducible computational workflows},
	volume = 35,
	year = 2017}

@article{Jwa2022-dq,
	abstract = {Sharing data is a scientific imperative that accelerates
              scientific discoveries, reinforces open science inquiry, and
              allows for efficient use of public investment and research
              resources. Considering these benefits, data sharing has been
              widely promoted in diverse fields and neuroscience has been no
              exception to this movement. For all its promise, however, the
              sharing of human neuroimaging data raises critical ethical and
              legal issues, such as data privacy. Recently, the heightened
              risks to data privacy posed by the rapid advances in artificial
              intelligence and machine learning techniques have made data
              sharing more challenging; the regulatory landscape around data
              sharing has also been evolving rapidly. Here we present an
              in-depth ethical and regulatory analysis that examines how
              neuroimaging data are currently shared against the backdrop of
              the relevant regulations and policies in the United States and
              how advanced software tools and algorithms might undermine
              subjects' privacy in neuroimaging data sharing. The implications
              of these novel technological threats to privacy in neuroimaging
              data sharing practices and policies will also be discussed. We
              then conclude with a proposal for a legal prohibition against
              malicious use of neuroscience data as a regulatory mechanism to
              address privacy risks associated with the data while maximizing
              the benefits of data sharing and open science practice in the
              field of neuroscience.},
	author = {Jwa, Anita S and Poldrack, Russell A},
	journal = {Hum. Brain Mapp.},
	keywords = {data privacy; data re-identification; data sharing; data use agreement; neuroethics; neuroimaging},
	language = {en},
	month = jun,
	number = 8,
	pages = {2707--2721},
	title = {The spectrum of data sharing policies in neuroimaging data repositories},
	volume = 43,
	year = 2022}

@article{Barnes2010-rl,
	author = {Barnes, Nick},
	journal = {Nature},
	language = {en},
	month = oct,
	number = 7317,
	pages = {753},
	title = {Publish your computer code: It is good enough},
	volume = 467,
	year = 2010}

@article{Marcus2007-nt,
	abstract = {The Extensible Neuroimaging Archive Toolkit (XNAT) is a software
              platform designed to facilitate common management and
              productivity tasks for neuroimaging and associated data. In
              particular, XNAT enables qualitycontrol procedures and provides
              secure access to and storage of data. XNAT follows a threetiered
              architecture that includes a data archive, user interface, and
              middleware engine. Data can be entered into the archive as XML or
              through data entry forms. Newly added data are stored in a
              virtual quarantine until an authorized user has validated it.
              XNAT subsequently maintains a history profile to track all
              changes made to the managed data. User access to the archive is
              provided by a secure web application. The web application
              provides a number of quality control and productivity features,
              including data entry forms, data-type-specific searches, searches
              that combine across data types, detailed reports, and listings of
              experimental data, upload/download tools, access to standard
              laboratory workflows, and administration and security tools. XNAT
              also includes an online image viewer that supports a number of
              common neuroimaging formats, including DICOM and Analyze. The
              viewer can be extended to support additional formats and to
              generate custom displays. By managing data with XNAT,
              laboratories are prepared to better maintain the long-term
              integrity of their data, to explore emergent relations across
              data types, and to share their data with the broader neuroimaging
              community.},
	author = {Marcus, Daniel S and Olsen, Timothy R and Ramaratnam, Mohana and Buckner, Randy L},
	journal = {Neuroinformatics},
	language = {en},
	number = 1,
	pages = {11--34},
	title = {The Extensible Neuroimaging Archive Toolkit: an informatics platform for managing, exploring, and sharing neuroimaging data},
	volume = 5,
	year = 2007}

@article{Eiss2020-dq,
	author = {Eiss, Robert},
	journal = {Nature},
	keywords = {Law; Policy; Research data},
	language = {en},
	month = aug,
	number = 7822,
	pages = {498},
	title = {Confusion over Europe's data-protection law is stalling scientific progress},
	volume = 584,
	year = 2020}

@article{McKiernan2016-yn,
	author = {{McKiernan} and {Bourne} and {Brown} and {Buck} and {Kenall} and {others}},
	journal = {Elife},
	title = {\& Spies, {JR} (2016). Point of view: How open science helps researchers succeed},
	year = 2016}

@article{Pelli1997-cb,
	abstract = {The VideoToolbox is a free collection of two hundred C
              subroutines for Macintosh computers that calibrates and controls
              the computer-display interface to create accurately specified
              visual stimuli. High-level platform-independent languages like
              MATLAB are best for creating the numbers that describe the
              desired images. Low-level, computer-specific VideoToolbox
              routines control the hardware that transforms those numbers into
              a movie. Transcending the particular computer and language, we
              discuss the nature of the computer-display interface, and how to
              calibrate and control it.},
	author = {Pelli, D G},
	journal = {Spatial Vision},
	language = {en},
	number = 4,
	pages = {437--442},
	title = {The {VideoToolbox} software for visual psychophysics: Transforming numbers into movies},
	volume = 10,
	year = 1997}

@article{Paul2021-zm,
	author = {Paul, Mariella and Govaart, Gisela and Schettino, Antonio},
	journal = {International Journal of Psychophysiology},
	pages = {52--63},
	title = {Making {ERP} research more transparent: Guidelines for preregistration},
	volume = 164,
	year = 2021}

@article{Wilson2017-td,
	abstract = {Author summary Computers are now essential in all branches of
               science, but most researchers are never taught the equivalent of
               basic lab skills for research computing. As a result, data can
               get lost, analyses can take much longer than necessary, and
               researchers are limited in how effectively they can work with
               software and data. Computing workflows need to follow the same
               practices as lab projects and notebooks, with organized data,
               documented steps, and the project structured for
               reproducibility, but researchers new to computing often don't
               know where to start. This paper presents a set of good computing
               practices that every researcher can adopt, regardless of their
               current level of computational skill. These practices, which
               encompass data management, programming, collaborating with
               colleagues, organizing projects, tracking work, and writing
               manuscripts, are drawn from a wide variety of published sources
               from our daily lives and from our work with volunteer
               organizations that have delivered workshops to over 11,000
               people since 2010.},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K},
	journal = {PLoS Computational Biology},
	month = jun,
	number = 6,
	pages = {e1005510},
	publisher = {Public Library of Science},
	title = {Good enough practices in scientific computing},
	volume = 13,
	year = 2017}

@article{Klein2018-wn,
	abstract = {We conducted preregistered replications of 28 classic and
               contemporary published findings, with protocols that were peer
               reviewed in advance, to examine variation in effect magnitudes
               across samples and settings. Each protocol was administered to
               approximately half of 125 samples that comprised 15,305
               participants from 36 countries and territories. Using the
               conventional criterion of statistical significance ( p},
	author = {Klein, Richard A and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G and Adams, Jr, Reginald B and Alper, Sinan and Aveyard, Mark and Axt, Jordan R and Babalola, Mayowa T and Bahn{\'\i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J and Berry, Daniel R and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E and Cheong, Winnee and Cicero, David C and Coen, Sharon and Coleman, Jennifer A and Collisson, Brian and Conway, Morgan A and Corker, Katherine S and Curran, Paul G and Cushman, Fiery and Dagona, Zubairu K and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E and de Bruijn, Maaike and De Schutter, Leander and Devos, Thierry and de Vries, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R and Edlund, John E and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C and Ghoshal, Tanuka and Giessner, Steffen R and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E and Grahek, Ivan and Green, Eva G T and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L and Hall, Michael P and Heffernan, Marie E and Hicks, Joshua A and Houdek, Petr and Huntsinger, Jeffrey R and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and Innes-Ker, {\AA}se H and Jim{\'e}nez-Leal, William and John, Melissa-Sue and Joy-Gaba, Jennifer A and Kamilo{\u g}lu, Roza G and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B and Levitan, Carmel A and Lewis, Jr, Neil A and Lins, Samuel and Lipsey, Nikolette P and Losee, Joy E and Maassen, Esther and Maitner, Angela T and Malingumu, Winfrida and Mallett, Robyn K and Marotta, Satia A and Me{\dj}edovi{\'c}, Janko and Mena-Pacheco, Fernando and Milfont, Taciano L and Morris, Wendy L and Murphy, Sean C and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and P{\'e}rez-S{\'a}nchez, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M H and Rutchick, Abraham M and Saavedra, Patricio and Saeri, Alexander K and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D and Sekerdej, Maciej B and Sirlop{\'u}, David and Skorinko, Jeanine L M and Smith, Michael A and Smith-Castro, Vanessa and Smolders, Karin C H J and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G and Stouten, Jeroen and Street, Chris N H and Sundfelt, Oskar K and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C W and Tanzer, Norbert and Tear, Morgan J and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M and Ujhelyi, Adrienn and van Aert, Robbie C M and van Assen, Marcel A L M and van der Hulst, Marije and van Lange, Paul A M and van 't Veer, Anna Elisabeth and V{\'a}squez- Echeverr{\'\i}a, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P J and Vranka, Marek A and Welch, Cheryl and Wichman, Aaron L and Williams, Lisa A and Wood, Michael and Woodzicka, Julie A and Wronska, Marta K and Young, Liane and Zelenski, John M and Zhijia, Zeng and Nosek, Brian A},
	journal = {Advances in Methods and Practices in Psychological Science},
	language = {en},
	month = dec,
	number = 4,
	pages = {443--490},
	publisher = {SAGE Publications},
	title = {Many labs 2: Investigating variation in replicability across samples and settings},
	volume = 1,
	year = 2018}

@article{Winter2016-nb,
	abstract = {Conclusion Regardless public or private healthcare system, MR
                costs determine healthcare outcomes. There is a high demand for
                affordable MR technology around the world to improve patient
                diagnosis and treatment. We, the MR research community are able
                to meet {\ldots}},
	author = {Winter, Lukas and Haopeng, H and Barghoorn, Antonia and Hoffmann, Werner and Hetzer, Stefan and Winkler, Simone and {Others}},
	conference = {International Society for Magnetic Resonance in Medicine},
	date-modified = {2022-07-28 16:56:06 +0300},
	journal = {Proc. of. the International Society for Magnetic Resonance in Medicine (ISMRM)},
	title = {Open source imaging initiative},
	volume = 3638,
	year = 2016}

@article{Pavlov2021-tg,
	abstract = {There is growing awareness across the neuroscience community that
              the replicability of findings about the relationship between
              brain activity and cognitive phenomena can be improved by
              conducting studies with high statistical power that adhere to
              well-defined and standardised analysis pipelines. Inspired by
              recent efforts from the psychological sciences, and with the
              desire to examine some of the foundational findings using
              electroencephalography (EEG), we have launched \#EEGManyLabs, a
              large-scale international collaborative replication effort. Since
              its discovery in the early 20th century, EEG has had a profound
              influence on our understanding of human cognition, but there is
              limited evidence on the replicability of some of the most highly
              cited discoveries. After a systematic search and selection
              process, we have identified 27 of the most influential and
              continually cited studies in the field. We plan to directly test
              the replicability of key findings from 20 of these studies in
              teams of at least three independent laboratories. The design and
              protocol of each replication effort will be submitted as a
              Registered Report and peer-reviewed prior to data collection.
              Prediction markets, open to all EEG researchers, will be used as
              a forecasting tool to examine which findings the community
              expects to replicate. This project will update our confidence in
              some of the most influential EEG findings and generate a large
              open access database that can be used to inform future research
              practices. Finally, through this international effort, we hope to
              create a cultural shift towards inclusive, high-powered
              multi-laboratory collaborations.},
	author = {Pavlov, Yuri G and Adamian, Nika and Appelhoff, Stefan and Arvaneh, Mahnaz and Benwell, Christopher S Y and Beste, Christian and Bland, Amy R and Bradford, Daniel E and Bublatzky, Florian and Busch, Niko A and Clayson, Peter E and Cruse, Damian and Czeszumski, Artur and Dreber, Anna and Dumas, Guillaume and Ehinger, Benedikt and Ganis, Giorgio and He, Xun and Hinojosa, Jos{\'e} A and Huber-Huber, Christoph and Inzlicht, Michael and Jack, Bradley N and Johannesson, Magnus and Jones, Rhiannon and Kalenkovich, Evgenii and Kaltwasser, Laura and Karimi-Rouzbahani, Hamid and Keil, Andreas and K{\"o}nig, Peter and Kouara, Layla and Kulke, Louisa and Ladouceur, Cecile D and Langer, Nicolas and Liesefeld, Heinrich R and Luque, David and MacNamara, Annmarie and Mudrik, Liad and Muthuraman, Muthuraman and Neal, Lauren B and Nilsonne, Gustav and Niso, Guiomar and Ocklenburg, Sebastian and Oostenveld, Robert and Pernet, Cyril R and Pourtois, Gilles and Ruzzoli, Manuela and Sass, Sarah M and Schaefer, Alexandre and Senderecka, Magdalena and Snyder, Joel S and Tamnes, Christian K and Tognoli, Emmanuelle and van Vugt, Marieke K and Verona, Edelyn and Vloeberghs, Robin and Welke, Dominik and Wessel, Jan R and Zakharov, Ilya and Mushtaq, Faisal},
	journal = {Cortex},
	keywords = {Cognitive neuroscience; EEG; ERP; Many labs; Open science; Replication},
	language = {en},
	month = apr,
	pages = {213--229},
	title = {{\#EEGManyLabs}: Investigating the replicability of influential {EEG} experiments},
	volume = 144,
	year = 2021}

@article{Forbes2021-rs,
	abstract = {Over the past decade, the field of psychology has come under
              increasing fire for the replicability of purported findings, for
              the transparency of the methods used, and for the
              generalisability of the claims. In general, these criticisms have
              focused on the methodological and statistical aspects of
              published work. Herein, we focus on an underdiscussed issue: we
              highlight the importance of diversity of both our experimental
              samples and of our researchers within developmental psychology as
              a barrier to generalisability. Far beyond being a purely
              methodological question, e.g., of heterogenous sampling, ignoring
              the importance of context and environment in development implies
              risking to comprehend pivotal facets of development. Importantly,
              we discuss the harms done to community-building and to our own
              science's theoretical contributions, as a direct result of
              defining and maintaining misplaced ``norms'' or ``typical''
              developmental scenarios. Finally, we outline how even small steps
              by individuals can be impactful, such as ceasing to request
              unsubstantiated comparisons to the Western ``norm'' in peer
              review.},
	author = {Forbes, Samuel H and Aneja, Prerna and Guest, Olivia},
	date-modified = {2022-08-01 13:42:11 +0300},
	journal = {PsyArXiv},
	month = oct,
	title = {The Myth of (A)typical Development},
	year = 2021}

@article{Brainard1997-zs,
	author = {Brainard, David H},
	journal = {Spatial Vision},
	number = 4,
	pages = {433--436},
	title = {The Psychophysics Toolbox},
	volume = 10,
	year = 1997}
